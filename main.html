<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Felicitas Pojtinger (Stuttgart Media University)" />
  <meta name="dcterms.date" content="2023-08-04" />
  <meta name="keywords" content="true" />
  <title>Efficient Synchronization of Linux Memory Regions over a Network: A Comparative Study and Implementation</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Efficient Synchronization of Linux Memory Regions over
a Network: A Comparative Study and Implementation</h1>
<p class="subtitle">TODO: Add subtitle</p>
<p class="author">Felicitas Pojtinger (Stuttgart Media University)</p>
<p class="date">2023-08-04</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<h2 id="section" class="unnumbered unlisted"></h2>
<p>TODO: Add abstract</p>
</div>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1.1</span>
Introduction</a></li>
<li><a href="#technology"><span class="toc-section-number">1.2</span>
Technology</a>
<ul>
<li><a href="#the-linux-kernel"><span class="toc-section-number">1.2.1</span> The Linux Kernel</a></li>
<li><a href="#linux-kernel-modules"><span class="toc-section-number">1.2.2</span> Linux Kernel Modules</a></li>
<li><a href="#unix-signals-and-handlers"><span class="toc-section-number">1.2.3</span> UNIX Signals and
Handlers</a></li>
<li><a href="#principle-of-locality"><span class="toc-section-number">1.2.4</span> Principle of Locality</a></li>
<li><a href="#memory-hierarchy"><span class="toc-section-number">1.2.5</span> Memory Hierarchy</a></li>
<li><a href="#memory-management-in-linux"><span class="toc-section-number">1.2.6</span> Memory Management in
Linux</a></li>
<li><a href="#swap-space"><span class="toc-section-number">1.2.7</span>
Swap Space</a></li>
<li><a href="#page-faults"><span class="toc-section-number">1.2.8</span>
Page Faults</a></li>
<li><a href="#mmap"><span class="toc-section-number">1.2.9</span>
<code>mmap</code></a></li>
<li><a href="#inotify"><span class="toc-section-number">1.2.10</span>
<code>inotify</code></a></li>
<li><a href="#linux-kernel-caching"><span class="toc-section-number">1.2.11</span> Linux Kernel Caching</a></li>
<li><a href="#tcp-udp-and-quic"><span class="toc-section-number">1.2.12</span> TCP, UDP and QUIC</a></li>
<li><a href="#delta-synchronization"><span class="toc-section-number">1.2.13</span> Delta Synchronization</a></li>
<li><a href="#file-systems-in-userspace-fuse"><span class="toc-section-number">1.2.14</span> File Systems In Userspace
(FUSE)</a></li>
<li><a href="#network-block-device-nbd"><span class="toc-section-number">1.2.15</span> Network Block Device
(NBD)</a></li>
<li><a href="#virtual-machine-live-migration"><span class="toc-section-number">1.2.16</span> Virtual Machine Live
Migration</a></li>
<li><a href="#streams-and-pipelines"><span class="toc-section-number">1.2.17</span> Streams and Pipelines</a></li>
<li><a href="#grpc"><span class="toc-section-number">1.2.18</span>
gRPC</a></li>
<li><a href="#redis"><span class="toc-section-number">1.2.19</span>
Redis</a></li>
<li><a href="#s3-and-minio"><span class="toc-section-number">1.2.20</span> S3 and Minio</a></li>
<li><a href="#cassandra-and-scyllladb"><span class="toc-section-number">1.2.21</span> Cassandra and
ScylllaDB</a></li>
</ul></li>
</ul>
</nav>
<section id="introduction" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Introduction</h2>
<p>TODO: Add introduction</p>
</section>
<section id="technology" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Technology</h2>
<section id="the-linux-kernel" data-number="0.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span>
The Linux Kernel</h3>
<p>The open-source Linux kernel, was created by Linus Torvalds in 1991.
Developed primarily in the C programming language, it has recently seen
the addition of Rust as an approved language for further expansion and
development, esp. of drivers<span class="citation" data-cites="linux2023docs">[1]</span>. The powers millions of devices
across the globe, including servers, desktop computers, mobile phones,
and embedded devices. It serves as an intermediary between hardware and
applications, as an abstraction layer that simplifies the interaction
between them. It is engineered for compatibility with a wide array of
architectures, such as ARM, x86, RISC-V, and others.</p>
<p>The kernel does not function as a standalone operating system. This
role is fulfilled by distributions, which build upon the Linux kernel to
create fully-fledged operating systems<span class="citation" data-cites="love2010linux">[2]</span>. Distributions supplement the
kernel with additional userspace tools, examples being GNU coreutils or
BusyBox. Depending on their target audience, they further enhance
functionality by integrating desktop environments and other
software.</p>
<p>The open-source nature of the Linux kernel makes it especially
interesting for academic exploration and usage. It offers transparency,
allowing anyone to inspect the source code in depth. Furthermore, it
encourages collaboration by enabling anyone to modify and contribute to
the source code. This transparency, coupled with the potential for
customization and improvement, makes developing for the Linux kernel a
good choice for this thesis.</p>
</section>
<section id="linux-kernel-modules" data-number="0.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span>
Linux Kernel Modules</h3>
<p>Linux is a extensible, but not a microkernel. Despite it’s monolithic
nature, it allows for the integration of kernel modules<span class="citation" data-cites="love2010linux">[2]</span>. Kernel modules
are small pieces of kernel-level code that can be dynamically
incorporated into the kernel, presenting the advantage of extending
kernel functionality without necessitating system reboots.</p>
<p>The dynamism of these modules comes from their ability to be loaded
and unloaded into the running kernel as per user needs. This
functionality aids in keeping the kernel size both manageable and
maintainable, thereby promoting efficiency. Kernel modules are
traditionally developed using the C programming language, like the
kernel itself, ensuring compatibility and consistent performance.</p>
<p>Kernel modules interact with the kernel via APIs (Application
Programming Interfaces). Despite their utility, since they run in kernel
space, modules do carry a potential risk. If not written with careful
attention to detail, they can introduce significant instability into the
kernel, negatively affecting the overall system performance and
reliability.</p>
<p>Modules can be managed and controlled at different stages, starting
from boot time, and be manipulated dynamically when the system is
already running. This is facilitated by utilities like
<code>modprobe</code> and <code>rmmod</code><span class="citation" data-cites="maurer2008professional">[3]</span>.</p>
<p>In the lifecycle of a kernel module, two key functions are of
significance: initialization and cleanup. The initialization function is
responsible for setting up the module when it’s loaded into the kernel.
Conversely, the cleanup function is used to safely remove the module
from the kernel, freeing up any resources it previously consumed. These
lifecycle functions, along with other such hooks, provide a more
structured approach to module development.</p>
</section>
<section id="unix-signals-and-handlers" data-number="0.2.3">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span>
UNIX Signals and Handlers</h3>
<p>UNIX signals are an integral component of UNIX-like systems,
including Linux. They function as software interrupts, notifying a
process of significant occurrences, such as exceptions. Signals may be
generated from various sources, including the kernel, user input, or
other processes, making them a versatile tool for inter-process
notifications.</p>
<p>Aside from this notification role, signals also serve as an
asynchronous communication mechanism between processes or between the
kernel and a process. As such, they have an inherent ability to deliver
important notifications without requiring the recipient process to be in
a specific state of readiness<span class="citation" data-cites="stevens2000advanced">[4]</span>. Each signal has a default
action associated with it, the most common of which are terminating the
process or simply ignoring the signal.</p>
<p>To customize how a process should react upon receiving a specific
signal, handlers can be utilized. Handlers dictate the course of action
a process should take when a signal is received. Using the
<code>sigaction()</code> function, a handler can be installed for a
specific signal, enabling a custom response to that signal such as
reloading configuration, cleaning up ressources before exiting or
enabling verbose logging <span class="citation" data-cites="robbins2003unix">[5]</span>.</p>
<p>It is however important to note that signals are not typically
utilized as a primary inter-process communication (IPC) mechanism. This
is primarily due to their limitation in carrying additional data. While
signals effectively alert a process of an event, they are not designed
to convey forther information related to that event; consequently, they
are best used in scenarios where simple event-based notifications are
sufficient, rather than for more complex data exchange requirements.</p>
</section>
<section id="principle-of-locality" data-number="0.2.4">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span>
Principle of Locality</h3>
<p>The principle of locality, or locality of reference, refers to the
tendency of a processor in a computer system to recurrently access the
same set of memory locations within a brief span of time. This principle
forms the basis of a predictable pattern of behavior that is evident
across computer systems, and can be divided into two distinct types:
temporal locality and spatial locality<span class="citation" data-cites="stallings2010architecture">[6]</span>.</p>
<p>Temporal locality revolves around the frequent use of particular data
within a limited time period. Essentially, if a memory location is
accessed once, it is probable that this same location will be accessed
again in the near future. To leverage this pattern and improve
performance, computer systems are designed to maintain a copy of this
frequently accessed data in a faster memory storage, which in turn,
significantly reduces the latency in subsequent references.</p>
<p>Spatial locality, on the other hand, refers to the use of data
elements that are stored in nearby locations. That is, once a particular
memory location is accessed, the system assumes that other nearby
locations are also likely to be accessed shortly. Therefore, to optimize
performance, the system tries to anticipate these subsequent accesses by
preparing for faster access to these nearby memory locations. Temporal
locality is considered a unique instance of spatial locality,
demonstrating how the two types are closely interlinked.</p>
<p>A specific instance of spatial locality, termed sequential locality,
occurs when the data elements are organized and accessed in a linear
sequence. An example of this is when elements in a one-dimensional array
are traversed systematically, accessing the elements one by one in their
sequential order.</p>
<p>Locality of reference can be instrumental in improving the overall
performance of a system. To achieve this, a variety of optimization
techniques are deployed, such as caching, which stores copies of
frequently accessed data in quick-access memory, and prefetching for
memory, which involves loading potential future data into cache before
it’s actually needed.</p>
</section>
<section id="memory-hierarchy" data-number="0.2.5">
<h3 data-number="1.2.5"><span class="header-section-number">1.2.5</span>
Memory Hierarchy</h3>
<p>The memory hierarchy in computers is an organized structure based on
factors such as size, speed, cost, and proximity to the Central
Processing Unit (CPU). It follows the principle of locality, which
suggests that data and instructions that are accessed frequently should
be stored as close to the CPU as possible<span class="citation" data-cites="smith1982cache">[7]</span>. This principle is crucial
primarily due to the limitations of “the speed of the cable”, where both
throughput and latency decrease as distance increases due to factors
like signal dampening and the finite speed of light.</p>
<p>TODO: Add graphic of the memory hierarchy</p>
<p>At the top of the hierarchy are registers, which are closest to the
CPU. They offer very high speed, but provide limited storage space,
typically accommodating 32-64 bits of data. These registers are used by
the CPU to perform operations.</p>
<p>Following registers in the hierarchy is cache memory, typically
divided into L1, L2, and L3 levels. As the level increases, each layer
becomes larger and less expensive. Cache memory serves as a buffer for
frequently accessed data, with predictive algorithms typically
optimizing its usage.</p>
<p>Main Memory, i.e. Random Access Memory (RAM), provides larger storage
capacity than cache but operates at a slower speed. It typically stores
running programs and open files.</p>
<p>Below main memory, we find secondary storage devices such as Solid
State Drives (SSD) or Hard Disk Drives (HDD). Although slower than RAM,
these devices can store larger amounts of data and typically contain the
operating system and application binary fies. Importantly, they are
persistent, meaning they retain data even after power is cut.</p>
<p>Tertiary storage, including optical disks and tape, is slow but very
cost-effective. Tape storage can store very large amounts of data for
long periods of time. These types of storage are typically used for
archiving or physically transporting data, such as importing data from
personal infrastructure to a service like AWS<span class="citation" data-cites="barr2021offline">[8]</span>.</p>
<p>The memory hierarchy is not static but evolves with technological
advancements, leading to some blurring of these distinct layers<span class="citation" data-cites="maruf2023memory">[9]</span>. For instance,
Non-Volatile Memory Express (NVMe) storage technologies can rival the
speed of RAM while offering greater storage capacities. Similarly, some
research, such as the work presented in this thesis, further challenges
traditional hierarchies by exposing tertiary or secondary storage with
the same interface as main memory.</p>
</section>
<section id="memory-management-in-linux" data-number="0.2.6">
<h3 data-number="1.2.6"><span class="header-section-number">1.2.6</span>
Memory Management in Linux</h3>
<p>Memory management forms a cornerstone of any operating system,
serving as a critical buffer between applications and physical memory.
Arguably, it can be considered one of the fundamental purposes of an
operating system itself. This system helps maintain system stability and
provides security guarantees, such as ensuring that only a specific
process can access its allocated memory.</p>
<p>Within the context of the Linux operating system, memory management
is divided into two major segments: kernel space and user space.</p>
<p>Kernel space is where the kernel itself and kernel modules operate.
The kernel memory module is responsible for managing this segment. Slab
allocation is a technique employed in kernel space management; this
technique groups objects of the same size into caches, enhancing memory
allocation speed and reducing fragmentation of memory<span class="citation" data-cites="bonwick1994slaballoc">[10]</span>.</p>
<p>User space is the memory segment where applications and certain
drivers store their memory<span class="citation" data-cites="gorman2004linuxmem">[11]</span>. User space memory
management involves a paging system, offering each application its
unique private virtual address space.</p>
<p>This virtual address space is divided into units known as pages, each
typically 4 KB in size. These pages can be mapped to any location in
physical memory, providing flexibility and optimizing memory
utilization. The use of this virtual address space further adds a layer
of abstraction between the application and the physical memory,
enhancing the security and isolation of processes.</p>
</section>
<section id="swap-space" data-number="0.2.7">
<h3 data-number="1.2.7"><span class="header-section-number">1.2.7</span>
Swap Space</h3>
<p>Swap space refers to a designated portion of the secondary storage
utilized as virtual memory in a computer system<span class="citation" data-cites="gorman2004linuxmem">[11]</span>. This feature plays a
crucial role in systems that run multiple applications simultaneously.
When memory resources are strained, swap space comes into play,
relocating inactive parts of the RAM to secondary storage. This action
frees up space in primary memory for other processes, enabling smoother
operation and preventing a potential system crash.</p>
<p>In the case of Linux, swap space implementation aligns with a demand
paging system. This means that memory is allocated only when required.
The swap space in Linux can be a swap partition, which is a distinct
area within the secondary storage, or it can take the form of a swap
file, which is a standard file that can be expanded or truncated based
on need. The usage of swap partitions and files is transparent to the
user.</p>
<p>The Linux kernel employs a Least Recently Used (LRU) algorithm to
determine which memory pages should be moved to swap space. This
algorithm effectively prioritizes pages based on their usage,
transferring those that have not been recently used to swap space.</p>
<p>Swap space also plays a significant role in system hibernation.
Before the system enters hibernation, the content of RAM is stored in
the swap space, where it remains persistent even without power. When the
system is resumed, the memory content is read back from swap space,
restoring the system to its pre-hibernation state<span class="citation" data-cites="kernel2023suspend">[12]</span>.</p>
<p>However, the use of swap space can impact system performance. Since
secondary storage devices are usually slower than primary memory, heavy
reliance on swap space can cause significant system slowdowns. To
mitigate this, Linux allows for the adjustment of “swappiness”, a
parameter that controls the system’s propensity to swap memory pages.
Adjusting this setting can balance the use of swap space to maintain
system performance while still preserving the benefits of virtual memory
management.</p>
</section>
<section id="page-faults" data-number="0.2.8">
<h3 data-number="1.2.8"><span class="header-section-number">1.2.8</span>
Page Faults</h3>
<p>Page faults are instances in which a process attempts to access a
page that is not currently available in primary memory. This situation
triggers the operating system to swap the necessary page from secondary
storage into primary memory. These are significant events in memory
management, as they determine how efficiently an operating system
utilizes its resources.</p>
<p>Page faults can be broadly categorized into two types: minor and
major. Minor page faults occur when the desired page resides in memory
but isn’t linked to the process that requires it. On the other hand, a
major page fault takes place when the page has to be loaded from
secondary storage, a process that typically takes more time and
resources<span class="citation" data-cites="maurer2008professional">[3]</span>.</p>
<p>To minimize the occurrence of page faults, memory management
algorithms such as the afore-mentioned Least Recently Used (LRU) and the
more straightforward clock algorithm are often employed. These
algorithms effectively manage the order and priority of memory pages,
helping to ensure that frequently used pages are readily available in
primary memory.</p>
<p>Handling page faults involves certain techniques to ensure smooth
operation. One such technique is prefetching, which anticipates future
page requests and proactively loads these pages into memory. Another
approach involves page compression, where inactive pages are compressed
and stored in memory preemptively<span class="citation" data-cites="silberschatz2018operating">[13]</span>. This reduces the
likelihood of major page faults by conserving memory space, allowing
more pages to reside in primary memory.</p>
<p>In general, handling page faults is a task delegated to the kernel.
This critical balance between resource availability and system
performance is part of the kernel’s memory management duties, ensuring
that processes can access the pages they require while maintaining
efficient use of system memory.</p>
</section>
<section id="mmap" data-number="0.2.9">
<h3 data-number="1.2.9"><span class="header-section-number">1.2.9</span>
<code>mmap</code></h3>
<p><code>mmap</code> is a versatile UNIX system call, used for mapping
files or devices into memory, enabling a variety of core tasks like
shared memory, file I/O, and fine-grained memory allocation. Due to its
powerful nature, it is commonly harnessed in applications like
databases.</p>
<p>One standout feature of <code>mmap</code> is its ability to create
what is essentially a direct memory mapping between a file and a region
of memory<span class="citation" data-cites="choi2017mmap">[14]</span>.
This connection means that read operations performed on the mapped
memory region directly correspond to reading the file and vice versa,
enhancing efficiency by reducing the overhead as the necessity for
context switches (compared to i.e. the <code>read</code> or
<code>write</code> system calls) diminishes.</p>
<p>The key advantage that <code>mmap</code> provides is the capacity to
facilitate zero-copy operations. In practical terms, this signifies data
can be accessed directly as if it were positioned in memory, eliminating
the need to copy it from the disk first. This direct memory access saves
time and reduces processing requirements, offering substantial
performance improvements.</p>
<p><code>mmap</code> is also proficient in sharing memory between
processes without having to pass through the kernel with system
calls<span class="citation" data-cites="stevens2000advanced">[4]</span>.
With this feature, <code>mmap</code> can create shared memory spaces
where multiple processes can read and write, enhancing interprocess
communication and data transfer efficiency.</p>
<p>The potential speed improvement does however come with a notable
drawback: It bypasses the file system cache, which can potentially
result in stale data when multiple processes are reading and writing
simultaneously. This bypass may lead to a scenario where one process
modifies data in the <code>mmap</code> region, and another process that
is not monitoring for changes might remain unaware and continue to work
with outdated data.</p>
</section>
<section id="inotify" data-number="0.2.10">
<h3 data-number="1.2.10"><span class="header-section-number">1.2.10</span> <code>inotify</code></h3>
<p>The <code>inotify</code> is an event-driven notification system of
the Linux kernel, designed to monitor the file system for different
events, such as modifications and accesses, among others<span class="citation" data-cites="prokop2010inotify">[15]</span>. Its
particularly useful because it can be configured to watch only write
operations on certain files, i.e. only <code>write</code> operations.
This level of control can offer considerable benefits in cases where
there is a need to focus system resources on certain file system events,
and not on others.</p>
<p>Naturally, <code>inotify</code> comes with some recognizable
advantages. Significantly, it diminishes overhead and resource use when
compared to polling strategies. Polling is an operation-heavy approach
as it continuously checks the status of the file system, regardless of
whether any changes have occurred. In contrast, <code>inotify</code>
works in a more event-driven way, where it only takes action when a
specific event actually occurs. This is usually more efficient, reducing
overhead especially where there are infrequent changes to the file
system.</p>
<p>Thanks to its efficiency and flexibility, <code>inotify</code> has
found its utilization across many applications, especially in file
synchronization services. In this usecase, the ability to instantly
notify the system of file changes aids in instant synchronization of
files, demonstrating how critical its role can be in real-time or near
real-time systems that are dependent on keeping data up-to-date.</p>
<p>However, as is the case with many system calls, there is a limit to
its scalability. <code>inotify</code> is constrained by a limit on how
many watches can be established. This limitation can pose challenges in
intricate systems where there is a high quantity of files or directories
to watch for, and might warrant additional management or fallback to
heavier polling mechanisms for some parts of the system.</p>
</section>
<section id="linux-kernel-caching" data-number="0.2.11">
<h3 data-number="1.2.11"><span class="header-section-number">1.2.11</span> Linux Kernel Caching</h3>
<p>Caching is a key feature of the Linux kernel that work to boost
efficiency and performance. Within this framework, there are two broad
categories: disk caching and file caching.</p>
<p>Disk caching in Linux is a strategic method that temporarily stores
frequently accessed data in RAM. It is implemented through the page
cache subsystem, and operates under the assumption that data situated
near data that has already been accessed will be needed soon. By
retaining data close to the CPU where it may be swiftly accessed without
costly disk reads can greatly reduce overall access time. The data
within the cache is also managed using the LRU algorithm, which prunes
the least recently used items first when space is needed.</p>
<p>Linux also caches file system metadata in specialized structures
known as the <code>dentry</code> and <code>inode</code> caches. This
metadata encompasses varied information such as file names, attributes,
and locations. The key benefit of this is that it expedites the
resolution of path names and file attributes, such as tracking when
files were last changed for polling. Notably, file read/write operations
are also channeled through the disk cache, further illustrating the
intricate interconnectedness of disk and file caching mechanisms in the
Linux Kernel.</p>
<p>While such caching mechanisms can improve performance, they also
introduce complexities. One such complexity involves maintaining data
consistency between the disk and cache through the process known as
writebacks; aggressive writebacks, where data is copied back to disk
frequently, can lead to reduced performance, while excessive delays may
risk data loss if the system crashes before data has been saved.</p>
<p>Another complexity arises from the necessity to release cached data
under memory pressure, known as cache eviction. This requires
sophisticated algorithms, such as LRU, to ensure effective utilization
of available cache space<span class="citation" data-cites="maurer2008professional">[3]</span>. Prioritizing what to
keep in cache when memory pressure builds does directly impact the
overall system performance.</p>
</section>
<section id="tcp-udp-and-quic" data-number="0.2.12">
<h3 data-number="1.2.12"><span class="header-section-number">1.2.12</span> TCP, UDP and QUIC</h3>
<p>TCP (Transmission Control Protocol), UDP (User Datagram Protocol),
and QUIC (Quick UDP Internet Connections) are three key communication
protocols utilized in the internet today.</p>
<p>TCP has long been the reliable backbone for internet communication
due to its connection-oriented nature <span class="citation" data-cites="postel1981tcp">[16]</span>. It ensures the guaranteed
delivery of data packets and their correct order, rendering it a highly
dependable means for data transmission. Significantly, TCP incorporates
error checking, allowing the detection and subsequent retransmission of
lost packets. TCP also includes a congestion control mechanism to manage
data transmission seamlessly during high traffic. Due to to these
features and it’s long legacy, TCP is widely used to power the majority
of the web where reliable, ordered, and error-checked data transmission
is required.</p>
<p>UDP is a connectionless protocol that does not make the same
guarantees about the reliability or ordered delivery of data packets
<span class="citation" data-cites="postel1980udp">[17]</span>. This
lends UDP a speed advantage over TCP, resulting in less communication
overhead. Although it lacks TCP’s robustness in handling errors and
maintaining data order, UDP finds use in applications where speed and
latency take precedence over reliability. This includes online gaming,
video calls, and other real-time communication modes where quick data
transmission is crucial even if temporary packet loss occurs.</p>
<p>QUIC, a modern UDP-base transport layer protocol, was originally
created by Google and standardized by the IETF in 2021<span class="citation" data-cites="rfc2021quic">[18]</span>. It aspires to
combine the best qualities of TCP and UDP <span class="citation" data-cites="langley2017quic">[19]</span>. Unlike raw UDP, QUIC ensures
the reliability of data transmission and guarantees the ordered delivery
of data packets similarly to TCP, while intending to keep UDP’s speed
advantages. One of QUIC’s standout features is its ability to reduce
connection establishment times, which effectively lowers initial
latency. It achieves this by merging the typically separate connection
and security handshakes, reducing the time taken for a connection to be
established. Additionally, QUIC is designed to prevent the issue of
“head-of-line blocking”, allowing for the independent delivery of
separate data streams. This means it can handle the delivery of separate
data streams without one stream blocking another, resulting in smoother
and more efficient transmission, a feature which is especially important
for applications with lots of concurrent transmissions.</p>
</section>
<section id="delta-synchronization" data-number="0.2.13">
<h3 data-number="1.2.13"><span class="header-section-number">1.2.13</span> Delta Synchronization</h3>
<p>Delta synchronization is a technique that allows for efficient
synchronization of files between hosts, aiming to transfer only those
parts of the file that have undergone changes instead of the entire file
in order to reduce network and I/O overhead. Perhaps the most recognized
tool employing this method of synchronization is <code>rsync</code>, an
open-source data synchronization utility in Unix-like operating
systems<span class="citation" data-cites="xiao2018rsync">[20]</span>.</p>
<p>TODO: Add sequence diagram of the delta sync protocol from
https://blog.acolyer.org/2018/03/02/towards-web-based-delta-synchronization-for-cloud-storage-systems/</p>
<p>While there are many applications of such an algorithm, it typically
starts on file block division, dissecting the file on the destination
side into fixed-size blocks. For each of these blocks, a quick albeit
weak checksum calculation is performed, and these checksums are
transferred to the source system.</p>
<p>The source initiates the same checksum calculation process. These
checksums are then compared to those received from the destination
(matching block identification). The outcome of this comparison allows
the source to detect the blocks which have transformed since the last
synchronization.</p>
<p>Once the altered blocks are identified, the source proceeds to send
the offset of each block alongside the data of the changed block to the
destination. Upon receiving a block, the destination writes it to the
specific offset in the file. This process results in the reconstruction
of the file in accordance with the modifications undertaken at the
source, after which the next synchronization cycle can start.</p>
</section>
<section id="file-systems-in-userspace-fuse" data-number="0.2.14">
<h3 data-number="1.2.14"><span class="header-section-number">1.2.14</span> File Systems In Userspace
(FUSE)</h3>
<p>File Systems in Userspace (FUSE) is a software interface that enables
the creation of custom file systems in the userspace, as opposed to
developing them as kernel modules. This reduces the need for the
low-level kernel development skills that are usually associated with
creating new file systems.</p>
<p>The FUSE APIs are available on various platforms; though mostly
deployed on Linux, it can also be found on macOS and FreeBSD. In FUSE, a
userspace program registers itself with the FUSE kernel module and
provides callbacks for the file system operations. A simple read-only
FUSE can for example implement the following callbacks:</p>
<p>The <code>getattr</code> function is responsible for getting the
attributes of a file. For a real file system, this would include things
like the file’s size, its permissions, when it was last accessed or
modified, and so forth:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="dt">int</span> example_getattr<span class="op">(</span><span class="dt">const</span> <span class="dt">char</span> <span class="op">*</span>path<span class="op">,</span> <span class="kw">struct</span> stat <span class="op">*</span>stbuf<span class="op">,</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                         <span class="kw">struct</span> fuse_file_info <span class="op">*</span>fi<span class="op">);</span></span></code></pre></div>
<p>The <code>readdir</code> function is used when a process wants to
list the files in a directory. It’s responsible for filling in the
entries for that directory:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="dt">int</span> example_readdir<span class="op">(</span><span class="dt">const</span> <span class="dt">char</span> <span class="op">*</span>path<span class="op">,</span> <span class="dt">void</span> <span class="op">*</span>buf<span class="op">,</span> fuse_fill_dir_t filler<span class="op">,</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                         off_t offset<span class="op">,</span> <span class="kw">struct</span> fuse_file_info <span class="op">*</span>fi<span class="op">,</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>                         <span class="kw">enum</span> fuse_readdir_flags flags<span class="op">);</span></span></code></pre></div>
<p>The <code>open</code> function is called when a process opens a file.
It’s responsible for checking that the operation is permitted (i.e. the
file exists and the process has the necessary permissions), and for
doing any necessary setup:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="dt">int</span> example_open<span class="op">(</span><span class="dt">const</span> <span class="dt">char</span> <span class="op">*</span>path<span class="op">,</span> <span class="kw">struct</span> fuse_file_info <span class="op">*</span>fi<span class="op">);</span></span></code></pre></div>
<p>Finally, the <code>read</code> function is used when a process wants
to read data from a file. It’s responsible for copying the requested
data into the provided buffer:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> <span class="dt">int</span> example_read<span class="op">(</span><span class="dt">const</span> <span class="dt">char</span> <span class="op">*</span>path<span class="op">,</span> <span class="dt">char</span> <span class="op">*</span>buf<span class="op">,</span> <span class="dt">size_t</span> size<span class="op">,</span> off_t offset<span class="op">,</span> <span class="kw">struct</span> fuse_file_info <span class="op">*</span>fi<span class="op">);</span></span></code></pre></div>
<p>These callbacks would then be added to the FUSE operations struct and
passed to <code>fuse_main</code>, which takes care of registering the
operations with the FUSE kernel module and mounts the FUSE to a
directory. Similarly to this, callbacks for handling writes etc. can be
provided to the operation struct for a read-write capable FUSE<span class="citation" data-cites="libfuse2020example">[21]</span>.</p>
<p>When a user then performs a file system operation on a mounted FUSE
file system, the kernel module sends a request for executing that
operation to the userspace program. This is followed by the userspace
program returning a response, which the FUSE kernel module conveys back
to the user. As such, FUSE circumvents the complexity of coding the file
system implementation directly in the kernel. This approach enhances
safety, preventing entire kernel crashes due to errors within the
implementation being limited to user instead of kernel space.</p>
<p>TODO: Add graphic from
https://en.wikipedia.org/wiki/Filesystem_in_Userspace#/media/File:FUSE_structure.svg</p>
<p>Another benefit of a file system implemented as a FUSE is its
inherent portability. Unlike a file system created as a kernel module,
its interaction with the FUSE module rather than the kernel itself
creates a stronger contract between the two, and allows shipping the
file system as a plain binary instead of a binary kernel module, which
typically need to be built from source on the target machine unless they
are vendored by a distribution. Despite these benefits of FUSE, there is
a noticeable performance overhead associated with it. This is largely
due to the context switching between the kernel and the userspace that
occurs during its operation<span class="citation" data-cites="vangoor2017fuse">[22]</span>.</p>
<p>Today, FUSE is widely utilized to mount high-level external services
as file systems. For instance, it can be used to mount remote AWS S3
buckets with <code>s3fs</code><span class="citation" data-cites="gaul2023s3fs">[23]</span> or to mount a remote system’s disk
via Secure Shell (SSH) with SSHFS <span class="citation" data-cites="libfuse2022sshfs">[24]</span>.</p>
</section>
<section id="network-block-device-nbd" data-number="0.2.15">
<h3 data-number="1.2.15"><span class="header-section-number">1.2.15</span> Network Block Device
(NBD)</h3>
<p>Network Block Device (NBD) is a protocol for connecting to a remote
Linux block device. It typically works by communicating between a user
space-provided server and a Kernel-provided client. Though potentially
deployable over Wide Area Networks (WAN), it is primarily designed for
Local Area Networks (LAN) or localhost usage. The protocol is divided
into two phases: the handshake and the transmission<span class="citation" data-cites="blake2023nbd">[25]</span>.</p>
<p>TODO: Add sequence diagram of the NBD protocol</p>
<p>The NBD protocol involves multiple participants, notably one or
several clients, a server, and the concept of an export. It starts with
a client establishing a connection with the server. The server
reciprocates by delivering a greeting message highlighting various
server flags. The client responds by transmitting its own flags along
with the name of an export to use; a single NBD server can expose
multiple devices.</p>
<p>After receiving this, the server sends the size of the export and
other metadata. The client acknowledges this data, completing the
handshake. Post handshake, the client and server exchange commands and
replies. A command can correspond to any of the basic actions needed to
access a block device, for instance read, write or flush. These commands
might also contain data such as a chunk for writing, offsets, and
lengths among other elements. Replies may contain error messages,
success status, or data contingent on the reply type.</p>
<p>While powerful in many regards, NBD has some limitations. Its maximum
message size is capped at 32 MB<span class="citation" data-cites="clements2013nbd">[26]</span>, and the maximum block or chunk
size supported by the Kernel’s NBD client is a mere 4KB<span class="citation" data-cites="verhelst2023nbdclient">[27]</span>. Thus,
it might not be the most optimal protocol for WAN usage, especially in
scenarios with high latency.</p>
<p>NBD, being a protocol with a long legacy, comes with its own set of
operational quirks such as multiple different handshake versions and
legacy features. As a result, it is advisable to only implement the
latest recommended versions and the foundational feature set when
considering it NBD for a narrow usecase.</p>
<p>Despite the simplicity of the protocol, there are certain scenarios
where NBD falls short. Compared to FUSE, it has limitations when dealing
with backing devices that operate drastically different from
random-access storage devices like a tape drive, since it lacks the
ability to work with high-level abstractions such as files or
directories. For example, it does not support shared access to the same
file for multiple clients. However, this shortcoming can be considered
as an advantage for narrow usecases like memory synchronization, given
that it operates on a block level, where such features are not needed or
implemented at a higher layer.</p>
</section>
<section id="virtual-machine-live-migration" data-number="0.2.16">
<h3 data-number="1.2.16"><span class="header-section-number">1.2.16</span> Virtual Machine Live
Migration</h3>
<p>Virtual machine live migration involves the shifting of a virtual
machine, its state, and its connected devices from one host to another,
with the objective to minimize disrupted service by minimizing downtime
during data transfer processes.</p>
<p>Algorithms that intent to implement this usecase can be categorized
into two broad types: pre-copy migration and post-copy migration.</p>
<section id="pre-copy" data-number="0.2.16.1">
<h4 data-number="1.2.16.1"><span class="header-section-number">1.2.16.1</span> Pre-Copy</h4>
<p>The primary characteristic of pre-copy migration is its
“run-while-copy” nature, meaning that the copying of data from the
source to the destination occurs concurrently while the VM continues to
operate. This method is also applicable in a generic migration context
where an application or another data state is being updated.</p>
<p>In the case of a VM, the pre-copy migration procedure starts with
transfering the initial state of VM’s memory to the destination host.
During this operation, if modifications occur to any chunks of data,
they are flagged as “dirty”. These modified or “dirty” chunks of data
are then transferred to the destination until only a small number remain
- an amount small enough to stay within the allowable maximum downtime
criteria.</p>
<p>Following this, the VM is suspended at the source, enabling the
synchronization of the remaining chunks of data to the destination
without having to continue tracking dirty chunks. Once this
synchronization process is completed, the VM is resumed at the
destination host.</p>
<p>The pre-copy migration process is fairly robust, especially in
instances where there might be network disruption during
synchronization. This is because of fact that, at any given point during
migration, the VM is readily available in full either at the source or
the destination. A limitation to the approach however is that, if the VM
or application alters too many chunks on the source during migration, it
may not be possible to meet the maximum acceptable downtime criteria.
Maximum permissible downtime is also inherently restricted by the
available round-trip time (RTT)<span class="citation" data-cites="he2016migration">[28]</span>.</p>
</section>
<section id="post-copy" data-number="0.2.16.2">
<h4 data-number="1.2.16.2"><span class="header-section-number">1.2.16.2</span> Post-Copy</h4>
<p>Post-copy migration is an alternative live migration approach. While
pre-copy migration operates by copying data before the VM halt,
post-copy migration opts for another strategy: it immediately suspends
the VM operation on the source and resumes it on the destination – all
with only a minimal subset of the VM’s data.</p>
<p>During this resumed operation, whenever the VM attempts to access a
chunk of data not initially transferred during the move, a page fault
arises. A page fault, in this context, is a type of interrupt generated
when the VM tries to read or write a chunk that is not currently present
on the destination. This triggers the system to retrieve the missing
chunk from the source host, enabling the VM to continue its
operations<span class="citation" data-cites="he2016migration">[28]</span>.</p>
<p>The main advantage of post-copy migration centers around the fact
that it eliminates the necessity of re-transmitting chunks of “dirty” or
changed data before hitting the maximum tolerable downtime. This process
can thus decrease the necessary downtime and also reduces the amount of
network traffic between source and destination.</p>
<p>However, this approach is also not without its drawbacks. Post-copy
migration could potentially lead to extended migration times, as a
consequence of its “fetch-on-demand” model for retrieving chunks. This
model is highly sensitive to network latency and round-trip time (RTT).
Unlike the pre-copy model, this also means that the VM is not available
in full on either the source or the destination during migration,
requiring potential recovery solutions if network connectivity is lost
during the migration.</p>
</section>
<section id="workload-analysis" data-number="0.2.16.3">
<h4 data-number="1.2.16.3"><span class="header-section-number">1.2.16.3</span> Workload Analysis</h4>
<p>Recent studies have explored different strategies to determine the
most suitable timing for virtual machine migration. Even though these
mostly focus on virtual machines, the methodologies proposed could be
adapted for use with various other applications or migration
circumstances, too.</p>
<p>One method<span class="citation" data-cites="baruchi2015workload">[29]</span> proposed identifies
cyclical workload patterns of VMs and leverages this knowledge to delay
migration when it is beneficial. This is achieved by analyzing recurring
patterns that may unnecessarily postpone VM migration, and then
constructing a model of optimal cycles within which VMs can be migrated.
In the context of VM migration, such cycles could for example be
triggered by a large application’s garbage collector that results in
numerous changes to VM memory.</p>
<p>When migration is proposed, the system verifies whether it is in an
optimal cycle for migration. If it is, the migration proceeds; if not,
the migration is postponed until the next cycle. The proposed process
employs a Bayesian classifier to distinguish between favorable and
unfavorable cycles.</p>
<p>Compared to the popular alternative method which usually involves
waiting for a significant amount of unchanged chunks to synchronize
first, the proposed pattern recognition-based approach potentially
offers substantial improvements. The study found that this method
yielded an enhancement of up to 74% in terms of live migration
time/downtime and a 43% reduction concerning the volume of data
transferred over the network.</p>
</section>
</section>
<section id="streams-and-pipelines" data-number="0.2.17">
<h3 data-number="1.2.17"><span class="header-section-number">1.2.17</span> Streams and Pipelines</h3>
<p>Streams and pipelines are fundamental constructs in computer science,
enabling efficient, sequential processing of large datasets without the
need for loading an entire dataset into memory. They form the backbone
of modular and efficient data processing techniques, with each concept
having its unique characteristics and use cases.</p>
<p>A stream represents a continuous sequence of data, serving as a
connector between different points in a system. Streams can be either a
source or a destination for data. Examples include files, network
connections, and standard input/output devices and many others. The
power of streams comes from their ability to process data as it becomes
available; this aspect allows for minimization of memory consumption,
making streams particularly impactful for scenarios involving
long-running processes where data is streamed over extended periods of
time<span class="citation" data-cites="akidau2018streaming">[30]</span>.</p>
<p>Pipelines comprise a series of data processing stages, wherein the
output of one stage directly serves as the input to the next. It’s this
chain of processing stages that forms a “pipeline”. Often, these stages
can run concurrently; this parallel execution can result in a
significant performance improvement due to a higher degree of
concurrency.</p>
<p>One of the classic examples of pipelines is the instruction pipeline
in CPUs, where different stages of instruction execution - fetch,
decode, execute, and writeback - are performed in parallel. This design
increases the instruction throughput of the CPU, allowing it to process
multiple instructions simultaneously at different stages of the
pipeline.</p>
<p>Another familiar implementation is observed in UNIX pipes, a
fundamental part of shells such as GNU Bash or POSIX <code>sh</code>.
Here, the output of a command can be “piped” into another for further
processing; for instance, the results from a <code>curl</code> command
fetching data from an API could be piped into the <code>jq</code> tool
for JSON manipulation<span class="citation" data-cites="peek1994unix">[31]</span>.</p>
</section>
<section id="grpc" data-number="0.2.18">
<h3 data-number="1.2.18"><span class="header-section-number">1.2.18</span> gRPC</h3>
<p>gRPC is an open-source, high-performance remote procedure call (RPC)
framework developed by Google in 2015. It is recognized for its
cross-platform compatibility, supporting a variety of languages
including Go, Rust, JavaScript and more. gRPC is being maintained by the
Cloud Native Computing Foundation (CNCF), which ensures vendor
neutrality.</p>
<p>One of the notable features of the gRPC is its usage of HTTP/2 as the
transport protocol. This allows it to exploit features of HTTP/2 such as
header compression, which minimizes bandwidth usage, and request
multiplexing, enabling multiple requests to be sent concurrently over a
single connection. In addition to HTTP/2, gRPC utilizes Protocol Buffers
(protobuf) as the Interface Definition Language (IDL) and wire format.
Protobuf is a compact, high-performance, and language-neutral mechanism
for data serialization. This makes it preferable over the more dynamic,
but more verbose and slower JSON format often used in REST APIs.</p>
<p>One of the strengths of the gRPC framework is its support for various
types of RPCs. Not only does it support unary RPCs where the client
sends a single request to the server and receives a single response in
return, mirroring the functionality of a traditional function call, but
also server-streaming RPCs, wherein the client sends a request, and the
server responds with a stream of messages. Conversely, in
client-streaming RPCs, the client sends a stream of messages to a server
in response to a request. It also supports bidirectional RPCs, wherein
both client and server can send messages to each other.</p>
<p>What distinguishes gRPC is its pluggable structure that allows for
added functionalities such as load balancing, tracing, health checking,
and authentication, which make it a comprehensive solution for
developing distributed systems<span class="citation" data-cites="google2023grpc">[32]</span>.</p>
</section>
<section id="redis" data-number="0.2.19">
<h3 data-number="1.2.19"><span class="header-section-number">1.2.19</span> Redis</h3>
<p>Redis (Remote Dictionary Server) is an in-memory data structure
store, primarily utilized as an ephemeral database, cache, and message
broker introduced by Salvatore Sanfilippo in 2009. Compared to other
key-value stores and NoSQL databases, Redis supports a multitude of data
structures, including lists, sets, hashes, and bitmaps, making it a good
choice for caching or storing data that does not fit well into a
traditional SQL architecture<span class="citation" data-cites="redis2023introduction">[33]</span>.</p>
<p>One of the primary reasons for Redis’s speed is its reliance on
in-memory data storage rather than on disk, enabling very low-latency
reads and writes. While the primary usecase of Redis is in in-memory
operations, it also supports persistence by flushing data to disk. This
feature broadens the use cases for Redis, allowing it to handle
applications that require longer-term data storage in addition to a
caching mechanism. In addition to it being mostly in-memory, Redis also
supports quick concurrent reads/writes thanks to its non-blocking I/O
model, making it a good choice for systems that require the store to be
available to many workers or clients.</p>
<p>Redis also includes a publish-subscribe (pub-sub) system. This
enables it to function as a message broker, where messages are published
to channels and delivered to all the subscribers interested in those
channels. This makes it a particularly compelling choice for systems
that require both caching and a memory broker, such as queue
systems<span class="citation" data-cites="redis2023pubsub">[34]</span>.</p>
</section>
<section id="s3-and-minio" data-number="0.2.20">
<h3 data-number="1.2.20"><span class="header-section-number">1.2.20</span> S3 and Minio</h3>
<p>S3 is a scalable object storage service, especially designed for
large-scale applications with frequent reads and writes. It is one of
the prominent services offered by Amazon Web Services. S3’s design
allows for global distribution, which means the data can be stored
across multiple geographically diverse servers. This permits fast access
times from virtually any location on the globe, crucial for globally
distributed services or applications with users spread across different
continents.</p>
<p>S3 offers a variety of storage classes for to different needs,
i.e. for whether the requirement is for frequent data access, infrequent
data retrieval, or long-term archival. This ensures that it can meet a
wide array of demands through the same API. S3 also comes equipped with
comprehensive security features, including authentication and
authorization mechanisms.</p>
<p>Communication with S3 is done through a HTTP API. Users and
applications can interact with the stored data - including files and
folders - via this API.<span class="citation" data-cites="aws2023s3">[35]</span>.</p>
<p>Minio is an open-source storage server that is compatible Amazon S3’s
API. Due to it being written in the Go programming language, Minio is
very lightweight and even ships as single static binary. Unlike with AWS
S3, which is only offered as a service, Minio’s open-source nature means
that users have the ability to view, modify, and distribute Minio’s
source code, allowing community-driven development and innovation.</p>
<p>A critical distinction of Minio is its suitability for on-premises
hosting, making it a good fit for organizations with specific security
regulations, those preferring to maintain direct control over their data
and developers prefering to work on the local system. It also supports
horizontal scalability, designed to distribute large quantities of data
across multiple nodes, meaning that it can be used in large-scale
deployments similarly to AWS S3<span class="citation" data-cites="minio2023coreadmin">[36]</span>.</p>
</section>
<section id="cassandra-and-scyllladb" data-number="0.2.21">
<h3 data-number="1.2.21"><span class="header-section-number">1.2.21</span> Cassandra and ScylllaDB</h3>
<p>Apache Cassandra is a wide-column NoSQL database tailored for
large-scale, distributed data management tasks. It blends the
distributed nature of Amazon’s Dynamo model with the structure of
Google’s Bigtable model, leading to a highly available database system.
It is known for its scalability, designed to handle vast amounts of data
spread across numerous servers. Unique to Cassandra is the absence of a
single point of failure, thus ensuring continuous availability and
robustness, which is critical for systems requiring high uptime.</p>
<p>Cassandra’s consistency model is tunable according to needs, ranging
from eventual to strong consistency. It distinguishes itself by not
employing master nodes due to its usage of a peer-to-peer protocol and a
distributed hash ring design. These design choices eradicate the
bottleneck and failure risks associated with master nodes<span class="citation" data-cites="lakshman2010cassandra">[37]</span>.</p>
<p>Despite these robust capabilities, Cassandra does come with certain
limitations. Under heavy load, it experiences high latency that can
negatively affect system performance. Besides this, it also demands
complex configuration and fine-tuning to peform optimally.</p>
<p>In response to the perceived shortcomings of Cassandra, ScyllaDB was
launched in 2015. It shares design principles with Cassandra, such as
compatibility with Cassandra’s API and data model, but has architectural
differences intended to overcome Cassandra’s limitations. It’s primarily
written in C++, contrary to Cassandra’s Java-based code. This
contributes to ScyllaDB’s shared-nothing architecture, a design that
aims to minimize contention and enhance performance.</p>
<p>ScyllaDB was particularly engineered to address one shortcoming of
Cassandra - issues around latency, specifically the 99th percentile
latency that impacts system reliability and predictability. ScyllaDB’s
design improvements and performance gains over Cassandra have been
endorsed by various benchmarking studies<span class="citation" data-cites="grabowski2021scylladb">[38]</span>.</p>
<p>TODO: Add graph of the Cassandra vs. ScyllaDB benchmark from the
benchmarking study</p>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-linux2023docs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">T.
kernel development community, <span>“Quick start.”</span> <a href="https://www.kernel.org/doc/html/next/rust/quick-start.html" class="uri">https://www.kernel.org/doc/html/next/rust/quick-start.html</a>,
2023.</div>
</div>
<div id="ref-love2010linux" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">R.
Love, <em>Linux kernel development</em>, 3rd ed. Pearson Education,
Inc., 2010.</div>
</div>
<div id="ref-maurer2008professional" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">W.
Mauerer, <em>Professional linux kernel architecture</em>. Indianapolis,
IN: Wiley Publishing, Inc., 2008.</div>
</div>
<div id="ref-stevens2000advanced" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">W.
R. Stevens, <em>Advanced programming in the UNIX environment</em>.
Delhi: Addison Wesley Logman (Singapore) Pte Ltd., Indian Branch,
2000.</div>
</div>
<div id="ref-robbins2003unix" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">K.
A. Robbins and S. Robbins, <em>Unix™ systems programming: Communication,
concurrency, and threads</em>. Prentice Hall PTR, 2003.</div>
</div>
<div id="ref-stallings2010architecture" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">W.
Stallings, <em>Computer organization and architecture: Designing for
performance</em>. Upper Saddle River, New Jersey, 07458: Pearson
Education, Inc., 2010.</div>
</div>
<div id="ref-smith1982cache" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">A.
J. Smith, <span>“Cache memories,”</span> <em>ACM Comput. Surv.</em>,
vol. 14, no. 3, pp. 473–530, Sep. 1982, doi: <a href="https://doi.org/10.1145/356887.356892">10.1145/356887.356892</a>.</div>
</div>
<div id="ref-barr2021offline" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">J.
Barr, <span>“New - offline tape migration using AWS snowball
edge.”</span> <a href="https://aws.amazon.com/blogs/aws/new-offline-tape-migration-using-aws-snowball-edge/" class="uri">https://aws.amazon.com/blogs/aws/new-offline-tape-migration-using-aws-snowball-edge/</a>,
2021.</div>
</div>
<div id="ref-maruf2023memory" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">H.
A. Maruf and M. Chowdhury, <span>“Memory disaggregation: Advances and
open challenges.”</span> 2023.Available: <a href="https://arxiv.org/abs/2305.03943">https://arxiv.org/abs/2305.03943</a></div>
</div>
<div id="ref-bonwick1994slaballoc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">J.
Bonwick, <span>“The slab allocator: An <span>Object-Caching</span>
kernel,”</span> Jun. 1994.Available: <a href="https://www.usenix.org/conference/usenix-summer-1994-technical-conference/slab-allocator-object-caching-kernel">https://www.usenix.org/conference/usenix-summer-1994-technical-conference/slab-allocator-object-caching-kernel</a></div>
</div>
<div id="ref-gorman2004linuxmem" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">M.
Gorman, <em>Understanding the linux virtual memory manager</em>. Upper
Saddle River, New Jersey 07458: Pearson Education, Inc. Publishing as
Prentice Hall Professional Technical Reference, 2004.</div>
</div>
<div id="ref-kernel2023suspend" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">T.
K. D. Community, <span>“Swap suspend,”</span> 2023. <a href="https://www.kernel.org/doc/html/latest/power/swsusp.html">https://www.kernel.org/doc/html/latest/power/swsusp.html</a>
(accessed Jul. 19, 2023).</div>
</div>
<div id="ref-silberschatz2018operating" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">A.
Silberschatz, P. B. Galvin, and G. Gagne, <em>Operating system
concepts</em>, 10th ed. Hoboken, NJ: Wiley, 2018.Available: <a href="https://lccn.loc.gov/2017043464">https://lccn.loc.gov/2017043464</a></div>
</div>
<div id="ref-choi2017mmap" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">J.
Choi, J. Kim, and H. Han, <span>“Efficient memory mapped file
<span>I/O</span> for <span>In-Memory</span> file systems,”</span> Jul.
2017.Available: <a href="https://www.usenix.org/conference/hotstorage17/program/presentation/choi">https://www.usenix.org/conference/hotstorage17/program/presentation/choi</a></div>
</div>
<div id="ref-prokop2010inotify" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">M.
Prokop, <span>“Inotify: Efficient, real-time linux file system event
monitoring,”</span> Apr. 2010. <a href="https://www.infoq.com/articles/inotify-linux-file-system-event-monitoring/">https://www.infoq.com/articles/inotify-linux-file-system-event-monitoring/</a></div>
</div>
<div id="ref-postel1981tcp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline"><span>“<span>Transmission Control
Protocol</span>.”</span> RFC 793; J. Postel, Sep. 1981. doi: <a href="https://doi.org/10.17487/RFC0793">10.17487/RFC0793</a>.</div>
</div>
<div id="ref-postel1980udp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline"><span>“<span>User Datagram
Protocol</span>.”</span> RFC 768; J. Postel, Aug. 1980. doi: <a href="https://doi.org/10.17487/RFC0768">10.17487/RFC0768</a>.</div>
</div>
<div id="ref-rfc2021quic" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">J.
Iyengar and M. Thomson, <span>“<span class="nocase">QUIC: A UDP-Based
Multiplexed and Secure Transport</span>.”</span> RFC 9000; RFC Editor,
May 2021. doi: <a href="https://doi.org/10.17487/RFC9000">10.17487/RFC9000</a>.</div>
</div>
<div id="ref-langley2017quic" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">A.
Langley <em>et al.</em>, <span>“The QUIC transport protocol: Design and
internet-scale deployment,”</span> in <em>Proceedings of the conference
of the ACM special interest group on data communication</em>, 2017, pp.
183–196. doi: <a href="https://doi.org/10.1145/3098822.3098842">10.1145/3098822.3098842</a>.</div>
</div>
<div id="ref-xiao2018rsync" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">H.
Xiao <em>et al.</em>, <span>“Towards web-based delta synchronization for
cloud storage services,”</span> in <em>16th USENIX conference on file
and storage technologies (FAST 18)</em>, Feb. 2018, pp.
155–168.Available: <a href="https://www.usenix.org/conference/fast18/presentation/xiao">https://www.usenix.org/conference/fast18/presentation/xiao</a></div>
</div>
<div id="ref-libfuse2020example" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">T.
libfuse authors, <span>“FUSE minimal example filesystem using high-level
API.”</span> <a href="https://github.com/libfuse/libfuse/blob/master/example/hello.c" class="uri">https://github.com/libfuse/libfuse/blob/master/example/hello.c</a>,
2020.</div>
</div>
<div id="ref-vangoor2017fuse" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">B.
K. R. Vangoor, V. Tarasov, and E. Zadok, <span>“To <span>FUSE</span> or
not to <span>FUSE</span>: Performance of <span>User-Space</span> file
systems,”</span> in <em>15th USENIX conference on file and storage
technologies (FAST 17)</em>, Feb. 2017, pp. 59–72.Available: <a href="https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor">https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor</a></div>
</div>
<div id="ref-gaul2023s3fs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">A.
Gaul, T. Nakatani, and @rrizun, <span>“s3fs: FUSE-based file system
backed by amazon S3.”</span> <a href="https://github.com/s3fs-fuse/s3fs-fuse" class="uri">https://github.com/s3fs-fuse/s3fs-fuse</a>, 2023.</div>
</div>
<div id="ref-libfuse2022sshfs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">T.
libfuse authors, <span>“SSHFS: A network filesystem client to connect to
SSH servers.”</span> <a href="https://github.com/libfuse/sshfs" class="uri">https://github.com/libfuse/sshfs</a>, 2022.</div>
</div>
<div id="ref-blake2023nbd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">E.
Blake, W. Verhelst, and other NBD maintainers, <span>“The NBD
protocol.”</span> <a href="https://github.com/NetworkBlockDevice/nbd/blob/master/doc/proto.md" class="uri">https://github.com/NetworkBlockDevice/nbd/blob/master/doc/proto.md</a>,
Apr. 2023.</div>
</div>
<div id="ref-clements2013nbd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">P.
Clements, <span>“[PATCH] nbd: Increase default and max request
sizes.”</span> <a href="https://lore.kernel.org/lkml/20130402194120.54043222C0@clements/" class="uri">https://lore.kernel.org/lkml/20130402194120.54043222C0@clements/</a>,
Apr. 02, 2013.</div>
</div>
<div id="ref-verhelst2023nbdclient" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">W.
Verhelst, <em>Nbd-client man page</em>. 2023.Available: <a href="https://manpages.ubuntu.com/manpages/lunar/en/man8/nbd-client.8.html">https://manpages.ubuntu.com/manpages/lunar/en/man8/nbd-client.8.html</a></div>
</div>
<div id="ref-he2016migration" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">S.
He, C. Hu, B. Shi, T. Wo, and B. Li, <span>“Optimizing virtual machine
live migration without shared storage in hybrid clouds,”</span> in
<em>2016 IEEE 18th international conference on high performance
computing and communications; IEEE 14th international conference on
smart city; IEEE 2nd international conference on data science and
systems (HPCC/SmartCity/DSS)</em>, 2016, pp. 921–928. doi: <a href="https://doi.org/10.1109/HPCC-SmartCity-DSS.2016.0132">10.1109/HPCC-SmartCity-DSS.2016.0132</a>.</div>
</div>
<div id="ref-baruchi2015workload" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">A.
Baruchi, E. Toshimi Midorikawa, and L. Matsumoto Sato, <span>“Reducing
virtual machine live migration overhead via workload analysis,”</span>
<em>IEEE Latin America Transactions</em>, vol. 13, no. 4, pp. 1178–1186,
2015, doi: <a href="https://doi.org/10.1109/TLA.2015.7106373">10.1109/TLA.2015.7106373</a>.</div>
</div>
<div id="ref-akidau2018streaming" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">T.
Akidau, S. Chernyak, and R. Lax, <em>Streaming systems</em>. Sebastopol,
CA: O’Reilly Media, Inc., 2018.</div>
</div>
<div id="ref-peek1994unix" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">J.
D. Peek, <em>UNIX power tools</em>. Sebastopol, CA; New York: O’Reilly
Associates; Bantam Books, 1994.</div>
</div>
<div id="ref-google2023grpc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">gRPC Authors, <span>“Introduction to
gRPC.”</span> 2023.Available: <a href="https://grpc.io/docs/what-is-grpc/introduction/">https://grpc.io/docs/what-is-grpc/introduction/</a></div>
</div>
<div id="ref-redis2023introduction" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">Redis Ltd, <span>“Introduction to
redis.”</span> <a href="https://redis.io/docs/about/" class="uri">https://redis.io/docs/about/</a>, 2023.</div>
</div>
<div id="ref-redis2023pubsub" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">Redis Ltd, <span>“Redis pub/sub.”</span> <a href="https://redis.io/docs/interact/pubsub/" class="uri">https://redis.io/docs/interact/pubsub/</a>, 2023.</div>
</div>
<div id="ref-aws2023s3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">Amazon Web Services, Inc, <span>“What is amazon
S3?”</span> <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html" class="uri">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html</a>,
2023.</div>
</div>
<div id="ref-minio2023coreadmin" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">MinIO, Inc, <span>“Core administration
concepts.”</span> <a href="https://min.io/docs/minio/kubernetes/upstream/administration/concepts.html" class="uri">https://min.io/docs/minio/kubernetes/upstream/administration/concepts.html</a>,
2023.</div>
</div>
<div id="ref-lakshman2010cassandra" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">A.
Lakshman and P. Malik, <span>“Cassandra: A decentralized structured
storage system,”</span> <em>SIGOPS Oper. Syst. Rev.</em>, vol. 44, no.
2, pp. 35–40, Apr. 2010, doi: <a href="https://doi.org/10.1145/1773912.1773922">10.1145/1773912.1773922</a>.</div>
</div>
<div id="ref-grabowski2021scylladb" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">P.
Grabowski, J. Stasiewicz, and K. Baryla, <span>“Apache cassandra 4.0
performance benchmark: Comparing cassandra 4.0, cassandra 3.11 and
scylla open source 4.4,”</span> ScyllaDB Inc, 2021.Available: <a href="https://www.scylladb.com/wp-content/uploads/wp-apache-cassandra-4-performance-benchmark-3.pdf">https://www.scylladb.com/wp-content/uploads/wp-apache-cassandra-4-performance-benchmark-3.pdf</a></div>
</div>
</div>
</section>
</section>
</body>
</html>
