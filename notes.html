<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Felicitas Pojtinger (Stuttgart Media University)" />
  <meta name="dcterms.date" content="2023-08-04" />
  <meta name="keywords" content="linux memory
management, userfaultfd, mmap, inotify, hash-based change
detection, delta synchronization, msync, custom filesystem, nbd
protocol, performance evaluation" />
  <title>Efficient Synchronization of Linux Memory Regions over a Network: A Comparative Study and Implementation (Notes)</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Efficient Synchronization of Linux Memory Regions over
a Network: A Comparative Study and Implementation (Notes)</h1>
<p class="subtitle">A user-friendly approach to application-agnostic
state synchronization</p>
<p class="author">Felicitas Pojtinger (Stuttgart Media University)</p>
<p class="date">2023-08-04</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#rough-structure"><span class="toc-section-number">1.1</span> Rough Structure</a></li>
<li><a href="#sectionsresearch-questionsideas-brainstorming"><span class="toc-section-number">1.2</span> Sections/Research Questions/Ideas
Brainstorming</a></li>
<li><a href="#alternative-outline"><span class="toc-section-number">1.3</span> Alternative Outline</a></li>
<li><a href="#story"><span class="toc-section-number">1.4</span>
Story</a></li>
<li><a href="#revised-structure"><span class="toc-section-number">1.5</span> Revised Structure</a></li>
</ul>
</nav>
<section id="rough-structure" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Rough Structure</h2>
<ul>
<li>Abstract: A comparative analysis and implementation of various
methods for synchronizing Linux memory options over a network</li>
<li>Introduction
<ul>
<li>Examining Linux’s memory management and relevant APIs</li>
<li>Use cases for memory region synchronization</li>
</ul></li>
<li>Option 1: Handling page faults in userspace with
<code>userfaultfd</code>
<ul>
<li>Introduction to userfaultfd</li>
<li>Implementing userfaultfd handlers and registration in Go</li>
<li>Transferring sockets between processes</li>
<li>Examples of handler and registration interfaces (byte slice, file,
S3 object)</li>
<li>Performance assessment of this approach</li>
</ul></li>
<li>Option 2: Utilizing <code>mmap</code> for change notifications
<ul>
<li>Concept: <code>mmap</code> a memory region with MMAP_SHARED to track
changes in a file</li>
<li>Method 1 for detecting file changes: inotify</li>
<li>Limitations: <code>mmap</code> does not generate WRITE events</li>
</ul></li>
<li>Option 3: Hash-based change detection
<ul>
<li>Comparing hashes of local and remote <code>mmap</code>ed
regions</li>
<li>Evaluation of hashing algorithms</li>
<li>Introduction to delta synchronization (e.g., rsync)</li>
<li>Custom protocol for delta synchronization</li>
<li>Multiplexing synchronization streams</li>
<li>The function of <code>msync</code></li>
<li>Performance assessment of this approach</li>
</ul></li>
<li>Option 4: Detecting changes with a custom filesystem implementation
<ul>
<li>Intercepting writes to the <code>mmap</code>ed region using a custom
filesystem</li>
<li>Exploring methods for creating a new, custom Linux filesystem
<ul>
<li>In the kernel</li>
<li>NBD</li>
<li>CUSE</li>
<li>BUSE</li>
<li>FUSE</li>
<li>Upcoming options (ublk, etc.)</li>
</ul></li>
<li>Detailed analysis of the NBD protocol (client &amp; server)</li>
<li>Implementing the client and server in Go based on the protocol</li>
<li>Server backend interface and example implementations</li>
<li>Asynchronous writeback protocol and caching mechanism</li>
<li>Performance assessment of this approach</li>
</ul></li>
<li>Summary:
<ul>
<li>Comparing options in terms of ease of implementation, CPU load, and
network traffic</li>
<li>Identifying the optimal solution for specific use cases: data change
frequency, kernel/OS compatibility, etc.</li>
</ul></li>
</ul>
</section>
<section id="sectionsresearch-questionsideas-brainstorming" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Sections/Research Questions/Ideas Brainstorming</h2>
<ul>
<li>Usecases: Direct Mount vs. Managed Mount vs. Migration</li>
<li>Effects of high latency on different pull methods (esp. direct
vs. managed)</li>
<li>Effects of slow local disks or RAM on pull methods</li>
<li>The asynchronous background push method (for mounts); how chunks are
marked as dirty when they are being written to before the download has
finished completely</li>
<li>Mount backend API vs. seeder API</li>
<li>Preemptive pulls and parallelized startups (n MB saved)</li>
<li>Background pulling system and interface (rwat), % of
availability</li>
<li>Chunking system/non-aligned reads and writes, checking for correct
chunking behavior</li>
<li>Local vs. remote chunking</li>
<li>Backend implementations, performance and usecases: File, memory,
directory, dudirekta, gRPC, fRPC, Redis, S3, Cassandra</li>
<li>Transport layers: Dudirekta, gRPC, fRPC (esp. benefits and problems
with concurrent RPCs, connection pooling like with dRPC, benchmarks with
latency and throughput etc.)</li>
<li>NBD protocol overview and limitations</li>
<li>NBD protocol phases</li>
<li>Minimal viable NBD protocol needs</li>
<li>Go NBD server implementation: Multiple clients, error handling</li>
<li>Go NBD client and server implementation: The kernel’s NBD client,
CGo for <code>ioctl</code> numbers</li>
<li>Finding unused NBD devices, detecting client availability (polling
sysfs vs. udev; add benchmarks)</li>
<li>go-nbd pluggable backend API design/interface</li>
<li>go-nbd project scope &amp; keeping it maintainable, esp. vs other
NBD implementations</li>
<li>Migration API lifecycle &amp; lockable rwats</li>
<li>Path vs. file vs. slice mounts/migrations</li>
<li>Migration protocol actors, phases and state machine</li>
<li>Managed mount protocol actors, phases, sequence and state
machine</li>
<li>Performance tuning parameters (chunk size, push/pull workers)</li>
<li>P2P vs. relay systems/hub and spoke systems</li>
<li>Pull priority function/heuristic: Benchmarks when accessing from end
of file to start vs. other way around, latency vs. throughput changes
with/without heuristic, using LLMs etc. to analyze access patterns with
<code>pullWorkers: 0</code> and then generating an automatic pull
heuristic</li>
<li>Performance of different hashing algorithms for detecting changes to
a <code>mmap</code>ed region</li>
<li>Usage of the r3map’s API vs. e.g. “Remote regions” paper</li>
<li>Tapisk as an example of using the mount APIs for a filesystem;
esp. with support for writebacks in the future, and comparing this to
STFS</li>
<li><code>ram-dl</code> as an example of using a remote backend to
provide more RAM/Swap</li>
<li>Migrating app state (e.g. a TODO list) between two hosts</li>
<li>Minimum acceptable downtime</li>
<li>Concurrent access/consistency guarantees for mounts vs. migrations
etc. - <code>Track()</code>, why we can’t modify a mount’s source</li>
<li>Usage of QUIC, UDP and other protocols for skipping on RTT to
improve minimal latency</li>
<li>Limitations and benefits of <code>mmap</code> for accessing a mount
vs. a file (concurrent reads/writes etc.)</li>
<li>Criticality in protocols (e.g. recovering from a network outage in
mount vs. migration, finalization step can’t be aborted etc.)</li>
<li>Encryption of regions and the wire protocol, authn, DoS
vulnerabilities</li>
<li>Integration with existing app migration systems</li>
<li>When to best <code>Finalize()</code> a migration; analyzing app
usage patterns?</li>
<li>How does Linux actually manage memory, O_DIRECT vs
<code>mmap</code>, RAM vs Swap etc.</li>
<li>userfaultfd is read-only</li>
<li>userfaultfd can only be used to fetch the first (missing) chunk, not
subsequent ones</li>
<li>userfaultfd is limited to ~50MB/s of throughput</li>
<li>Biggest benefit of userfaultfd: It has minimal registration overhead
&amp; latency</li>
<li>userfaultfd’s interface is just an io.ReaderAt</li>
<li>Backends can use custom indexes to map linear media (e.g. tape
drives) into memory by mapping the block device offset to a real,
append-only record number and swapping it out for a new one when things
get overwritten in the block device</li>
</ul>
</section>
<section id="alternative-outline" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span>
Alternative Outline</h2>
<ol type="1">
<li>Abstract
<ul>
<li>A comparative analysis and implementation of various methods for
synchronizing Linux memory options over a network</li>
</ul></li>
<li>Introduction
<ol type="1">
<li>Background: Examining Linux’s memory management and relevant
APIs</li>
<li>Purpose: Use cases for memory region synchronization (Direct Mount,
Managed Mount, Migration)
<ul>
<li>Discuss potential effects of high latency, slow local disks or RAM
on different pull methods</li>
</ul></li>
</ol></li>
<li>Methodologies
<ol type="1">
<li>Option 1: Handling page faults in userspace with
<code>userfaultfd</code>
<ol type="1">
<li>Explanation of userfaultfd and its implementation</li>
<li>Description of userfaultfd handlers and registration in Go</li>
<li>The process of transferring sockets between processes</li>
<li>Examples of handler and registration interfaces</li>
<li>Performance assessment of this approach, with focus on pull methods
and effects of system constraints</li>
</ol></li>
<li>Option 2: Utilizing <code>mmap</code> for change notifications
<ol type="1">
<li>Understanding mmap and its application</li>
<li>Detecting file changes using inotify and its limitations</li>
<li>Performance evaluation of this approach, with focus on preemptive
pulls and parallelized startups</li>
</ol></li>
<li>Option 3: Hash-based change detection
<ol type="1">
<li>Comparing hashes of local and remote <code>mmap</code>ed
regions</li>
<li>Evaluation of different hashing algorithms</li>
<li>Introduction to delta synchronization and the role of rsync</li>
<li>Custom protocol for delta synchronization</li>
<li>Multiplexing synchronization streams and the function of
<code>msync</code></li>
<li>Performance assessment of this approach, with an analysis of the
effects of various pull priority functions/heuristics</li>
</ol></li>
<li>Option 4: Detecting changes with a custom filesystem implementation
<ol type="1">
<li>Intercepting writes to the <code>mmap</code>ed region using a custom
filesystem</li>
<li>Understanding methods for creating a new, custom Linux filesystem
(in the kernel, NBD, CUSE, BUSE, FUSE, etc.)</li>
<li>Performance assessment of this approach</li>
</ol></li>
<li>Option 5: Block device with userspace backend
<ol type="1">
<li>Detailed analysis of the NBD protocol and its implementation in
Go</li>
<li>Server backend interface and example implementations</li>
<li>Asynchronous writeback protocol and caching mechanism</li>
<li>Performance assessment of this approach, with an analysis of Go NBD
client and server implementation and the project scope</li>
</ol></li>
</ol></li>
<li>Evaluation and Comparison of Approaches
<ol type="1">
<li>Comparing options in terms of ease of implementation, CPU load, and
network traffic</li>
<li>Identifying the optimal solution for specific use cases: data change
frequency, kernel/OS compatibility, etc.</li>
<li>Analysis of Go NBD client and server implementation and the project
scope</li>
<li>Examination of different transport layers and their implications on
performance and concurrency</li>
<li>Discussion on migration protocol actors, phases and state
machine</li>
<li>Performance tuning parameters (chunk size, push/pull workers)</li>
<li>Examination of P2P vs. relay systems/hub and spoke systems</li>
</ol></li>
<li>Case Studies
<ol type="1">
<li>Tapisk as an example of using the mount APIs for a filesystem,
esp. one with very low read/write speeds and high latency</li>
<li><code>ram-dl</code> as an example of using a remote backend to
provide more RAM/Swap</li>
<li>Migrating app state (e.g. a TODO list) between two hosts in a
universal (byte-slice/by using the underlying memory) manner</li>
<li>Mounting remote filesystems and combining the benefits of
traditional FUSE-based mounts (e.g. s3-fuse) with Google
Drive/Nextcloud-style synchronization</li>
<li>Using mounts for SQLite etc. database access without having to use
range requests</li>
<li>Streaming video using formats that usually don’t support streaming,
e.g. MP4, where an index is required</li>
<li>Improving game download speeds by mounting the remote assets with
managed mounts, using a pull heuristic that defines typical access
patterns, e.g. by levels (making the game immediately playable)</li>
<li>Executing remote binaries or scripts that don’t need to be scanned
first without fully downloading them</li>
</ol></li>
<li>Conclusion
<ol type="1">
<li>Summary of findings and optimal solutions for specific use
cases</li>
<li>Discussion of minimum acceptable downtime</li>
<li>Future recommendations for research and improvements</li>
</ol></li>
</ol>
</section>
<section id="story" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span>
Story</h2>
<ul>
<li>Introduction: How does memory in Linux work? Paging, swap etc.</li>
<li>An introduction to mmap and how we can use it to map a file into
memory/a byte slice, the role of <code>msync</code></li>
<li>Implementing push-based memory sync by tracking changes to a
<code>mmap</code>ed slice with polled hashing of individual chunks, why
we can’t use <code>inotify</code>, and the CPU-bound limitations of this
approach</li>
<li>Implementing pull-based memory sync with <code>userfaultfd</code>;
and implementation and throughput limitations</li>
<li>Implementing push-pull based memory sync with a FUSE; limitations
and complexity (citing STFS)</li>
<li>Implementing push-pull based memory sync with NBD; implementation of
go-nbd</li>
<li>Using NBD directly as a mount-based sync system with the direct
mount API; limitations with latency etc., and improvements with
background pulls and pushes, different backends etc., the mount wire
protocol, pull heuristics</li>
<li>Taking inspiration from VM live migration and adding a migration
system for memory sync, two-phase commit, the sync wire protocol,
minimum acceptable downtime as the metric to optimize for</li>
<li>Use cases, case studies and comparison of approaches, finding the
one that is right for each one, and showing an implementation for each,
benchmarks, performance tuning</li>
<li>Conclusion with a summary of the different approaches, further
research recommendations</li>
</ul>
</section>
<section id="revised-structure" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span>
Revised Structure</h2>
<ol type="1">
<li>Introduction</li>
<li>Synchronization Strategies</li>
<li>Case Studies</li>
<li>Conclusion</li>
</ol>
</section>
</body>
</html>
