<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Felicitas Pojtinger (Stuttgart Media University)" />
  <meta name="dcterms.date" content="2023-08-04" />
  <meta name="keywords" content="linux memory
management, userfaultfd, mmap, inotify, hash-based change
detection, delta synchronization, msync, custom filesystem, nbd
protocol, performance evaluation" />
  <title>Efficient Synchronization of Linux Memory Regions over a Network: A Comparative Study and Implementation (Notes)</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Efficient Synchronization of Linux Memory Regions over
a Network: A Comparative Study and Implementation (Notes)</h1>
<p class="subtitle">A user-friendly approach to application-agnostic
state synchronization</p>
<p class="author">Felicitas Pojtinger (Stuttgart Media University)</p>
<p class="date">2023-08-04</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<h2 id="section" class="unnumbered unlisted"></h2>
<p>This study presents a comprehensive comparison and implementation of
various methods for synchronizing memory regions in Linux systems over a
network. Four approaches are evaluated: (1) handling page faults in
userspace with <code>userfaultfd</code>, (2) utilizing <code>mmap</code>
for change notifications, (3) hash-based change detection, and (4)
custom filesystem implementation. Each option is thoroughly examined in
terms of implementation, performance, and associated trade-offs. The
study culminates in a summary that compares the options based on ease of
implementation, CPU load, and network traffic, and offers
recommendations for the optimal solution depending on the specific use
case, such as data change frequency and kernel/OS compatibility.</p>
</div>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1.1</span>
Introduction</a></li>
<li><a href="#technology"><span class="toc-section-number">1.2</span>
Technology</a>
<ul>
<li><a href="#the-linux-kernel"><span class="toc-section-number">1.2.1</span> The Linux Kernel</a></li>
<li><a href="#linux-kernel-modules"><span class="toc-section-number">1.2.2</span> Linux Kernel Modules</a></li>
<li><a href="#unix-signals-and-handlers"><span class="toc-section-number">1.2.3</span> UNIX Signals and
Handlers</a></li>
<li><a href="#memory-hierarchy"><span class="toc-section-number">1.2.4</span> Memory Hierarchy</a></li>
<li><a href="#memory-management-in-linux"><span class="toc-section-number">1.2.5</span> Memory Management in
Linux</a></li>
<li><a href="#swap-space"><span class="toc-section-number">1.2.6</span>
Swap Space</a></li>
<li><a href="#page-faults"><span class="toc-section-number">1.2.7</span>
Page Faults</a></li>
<li><a href="#mmap"><span class="toc-section-number">1.2.8</span>
<code>mmap</code></a></li>
<li><a href="#inotify"><span class="toc-section-number">1.2.9</span>
<code>inotify</code></a></li>
<li><a href="#linux-kernel-disk-and-file-caching"><span class="toc-section-number">1.2.10</span> Linux Kernel Disk and File
Caching</a></li>
<li><a href="#tcp-udp-and-quic"><span class="toc-section-number">1.2.11</span> TCP, UDP and QUIC</a></li>
<li><a href="#delta-synchronization"><span class="toc-section-number">1.2.12</span> Delta Synchronization</a></li>
<li><a href="#file-systems-in-userspace-fuse"><span class="toc-section-number">1.2.13</span> File Systems In Userspace
(FUSE)</a></li>
<li><a href="#network-block-device-nbd"><span class="toc-section-number">1.2.14</span> Network Block Device
(NBD)</a></li>
<li><a href="#virtual-machine-live-migration"><span class="toc-section-number">1.2.15</span> Virtual Machine Live
Migration</a></li>
<li><a href="#streams-and-pipelines"><span class="toc-section-number">1.2.16</span> Streams and Pipelines</a></li>
<li><a href="#grpc"><span class="toc-section-number">1.2.17</span>
gRPC</a></li>
<li><a href="#redis"><span class="toc-section-number">1.2.18</span>
Redis</a></li>
<li><a href="#s3-and-minio"><span class="toc-section-number">1.2.19</span> S3 and Minio</a></li>
<li><a href="#cassandra-and-scyllladb"><span class="toc-section-number">1.2.20</span> Cassandra and
ScylllaDB</a></li>
</ul></li>
<li><a href="#planning"><span class="toc-section-number">1.3</span>
Planning</a>
<ul>
<li><a href="#pull-based-synchronization-with-userfaultfd"><span class="toc-section-number">1.3.1</span> Pull-Based Synchronization With
<code>userfaultfd</code></a></li>
<li><a href="#push-based-synchronization-with-mmap-and-hashing"><span class="toc-section-number">1.3.2</span> Push-Based Synchronization With
<code>mmap</code> and Hashing</a></li>
<li><a href="#push-pull-synchronization-with-fuse"><span class="toc-section-number">1.3.3</span> Push-Pull Synchronization with
FUSE</a></li>
<li><a href="#mounts-with-nbd"><span class="toc-section-number">1.3.4</span> Mounts with NBD</a></li>
<li><a href="#push-pull-synchronization-with-mounts"><span class="toc-section-number">1.3.5</span> Push-Pull Synchronization with
Mounts</a></li>
<li><a href="#pull-based-synchronization-with-migrations"><span class="toc-section-number">1.3.6</span> Pull-Based Synchronization with
Migrations</a></li>
</ul></li>
<li><a href="#implementation"><span class="toc-section-number">1.4</span> Implementation</a>
<ul>
<li><a href="#userfaults-in-go-with-userfaultfd"><span class="toc-section-number">1.4.1</span> Userfaults in Go with
<code>userfaultfd</code></a></li>
<li><a href="#file-based-synchronization"><span class="toc-section-number">1.4.2</span> File-Based
Synchronization</a></li>
<li><a href="#fuse-implementation-in-go"><span class="toc-section-number">1.4.3</span> FUSE Implementation in
Go</a></li>
<li><a href="#nbd-with-go-nbd"><span class="toc-section-number">1.4.4</span> NBD with
<code>go-nbd</code></a></li>
<li><a href="#chunking-pushpull-mechanisms-and-lifecycle-for-mounts"><span class="toc-section-number">1.4.5</span> Chunking, Push/Pull Mechanisms
and Lifecycle for Mounts</a></li>
<li><a href="#live-migration-for-mounts"><span class="toc-section-number">1.4.6</span> Live Migration for
Mounts</a></li>
<li><a href="#pluggable-encryption-and-authentication"><span class="toc-section-number">1.4.7</span> Pluggable Encryption and
Authentication</a></li>
<li><a href="#optimizing-backends-for-high-rtt"><span class="toc-section-number">1.4.8</span> Optimizing Backends For High
RTT</a></li>
<li><a href="#optimizing-the-transport-protocol-for-throughput"><span class="toc-section-number">1.4.9</span> Optimizing The Transport
Protocol For Throughput</a></li>
<li><a href="#using-remote-stores-as-backends"><span class="toc-section-number">1.4.10</span> Using Remote Stores as
Backends</a></li>
<li><a href="#bi-directional-protocols-with-dudirekta"><span class="toc-section-number">1.4.11</span> Bi-Directional Protocols With
Dudirekta</a></li>
<li><a href="#connection-pooling-for-high-rtt-scenarios"><span class="toc-section-number">1.4.12</span> Connection Pooling For High RTT
Scenarios</a></li>
</ul></li>
<li><a href="#results"><span class="toc-section-number">1.5</span>
Results</a>
<ul>
<li><a href="#latency"><span class="toc-section-number">1.5.1</span>
Latency</a></li>
<li><a href="#throughput"><span class="toc-section-number">1.5.2</span>
Throughput</a></li>
<li><a href="#pull-heuristics"><span class="toc-section-number">1.5.3</span> Pull Heuristics</a></li>
<li><a href="#downtime"><span class="toc-section-number">1.5.4</span>
Downtime</a></li>
</ul></li>
<li><a href="#discussion"><span class="toc-section-number">1.6</span>
Discussion</a>
<ul>
<li><a href="#userfaults"><span class="toc-section-number">1.6.1</span>
Userfaults</a></li>
<li><a href="#file-based-synchronization-1"><span class="toc-section-number">1.6.2</span> File-Based
Synchronization</a></li>
<li><a href="#fuse"><span class="toc-section-number">1.6.3</span>
FUSE</a></li>
<li><a href="#nbd"><span class="toc-section-number">1.6.4</span>
NBD</a></li>
<li><a href="#mounts-and-live-migration"><span class="toc-section-number">1.6.5</span> Mounts and Live
Migration</a></li>
<li><a href="#use-cases"><span class="toc-section-number">1.6.6</span>
Use Cases</a></li>
</ul></li>
<li><a href="#summary"><span class="toc-section-number">1.7</span>
Summary</a></li>
<li><a href="#conclusion"><span class="toc-section-number">1.8</span>
Conclusion</a></li>
<li><a href="#references"><span class="toc-section-number">1.9</span>
References</a>
<ul>
<li><a href="#structure"><span class="toc-section-number">1.9.1</span>
Structure</a></li>
<li><a href="#citations">Citations</a></li>
</ul></li>
</ul>
</nav>
<section id="introduction" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Introduction</h2>
<ul>
<li>Research question: Could memory be the universal way to access and
migrate state?</li>
<li>Why efficient memory synchronization is the missing key
component</li>
<li>High-level use cases for memory synchronization in the industry
today</li>
</ul>
</section>
<section id="technology" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Technology</h2>
<section id="the-linux-kernel" data-number="0.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span>
The Linux Kernel</h3>
<ul>
<li>Open-Source kernel created by Linus Torvals in 1991</li>
<li>Written in C, with recently Rust being added as an additional
allowed language<span class="citation" data-cites="linux2023docs">[1]</span></li>
<li>Powers millions of devices worldwide (servers, desktops, phones,
embedded devices)</li>
<li>Is a bridge between applications and hardware
<ul>
<li>Provides an abstraction layer</li>
<li>Compatible with many architectures (ARM, x86, RISC-V etc.)</li>
</ul></li>
<li>Is not an operating system in itself, but is usually made an
operating system by a distribution<span class="citation" data-cites="love2010linux">[2]</span></li>
<li>Distributions add userspace tools (e.g. GNU coreutils or BusyBox),
desktop environments and more, turning into a full operating system</li>
<li>Is a good fit for this thesis due to it’s open-source nature (allows
anyone to view, modify and contribute to the source code)</li>
</ul>
</section>
<section id="linux-kernel-modules" data-number="0.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span>
Linux Kernel Modules</h3>
<ul>
<li>Linux kernel is monolithic, but extensible thanks to kernel
modules<span class="citation" data-cites="love2010linux">[2]</span></li>
<li>Small pieces of kernel-level code that can be loaded and unloaded as
kernel modules</li>
<li>Can extend the kernel functionality without reboots</li>
<li>Are dynamically linked into the running kernel</li>
<li>Helps keep kernel size manageable and maintainable</li>
<li>Kernel modules are written in C</li>
<li>Interact with kernel through APIs</li>
<li>Poorly written modules can cause signficant kernel instability</li>
<li>Modules can be loaded at boot time or dynamically
(<code>modprobe</code>, <code>rmmod</code> etc.)<span class="citation" data-cites="maurer2008professional">[3]</span></li>
<li>Module lifecycle can be implemented with initialization and cleanup
functions</li>
</ul>
</section>
<section id="unix-signals-and-handlers" data-number="0.2.3">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span>
UNIX Signals and Handlers</h3>
<ul>
<li>Signals
<ul>
<li>Are software interrups that notify a process of important events
(exceptions etc.)</li>
<li>Can originate from the kernel, user input or different
processes</li>
<li>Function as an asynchronous communication mechanism between
processes or the kernel and a process</li>
<li>Have default actions, i.e. terminating the process or ignoring a
signal<span class="citation" data-cites="stevens2000advanced">[4]</span></li>
</ul></li>
<li>Handlers
<ul>
<li>Can be used to customize how a process should respond to a
signal</li>
<li>Can be installed with <code>sigaction()</code><span class="citation" data-cites="robbins2003unix">[5]</span></li>
</ul></li>
<li>Signals are not designed as an IPC mechanism, since they can only
alert of an event, but not of any additional data for it</li>
</ul>
</section>
<section id="memory-hierarchy" data-number="0.2.4">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span>
Memory Hierarchy</h3>
<ul>
<li>Memory in computers can be classified based on size, speed, cost and
proximity to the CPU</li>
<li>Principle of locality: The most frequently accessed data and
instructions should be in the closest memory<span class="citation" data-cites="smith1982cache">[6]</span></li>
<li>Locality is important mostly due to the “speed of the cable” -
throughput (due to dampening) and latency (due to being limited by the
speed of light) decreases as distance increases</li>
<li>Registers
<ul>
<li>Closest to the CPU</li>
<li>Very small amount of storage (32-64 bits of data)</li>
<li>Used by the CPU to perform operations</li>
<li>Very high speed, but limited in storage size</li>
</ul></li>
<li>Cache Memory
<ul>
<li>Divided into L1, L2 and L3</li>
<li>The higher the level, the larger and less expensive a layer</li>
<li>Buffer for frequently accessed data</li>
<li>Predictive algorithms optimize data usage</li>
</ul></li>
<li>Main Memory (RAM)
<ul>
<li>Offers larger capacity than cache but is slower</li>
<li>Stores programs and open files</li>
</ul></li>
<li>Secondary Storage (SSD/HDD)
<ul>
<li>Slower but RAM but can store larger amounts of memory</li>
<li>Typically stores the OS etc.</li>
<li>Is persistent (keeps data after power is cut)</li>
</ul></li>
<li>Tertiary Storage (optical disks, tape)
<ul>
<li>Slow, but very cheap</li>
<li>Tape: Can store very large amounts of data for relatively long
amounts of time</li>
<li>Typically used for archives or physical data transport (e.g. import
from personal infrastructure to AWS)</li>
</ul></li>
<li>Evolution of the hierarchy
<ul>
<li>Technological advancements continue to blur this clear
hierarchy<span class="citation" data-cites="maruf2023memory">[7]</span></li>
<li>E.g. NVMe rivals RAM speeds but can store larger amounts of
data</li>
<li>This thesis also blurs the hierarchy by exposing e.g. tertiary or
secondary storage with the same interfae as main memory</li>
</ul></li>
</ul>
</section>
<section id="memory-management-in-linux" data-number="0.2.5">
<h3 data-number="1.2.5"><span class="header-section-number">1.2.5</span>
Memory Management in Linux</h3>
<ul>
<li>Memory management is a crucial part of every operation system -
maybe even the whole point of an operating system</li>
<li>Creates buffer between applications and physical memory</li>
<li>Can provide security guarantees (e.g. only one process can access
it’s memory)</li>
<li>Kernel space
<ul>
<li>Runs the kernel, kernel extensions, device drivers</li>
<li>Managed by the kernel memory module</li>
<li>Uses slab allocation (groups objects of the same size into caches,
speeds up memory allocation, reduces fragmentation of the memory)<span class="citation" data-cites="bonwick1994slaballoc">[8]</span></li>
</ul></li>
<li>User space
<ul>
<li>Applications (and some drivers) store their memory here<span class="citation" data-cites="gorman2004linuxmem">[9]</span></li>
<li>Managed through a paging system</li>
<li>Each application has it’s own private virtual address space</li>
<li>Virtual address space divided into pages of 4 KB</li>
<li>Pages can be mapped to any “real” location in physical memory</li>
</ul></li>
</ul>
</section>
<section id="swap-space" data-number="0.2.6">
<h3 data-number="1.2.6"><span class="header-section-number">1.2.6</span>
Swap Space</h3>
<ul>
<li>A portion of the secondary storage is for virtual memory<span class="citation" data-cites="gorman2004linuxmem">[9]</span></li>
<li>Essential for systems running multiple applications</li>
<li>Moves inactive parts of ram to secondary storage to free up space
for other processes</li>
<li>Implementation in Linux
<ul>
<li>Linux uses a demand paging system: Memory is only allocated when it
is needed</li>
<li>Can be either a swap partition (separate area of the secondary
storage) or file (regular file that can be expanded/trucnated)</li>
<li>Swap paritions and files are transparent to use</li>
<li>Kernel uses a LRU algoruithm for deciding which pages</li>
</ul></li>
<li>Role in hiberation
<ul>
<li>Before hibernating, the system saves the content of RAM into swap
(where it is persistent)</li>
<li>When resuming, memory is read back from swap</li>
</ul></li>
<li>Role on performance
<ul>
<li>If swap is used too heavily, since the secondary storage is usually
slower than primary memory, it can lead to signficant slowdowns</li>
<li>“Swapiness” can be set for the kernel, which controls how likely the
system is to swap memory pages</li>
</ul></li>
</ul>
</section>
<section id="page-faults" data-number="0.2.7">
<h3 data-number="1.2.7"><span class="header-section-number">1.2.7</span>
Page Faults</h3>
<ul>
<li>Page faults occur when the process tries to access a page not
available in primary memory, which ause the OS to swap the required page
from secondary storage into primary memory<span class="citation" data-cites="maurer2008professional">[3]</span></li>
<li>Types
<ul>
<li>Minor page faults: Page is already in memory, but not linked to the
process that needs it</li>
<li>Major page fault: Needs to be loaded from secondary storage</li>
</ul></li>
<li>The LRU (and simpler clock algorithm) can minimize page faults</li>
<li>Techniques for handling page faults
<ul>
<li>Prefetching: Anticipating future page requests and loading them into
memory in advance</li>
<li>Page compression: Compressing inactive pages and storing them in
memory pre-emptively (so that less major faults happen)<span class="citation" data-cites="silberschatz2018operating">[10]</span></li>
</ul></li>
<li>Usually, handling page faults is something that the kernel does</li>
</ul>
</section>
<section id="mmap" data-number="0.2.8">
<h3 data-number="1.2.8"><span class="header-section-number">1.2.8</span>
<code>mmap</code></h3>
<ul>
<li>Overview
<ul>
<li>UNIX system call for mapping files or devices into memory</li>
<li>Multiple possible usecases: Shared memory, file I/O, fine-grained
memory allocation</li>
<li>Commonly used in applications like databases</li>
<li>Is a “power tool”, needs to be used carefully and intentionally</li>
</ul></li>
<li>Functionality
<ul>
<li>Establishes direct link (memory mapping) between a file and a memory
region<span class="citation" data-cites="choi2017mmap">[11]</span></li>
<li>When the system reads from the mapped memory region, it reads from
the file directly and vice versa</li>
<li>Reduces overhead since no or less context switches are needed</li>
</ul></li>
<li>Benefits:
<ul>
<li>Enables zero-copy operations: Data can be accessed directly as
though it were in memory, without having to copy it from disk first</li>
<li>Can be used to share memory between processes without having to go
through the kernel with syscalls<span class="citation" data-cites="stevens2000advanced">[4]</span></li>
</ul></li>
<li>Drawback: Bypasses the file system cache, which can lead to stale
data if multiple processes read/write at the same time</li>
</ul>
</section>
<section id="inotify" data-number="0.2.9">
<h3 data-number="1.2.9"><span class="header-section-number">1.2.9</span>
<code>inotify</code></h3>
<ul>
<li>Event-driven notification system of the Linux kernel<span class="citation" data-cites="prokop2010inotify">[12]</span></li>
<li>Monitors file system for events (i.e. modifications, access
etc.)</li>
<li>Uses a watch feature for monitoring specific events, e.g. only
watching writes</li>
<li>Reduces overhead and ressource use compared to polling</li>
<li>Widely used in many applications, e.g. Dropbox for file
synchronization</li>
<li>Has limitations like the limit on how many watches can be
established</li>
</ul>
</section>
<section id="linux-kernel-disk-and-file-caching" data-number="0.2.10">
<h3 data-number="1.2.10"><span class="header-section-number">1.2.10</span> Linux Kernel Disk and File
Caching</h3>
<ul>
<li>Disk caching
<ul>
<li>Temporarely stores frequently accessed data in RAM</li>
<li>Uses principle of locality (see Memory Hierarchy)</li>
<li>Implemented using the page cache subsystem in Linux</li>
<li>Uses the LRU algorithm to manage cache contents</li>
</ul></li>
<li>File caching
<ul>
<li>Linux caches file system metadata in the <code>dentry</code> and
<code>inode</code> caches</li>
<li>Metadata includes i.e. file names, attributes and locations</li>
<li>This caching accelerates the resoluton of path names and file
attributes (i.e. the last change data for polling)</li>
<li>File reads/writes pass through the disk cache</li>
</ul></li>
<li>Complexities
<ul>
<li>Data consistency: Between the disk and cache via writebacks.
Aggressive writebacks lead to reduced performance, delays risk data
loss</li>
<li>Release of cached data under memory pressure: Cache eviction
requires intelligent algorithms, i.e. LRU<span class="citation" data-cites="maurer2008professional">[3]</span></li>
</ul></li>
</ul>
</section>
<section id="tcp-udp-and-quic" data-number="0.2.11">
<h3 data-number="1.2.11"><span class="header-section-number">1.2.11</span> TCP, UDP and QUIC</h3>
<ul>
<li>TCP
<ul>
<li>Connection-oriented</li>
<li>Has been the reliable backbone of internet commmunication</li>
<li>Guaranteed delivery and maintained data order</li>
<li>Includes error checking, lost packet retransmission, and congestion
control mechanisms</li>
<li>Powers the majority of the web<span class="citation" data-cites="postel1981tcp">[13]</span></li>
</ul></li>
<li>UDP
<ul>
<li>Connectionless</li>
<li>No reliability or ordered packet delivery guarantees</li>
<li>Faster than TCP due to less guarantees</li>
<li>Suitable for applications that require speed over reliability
(i.e. online gaming, video calls etc.)<span class="citation" data-cites="postel1980udp">[14]</span></li>
</ul></li>
<li>QUIC
<ul>
<li>Modern transport layer protocol developed by Google and standardized
by the IETF in 2020</li>
<li>Intents to combine the best aspects of TCP and UDP</li>
<li>Provides reliability and ordered delivery guarantees</li>
<li>Reduces connection establishment times/initial latency by combining
connection and security handshakes</li>
<li>Avoids head-of-line blocking by allowing independent delivery of
separate streams<span class="citation" data-cites="langley2017quic">[15]</span></li>
</ul></li>
</ul>
</section>
<section id="delta-synchronization" data-number="0.2.12">
<h3 data-number="1.2.12"><span class="header-section-number">1.2.12</span> Delta Synchronization</h3>
<ul>
<li>Traditionally, when files are synchronized between hosts, the entire
file is transfered</li>
<li>Delta synchronization is a technique that intents to instead
transfer only the part of the file that has changed</li>
<li>Can lead to reduced network and I/O overhead</li>
<li>The probably most popular tool for file synchronization like this is
rsync</li>
<li>When a delta-transfer algorithm is used, it computes the difference
between the local and the remote file, and then synchronizes the
changes</li>
<li>The delta sync algorithm first does file block division</li>
<li>The file on the destination is divided into fixed-size blocks</li>
<li>For each block in the destination, a weak and fast checksum is
calculated</li>
<li>The checksums are sent over to the source</li>
<li>On the source, the same checksum calculation process is run, and
compared against the checksums that were sent over (matching block
identification)</li>
<li>Once the changed blocks are known, the source sends over the offset
of each block and the changed block’s data to the destination</li>
<li>When a block is received, the destination writes the chunk to the
specified offset, reconstructing the file</li>
<li>Once one polling interval is done, the process begins again<span class="citation" data-cites="xiao2018rsync">[16]</span></li>
</ul>
</section>
<section id="file-systems-in-userspace-fuse" data-number="0.2.13">
<h3 data-number="1.2.13"><span class="header-section-number">1.2.13</span> File Systems In Userspace
(FUSE)</h3>
<ul>
<li>Software interface that allows writing custom file systems in
userspace</li>
<li>Developers can create file systems without having to engage in
low-level kernel development</li>
<li>Available on multiple platforms, mostly Linux but also macOS and
FreeBSD</li>
<li>In order to implement file systems in user space, we can use the
FUSE API</li>
<li>Here, a user space program registers itself with the FUSE kernel
module</li>
<li>This program provides callbacks for the file system operations,
e.g. for <code>open</code>, <code>read</code>, <code>write</code>
etc.</li>
<li>When the user performs a file system operation on a mounted FUSE
file system, the kernel module will send a request for the operation to
the user space program, which can then reply with a response, which the
FUSE kernel module will then return to the user</li>
<li>This makes it much easier to create a file system compared to
writing it in the kernel, as it can run in user space</li>
<li>It is also much safer as no custom kernel module is required and an
error in the FUSE or the backend can’t crash the entire kernel</li>
<li>Unlike a file system implemented as a kernel module, this layer of
indirection makes the file system portable, since it only needs to
communicate with the FUSE module</li>
<li>Does have significant performance overhead due to the context
switching between the kernel and the file system in userspace<span class="citation" data-cites="vangoor2017fuse">[17]</span></li>
<li>Is used for many high-level interfaces to extenal services, i.e. to
mount S3 buckets or a remote system’s disk via SSH</li>
</ul>
</section>
<section id="network-block-device-nbd" data-number="0.2.14">
<h3 data-number="1.2.14"><span class="header-section-number">1.2.14</span> Network Block Device
(NBD)</h3>
<ul>
<li>NBD uses a protocol to communicate between a server (provided by
user space) and a client (provided by the NBD kernel module)</li>
<li>The protocol can run over WAN, but is really mostly designed for LAN
or localhost usage</li>
<li>It has two phases: Handshake and transmission<span class="citation" data-cites="blake2023nbd">[18]</span></li>
<li>There are multiple actors in the protocol: One or multiple clients,
the server and the virtual concept of an export</li>
<li>When the client connects to the server, the server sends a greeting
message with the server’s flags</li>
<li>The client responds with its own flags and an export name (a single
NBD server can expose multiple devices) to use</li>
<li>The server sends the export’s size and other metadata, after which
the client acknowledges the received data and the handshake is
complete</li>
<li>After the handshake, the client and server start exchanging commands
and replies</li>
<li>A command can be any of the basic operations needed to access a
block device, e.g. read, write or flush</li>
<li>Depending on the command, it can also contain data (such as the
chunk to be written), offsets, lengths and more</li>
<li>Replies can contain an error, success value or data depending on the
reply’s type</li>
<li>NBD is however limited in some respects; the maximum message size is
32 MB, but the maximum block/chunk size supported by the kernel is just
4096 KB, making it a suboptimal protocol to run over WAN, esp. in high
latency scenarios</li>
<li>The protocol also allows for listing exports, making it possible to
e.g. list multiple memory regions on a single server</li>
<li>NBD is an older protocol with multiple different handshake versions
and legacy features</li>
<li>Since the purpose of NBD in this use case is minimal and both the
server and the client are typically controlled, it makes sense to only
implement the latest recommended versions and the baseline feature
set</li>
<li>The baseline feature set requires no TLS, the latest “fixed
newstyle” handshake, the ability to list and choose an export, as well
as the read, write and disc(onnect) commands and replies</li>
<li>As such, the protocol is very simple to implement</li>
<li>With this simplicity however also come some drawbacks: NBD is less
suited for use cases where the backing device behaves very differently
from a random-access store device, like for example a tape drive, since
it is not possible to work with high-level abstractions such as files or
directories</li>
<li>This is, for the narrow memory synchronization use case, however
more of a feature than a bug</li>
<li>Since it works on the block level, it can’t offer shared acesses to
the same file for multiple clients</li>
</ul>
</section>
<section id="virtual-machine-live-migration" data-number="0.2.15">
<h3 data-number="1.2.15"><span class="header-section-number">1.2.15</span> Virtual Machine Live
Migration</h3>
<section id="pre-copy" data-number="0.2.15.1">
<h4 data-number="1.2.15.1"><span class="header-section-number">1.2.15.1</span> Pre-Copy</h4>
<ul>
<li>While these systems already allow for some optimizations over simply
using the NBD protocol over WAN, they still mean that chunks will only
be fetched as they are being needed, which means that there still is a
guaranteed minimum downtime</li>
<li>In order to improve on this, a more advanced API (the managed mount
API) was created</li>
<li>A field that tries to optimize for this use case is live migration
of VMs</li>
<li>Live migration refers to moving a virtual machine, its state and
connected devices from one host to another with as little downtime as
possible</li>
<li>There are two types of such migration algorithms; pre-copy and
post-copy migration</li>
<li>Pre-copy migration works by copying data from the source to the
destination as the VM continues to run (or in the case of a generic
migration, app/other state continues being written to)</li>
<li>First, the initial state of the VM’s memory is copied to the
destination</li>
<li>If, during the push, chunks are being modified, they are being
marked as dirty</li>
<li>These dirty chunks are being copied over to the destination until
the number of remaining chunks is small enough to satisfy a maximum
downtime criteria</li>
<li>Once this is the case, the VM is suspended on the source, and the
remaining chunks are synced over to destination</li>
<li>Once the transfer is complete, the VM is resumed on the
destination</li>
<li>This process is helpful since the VM is always available in full on
either the source or the destination, and it is resilient to a network
outage occurring during the synchronization</li>
<li>If the VM (or app etc.) is however changing too many chunks on the
source during the migration, the maximum acceptable downtime criteria
might never get reached, and the maximum acceptable downtime is also
somewhat limited by the available RTT<span class="citation" data-cites="he2016migration">[19]</span></li>
</ul>
</section>
<section id="post-copy" data-number="0.2.15.2">
<h4 data-number="1.2.15.2"><span class="header-section-number">1.2.15.2</span> Post-Copy</h4>
<ul>
<li>An alternative to pre-copy migration is post-copy migration</li>
<li>In this approach, the VM is immediately suspended on the source,
moved to the destination with only a minimal set of chunks</li>
<li>After the VM has been moved to the destination, it is resumed</li>
<li>If the VM tries to access a chunk on the destination, a page fault
is raised, and the missing page is fetched from the source, and the VM
continues to execute</li>
<li>The benefit of post-copy migration is that it does not require
re-transmitting dirty chunks to the destination before the maximum
tolerable downtime is reached</li>
<li>The big drawback of post-copy migration is that it can result in
longer migration times, because the chunks need to be fetched from the
network on-demand, which is very latency/RTT-sensitive<span class="citation" data-cites="he2016migration">[19]</span></li>
</ul>
</section>
<section id="workload-analysis" data-number="0.2.15.3">
<h4 data-number="1.2.15.3"><span class="header-section-number">1.2.15.3</span> Workload Analysis</h4>
<ul>
<li>“Reducing Virtual Machine Live Migration Overhead via Workload
Analysis” provides an interesting analysis of options on how this
decision of when to migrate can be made<span class="citation" data-cites="baruchi2015workload">[20]</span></li>
<li>While being designed mostly for use with virtual machines, it could
serve as a basis for other applications or migration scenarios, too</li>
<li>The proposed method identifies workload cycles of VMs and uses this
information to postpone the migration if doing so is beneficial</li>
<li>This works by analyzing cyclic patters that can unnecessarily delay
a VM’s migration, and identifies optimal cycles to migrate VMs in from
this information</li>
<li>For the VM use case, such cycles could for example be the GC of a
large application triggering a lot of changes to the VMs memory
etc.</li>
<li>If a migration is proposed, the system checks for whether it is
currently in a beneficial cycle to migrate in which case it lets the
migration proceed; otherwise, it postpones it until the next cycle</li>
<li>The algorithm uses a Bayesian classifier to identify a favorable or
unfavorable cycle</li>
<li>Compared to the alternative, which is usually waiting for a
significant percentage of the chunks that were not changed before
tracking started to be synced first, this can potentially yield a lot of
improvements</li>
<li>The paper has found an improvement of up to 74% in terms of live
migration time/downtime and 43% in terms of the amount of data
transferred over the network</li>
<li>While such a system was not implemented for r3map, using r3map with
such a system would certainly be possible</li>
</ul>
</section>
</section>
<section id="streams-and-pipelines" data-number="0.2.16">
<h3 data-number="1.2.16"><span class="header-section-number">1.2.16</span> Streams and Pipelines</h3>
<ul>
<li>Fundamental concepts in computer science</li>
<li>Sequentually process elements</li>
<li>Allow for the efficient processing of large amounts of data, without
having to load everything into memory</li>
<li>Form the backbone of efficient, modular data processing</li>
<li>Streams
<ul>
<li>Represent a continous sequence of data</li>
<li>Can be a source or destination of data (i.e. files, network
connections, stdin/stdout etc.)</li>
<li>Allow processing of data as it becomes available</li>
<li>Minimized memory consumption</li>
<li>Especially well suited for long-running processes (where data gets
streamed in for a extended time)<span class="citation" data-cites="akidau2018streaming">[21]</span></li>
</ul></li>
<li>Pipelines
<ul>
<li>Series of data processing stages: Output of one stage serves as
input to the next<span class="citation" data-cites="peek1994unix">[22]</span></li>
<li>Stages can often be run in parallel, improving performance due to a
higher degree of concurrency</li>
<li>Example: Instruction pipeline in CPU, were the stages of instruction
execution can be performend in parallel</li>
<li>Example: UNIX pipes, where the output of a command
(e.g. <code>curl</code>) can be piped into another command
(e.g. <code>jq</code>) to achieve a larger goal</li>
</ul></li>
</ul>
</section>
<section id="grpc" data-number="0.2.17">
<h3 data-number="1.2.17"><span class="header-section-number">1.2.17</span> gRPC</h3>
<ul>
<li>Open-Source and high-performance RPC framework</li>
<li>Developed by Google in 2015</li>
<li>Features
<ul>
<li>Uses HTTP/2 as the transport protocol to benefit from header
compression and request multiplexing</li>
<li>Uses protobuf as the IDL and wire format, a high-performance,
polyglot mechanism for data serialization (instead of the slower and
more verbose JSON of REST APIs)</li>
<li>Supports unary RPCs, server-streaming RPCs, client-streaming RPCs
and bidirectional RPCs</li>
<li>Has pluggable support for load balancing, tracing, health checking
and authentication<span class="citation" data-cites="google2023grpc">[23]</span>j</li>
</ul></li>
<li>Supports many languages (Go, Rust, JS etc.)</li>
<li>Developed by the CNCF</li>
</ul>
</section>
<section id="redis" data-number="0.2.18">
<h3 data-number="1.2.18"><span class="header-section-number">1.2.18</span> Redis</h3>
<ul>
<li>In-memory data structure store</li>
<li>Used as a database, cache and/or message broker</li>
<li>Created by S. Sanfilippo in 2009</li>
<li>Different from other NoSQL databases by supporting various data
structures like lists, sets, hashes or bitmaps</li>
<li>Uses in-memory data storage for maximum speed and efficiency<span class="citation" data-cites="redis2023introduction">[24]</span></li>
<li>Allows for low-latency reads/writes</li>
<li>While not intended for persistance, it is possible to store data on
disk</li>
<li>Has a non-blocking I/O model and offers near real-time data
processing capabilities</li>
<li>Includes a pub-sub system to be able to function as a message
broker<span class="citation" data-cites="redis2023pubsub">[25]</span></li>
</ul>
</section>
<section id="s3-and-minio" data-number="0.2.19">
<h3 data-number="1.2.19"><span class="header-section-number">1.2.19</span> S3 and Minio</h3>
<ul>
<li>S3
<ul>
<li>Object storage service for data-intensive workloads</li>
<li>Offered by AWS</li>
<li>Can be globally distributed to allow for fast access times from
anywhere on the globe</li>
<li>Range of storage classes with different requirements</li>
<li>Includes authentication and authorization</li>
<li>Exposes HTTP API for accessing the stored folders and files<span class="citation" data-cites="aws2023s3">[26]</span></li>
</ul></li>
<li>Minio
<ul>
<li>Open-source storage server compatible wiht S3</li>
<li>Lightweight and simple, written in Go</li>
<li>Can be hosted on-prem and is open source</li>
<li>Allows horizontal scalability to storage large amounts of data
across nodes<span class="citation" data-cites="minio2023coreadmin">[27]</span></li>
</ul></li>
</ul>
</section>
<section id="cassandra-and-scyllladb" data-number="0.2.20">
<h3 data-number="1.2.20"><span class="header-section-number">1.2.20</span> Cassandra and ScylllaDB</h3>
<ul>
<li>Popular wide-column NoSQL databases</li>
<li>Combines Amazon’s Dynamo model and Google’s Bigtable model to create
a highly available database</li>
<li>Apache Cassandra
<ul>
<li>Highly scalable, eventually consistent</li>
<li>Can handle large amounts of data across many servers with no single
point of failure</li>
<li>Consistency can be tuned according to needs (eventual to
strong)</li>
<li>Doesn’t use master nodes due to it’s use of a P2P protocol and
distributed hash ring design</li>
<li>Does have high latency under heavy load and requires fairly complex
configuration<span class="citation" data-cites="lakshman2010cassandra">[28]</span></li>
</ul></li>
<li>ScyllaDB
<ul>
<li>Launched in 2015</li>
<li>Written in C++ and has a shared-nothing architecture, unlike
Cassandra which is written in Java</li>
<li>Compatible with Cassandra’s API and data model<span class="citation" data-cites="scylladb2023ring">[29]</span></li>
<li>Designed to overcome Cassandra’s limitations esp. around latency,
esp. P99 latency</li>
<li>Performance improvements were confirmed with various benchmarking
studies<span class="citation" data-cites="grabowski2021scylladb">[30]</span></li>
</ul></li>
</ul>
</section>
</section>
<section id="planning" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span>
Planning</h2>
<section id="pull-based-synchronization-with-userfaultfd" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span>
Pull-Based Synchronization With <code>userfaultfd</code></h3>
<ul>
<li>An implementation of post-copy migration</li>
<li>Memory region is created on the destination host</li>
<li>Reads to the region by the migrated ressource trigger a page
fault</li>
<li>When we encounter such a page fault, we want to fetch the relevant
offset from the remote</li>
<li>Typically, page faults are resolved by the Kernel</li>
<li>Here, we want to handle handle the page faults in user space</li>
<li>Traditionally, it was possible to use <code>SIGSEGV</code> signal
handlers to use handle this from the program</li>
<li>This however is complicated and slow</li>
<li>Instead we use a new kernel API (“Userfaults”)</li>
<li>We register the region to be handled by userfault</li>
<li>Then we start an handler that fetches the offsets from the remote on
a page fault</li>
<li>This handler is connected to the registered region using a file
descriptor</li>
<li>We can transfer the handler’s file descriptor between processes over
a socket</li>
</ul>
</section>
<section id="push-based-synchronization-with-mmap-and-hashing" data-number="0.3.2">
<h3 data-number="1.3.2"><span class="header-section-number">1.3.2</span>
Push-Based Synchronization With <code>mmap</code> and Hashing</h3>
<ul>
<li>As mentioned before <code>mmap</code> allows mapping a memory region
to a file</li>
<li>We can put the migratable ressource into this memory region</li>
<li>If we get writes to the region, we eventually get writes to the
region</li>
<li>If we’re able to detect these writes and copy them to the
destination, we can implement a pre-copy migration system</li>
<li><code>mmap</code>ed regions still using caching to speed up
reads</li>
<li>Changes from the region can be flushed to the disk with
<code>msync</code></li>
<li>Usually we could use inotify to detect changes to the file, but
inotify doesn’t work with <code>mmap</code>ed regions</li>
<li>As a result, we went for a polling-based solution instead</li>
<li>Polling has drawbacks, which we tried to work around upon in our
implementation</li>
</ul>
</section>
<section id="push-pull-synchronization-with-fuse" data-number="0.3.3">
<h3 data-number="1.3.3"><span class="header-section-number">1.3.3</span>
Push-Pull Synchronization with FUSE</h3>
<ul>
<li>Can serve as the basis for a pre- or post-copy migration</li>
<li>Similarly to the file-based synchronization we <code>mmap</code> a
file into memory</li>
<li>The file is stored on a custom file system</li>
<li>We then catch reads (for a post-copy migration) or writes (for a
pre-copy migration)</li>
<li>Implementing a file system in the kernel is possible but cumbersome
as described before</li>
<li>With FUSE we can implement the file system in user space, making
this much simpler, as we’ll show in the implementation</li>
</ul>
</section>
<section id="mounts-with-nbd" data-number="0.3.4">
<h3 data-number="1.3.4"><span class="header-section-number">1.3.4</span>
Mounts with NBD</h3>
<ul>
<li>Another <code>mmap</code>-based solution for pre- and/or post-copy
migration</li>
<li>Instead of <code>mmap</code>ing a file, a device is
<code>mmap</code>ed</li>
<li>This device is a block device provided by a NBD client</li>
<li>We can then connect the NBD client to a remote NBD server, which
contains the migratable resource</li>
<li>Any reads to/from the region go to/from the block device and are
resolved by the remote NBD server</li>
<li>Isn’t per se a synchronization methods on it’s own, but rather a
remote mount</li>
<li>Unlike a FUSE this means we only need to implement a block device,
not a full file system</li>
<li>Implementation and performance overhead of a NBD server is fairly
low as we’ll show in the implementation however</li>
</ul>
</section>
<section id="push-pull-synchronization-with-mounts" data-number="0.3.5">
<h3 data-number="1.3.5"><span class="header-section-number">1.3.5</span>
Push-Pull Synchronization with Mounts</h3>
<ul>
<li>Also tracks changes to the memory region of the migratable ressource
using NBD</li>
<li>Limitations of the NBD protocol in WAN
<ul>
<li>Usually, the NBD server and client don’t run on the same system</li>
<li>NBD was originally designed to used as a LAN protocol to access a
remote hard disk</li>
<li>As mentioned before, NBD can run over WAN, but is not designed for
this</li>
<li>The biggest problem with running NBD over a public network, even if
TLS is enabled is latency</li>
<li>Individual chunks would only be fetched to the local system as they
are being accessed, adding a guaranteed minimum latency of at least the
RTT</li>
<li>Instead of directly connecting a client to a remote server, we add a
layer of indirection, called a <code>Mount</code> that consists of both
a client and a server, both running on the local system</li>
</ul></li>
<li>Combining the NBD server and client to a reusable unit
<ul>
<li>We then connect the server to the backend with an API that is better
suited for WAN usage</li>
<li>This also makes it possible to implement smart pull/push strategies
instead of simply directly writing to/from the network (“managed
mounts”)</li>
</ul></li>
<li>The mount WAN protocol
<ul>
<li>The simplest form of the mount API is the direct mount API</li>
<li>This API simply swaps out NBD for a transport-independent RPC
framework, but does not do additional optimizations</li>
<li>It has two simple actors: A client and a server, with only the
server providing methods to be called (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/backend.go#L14-L19)</li>
<li>The protocol as such is stateless, as there is only a simple remote
read/write interface (add state machine and sequence diagram here)</li>
</ul></li>
<li>Chunking
<ul>
<li>One additional layer that needs to be implemented however is proper
chunking support</li>
<li>While we can specify a chunk size for the NBD client in the form of
a block size, we can only go up to 4 KB chunks</li>
<li>For scenarios where the RTT between the backend and server is large,
it might make sense to use a much larger chunk size for the actual
networked transfers</li>
<li>Many backends also have constraints that prevent them from
functioning without a specific chunk size or aligned offsets, such as
using e.g. tape drives, which require setting a block size and work best
when these chunks are multiple MBs instead of KBs</li>
<li>Even if there are no constraints on chunking on the backend side
(e.g. when a file is used as the backend), it might make sense to limit
the maximum supported message size between the client and server to
prevent DoS attacks by forcing the backend to allocate large chunks of
memory to satisfy requests, which requires a chunking system to
work</li>
</ul></li>
<li>Server-side vs. client-side chunking
<ul>
<li>It is possible to do the chunking in two places; on the mount API’s
side, and on the (potentially remote) backend’s side</li>
<li>Doing the chunking on the backend’s side is usually much faster than
on the mount API’s side, as writes with lengths smaller than the chunk
size will mean that the remote chunk needs to be fetched first,
significantly increasing the latency esp. in scenarios with high
RTT</li>
</ul></li>
<li>Combining pre- and post-copy migration with managed mounts
<ul>
<li>For the managed mount API, both paradigms were implemented</li>
<li>The managed mount API is primarily intended as an API for reading
from a remote resource and then syncing changes back to it however, not
migrating a resource between two hosts</li>
<li>The migration API (will be discussed later) is a more well-optimized
version for this use case</li>
</ul></li>
<li>Background pull
<ul>
<li>The pre-copy API is implemented in the form of preemptive pulls
based on another <code>ReaderAt</code></li>
<li>It allows passing in a pull heuristic function, which it uses to
determine which chunks should be pulled in which order</li>
<li>Many applications commonly access certain bits of data first</li>
<li>If a resource should be available locally as quickly as possible,
then using the correct pull heuristic can help a lot</li>
<li>For example, if the data pulled consists of a header, then using a
pull heuristic that pulls these header chunks first can be of help</li>
<li>If a file system is being synchronized, and the superblocks of the
file system are being stored in a known pattern, the pull heuristic can
be used to fetch these superblocks first</li>
<li>If a format like MP4, which has an index, is used then said index
can be fetched first, be accessed first and during the time needed to
parse the index, the remaining data can be pulled in the background</li>
</ul></li>
<li>Background push
<ul>
<li>In order to also be able to write back however, it needs to have a
push system as well</li>
<li>This push system is being started in parallel with the pull
system</li>
<li>It also takes a local and a remote <code>ReadWriterAt</code></li>
<li>This integrates with the callbacks supplied by the syncer, which
ensures that we don’t sync back changes that have been pulled but not
modified, only the ones that have been changed locally</li>
</ul></li>
</ul>
</section>
<section id="pull-based-synchronization-with-migrations" data-number="0.3.6">
<h3 data-number="1.3.6"><span class="header-section-number">1.3.6</span>
Pull-Based Synchronization with Migrations</h3>
<ul>
<li>Also tracks changes to the memory region of the migratable ressource
using NBD</li>
<li>Optimization mounts for migration scenarios
<ul>
<li>We have now implemented a managed mounts API</li>
<li>This API allows for efficient access to a remote resource through
memory</li>
<li>It is however not well suited for a migration scenario</li>
<li>For migrations, more optimization is needed to minimize the maximum
acceptable downtime</li>
<li>For the migration, the process is split into two distinct
phases</li>
<li>The same preemptive background pulls and parallelized device/syncer
startup can be used, but the push process is dropped</li>
<li>The two phases allow pulling the majority of the data first, and
only finalize the move later with the remaining data</li>
<li>This is inspired by the pre-copy approach to VM live migration, but
also allows for some of the benefits of the post-copy approach as we’ll
see later</li>
<li>Why is this useful? A constraint for the mount-based API that we
haven’t mentioned before is that it doesn’t allow safe concurrent access
of a resource by two readers or writers at the same time</li>
<li>This poses a problem for migration, where the downtime is what
should be optimized for, as the VM or app that is writing to the source
device would need to be suspended before the transfer could begin</li>
<li>This adds very significant latency, which is a problem</li>
<li>The mount API was also designed in such a way as to make it hard to
share a resource this way</li>
<li>The remote backend for example API doesn’t itself provide a mount to
access the underlying data, which further complicates migration by not
implementing a migration lifecycle</li>
</ul></li>
<li>The migration protocol
<ul>
<li>To fix this, the migration API defines two new actors: The seeder
and the leecher</li>
<li>The seeder represents a resource that can be migrated from/a host
that exposes a migratable resource</li>
<li>The leecher represents a client that wants to migrate a resource to
itself</li>
<li>Initially, the protocol starts by running an application with the
application’s state on the seeder’s mount</li>
<li>When a leecher connects to the seeder, the seeder starts tracking
any writes to it’s mount</li>
<li>The leecher starts pulling chunks from the seeder to it’s local
backend</li>
<li>Once the leecher has received a satisfactory level of locally
available chunks, it as the seeder to finalize, which then causes the
seeder to stop the remote app, <code>msync</code>/flushes the drive, and
returns the chunks that were changed between it started tracking and
finalizing</li>
<li>The leecher then marks these chunks as remote, immediately resumes
the VM, and queues them to be pulled immediately</li>
<li>By splitting the migration into these two distinct phases, the
overhead of having to start the device on the leecher can be skipped and
additional app initialization that doesn’t depend on the app’s state
(e.g. memory allocation, connecting to databases, loading models etc.)
can be performed before the application needs to be suspended</li>
<li>This solution combines both the pre-copy algorithm (by pulling the
chunks from the seeder ahead of time) and the post-copy algorithm (by
resolving dirty chunks from the seeder after the VM has been migrated)
into one coherent protocol (add state machine diagram here)</li>
<li>This way, the maximum tolerable downtime can be drastically reduced,
and dirty chunks don’t need to be re-transmitted multiple times</li>
<li>Effectively, it drops the maximum guaranteed downtime to the time it
takes to <code>msync</code> the seeder’s app state, the RTT and, if they
are being accessed immediately, how long it takes to fetch the chunks
that were written in between starting to track and finalize</li>
</ul></li>
<li>The finalization phase
<ul>
<li>A interesting question to ask with the two-step migration API is
when to start the finalization step</li>
<li>As is visible from the migration API protocol state machine showed
beforehand, the finalization stage is critical and hard or impossible to
recover from depending on the implementation</li>
<li>While for the memory sync on its own, one could just call
<code>Finalize</code> multiple times to restart it</li>
<li>But since <code>Finalize</code> needs to return a list of dirty
chunks, it requires the VM or app on the source device to be suspended
before <code>Finalize</code> can return</li>
<li>While not necessarily the case, such a suspend operation is not
idempotent (since it might not just be a suspension that is required,
but also a shutdown of dependencies etc.)</li>
</ul></li>
</ul>
</section>
</section>
<section id="implementation" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span>
Implementation</h2>
<section id="userfaults-in-go-with-userfaultfd" data-number="0.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span>
Userfaults in Go with <code>userfaultfd</code></h3>
<ul>
<li>General functionality
<ul>
<li>By listening to page faults, we know when a process wants to access
a specific piece of memory</li>
<li>We can use this to then pull the chunk of memory from a remote, map
it to the address on which the page fault occured, thus only fetching
data when it is required</li>
<li>Usually, handling page faults is something that the kernel does</li>
<li>In our case, we want to handle page faults in userspace, and
implement post-copy with them</li>
<li>In the past, this used to be possible from userspace by handling the
<code>SIGSEGV</code> signal in the process</li>
<li>In our case however, we can use a recent system called
<code>userfaultfd</code> to do this in a more elegant way (available
since kernel 4.11)</li>
<li><code>userfaultfd</code> allows handling these page faults in
userspace</li>
<li>The region that should be handled can be allocated with
e.g. <code>mmap</code></li>
<li>Once we have the file descriptor for the <code>userfaultfd</code>
API, we need to transfer this file descriptor to a process that should
respond with the chunks of memory to be put into the faulting
address</li>
<li>Once we have received the socket we need to register the handler for
the API to use</li>
<li>If the handler receives an address that has faulted, it responds
with the <code>UFFDIO_COPY</code> <code>ioctl</code> and a pointer to
the chunk of memory that should be used on the file descriptor (code
snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/pkg/mapper/handler.go)</li>
</ul></li>
<li>API design for <code>userfault-go</code>
<ul>
<li>Implementing this in Go was quite tricky, and it involves using
<code>unsafe</code></li>
<li>We can use the <code>syscall</code> and <code>unix</code> packages
to interact with <code>ioctl</code> etc.</li>
<li>We can use the <code>ioctl</code> syscall to get a file descriptor
to the <code>userfaultfd</code> API, and then register the API to handle
any faults on the region (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/pkg/mapper/register.go#L15)</li>
<li>Passing file descriptors between processes is possible by using a
UNIX socket (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/pkg/transfer/unix.go)</li>
</ul></li>
<li>Implementing <code>userfaultfd</code> backends
<ul>
<li>A big benefit of using <code>userfaultfd</code> and the pull method
is that we are able to simplify the backend of the entire system down to
a <code>io.ReaderAt</code> (code snippet from
https://pkg.go.dev/io#ReaderAt)</li>
<li>That means we can use almost any <code>io.ReaderAt</code> as a
backend for a <code>userfaultfd-go</code> registered object</li>
<li>We know that access will always be aligned to 4 KB chunks/the system
page size, so we can assume a chunk size on the server based on
that</li>
<li>For the first example, we can return a random pattern in the backend
(code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/cmd/userfaultfd-go-example-abc/main.go)
- this shows a great way of exposing truly arbitrary information into a
byte slice without having to pre-compute everything or changing the
application</li>
<li>Since a file is a valid <code>io.ReaderAt</code>, we can also use a
file as the backend directly, creating a system that essentially allows
for mounting a (remote) file into memory (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/cmd/userfaultfd-go-example-file/main.go)</li>
<li>Similarly so, we can use it map a remote object from S3 into memory,
and access only the chunks of it that we actually require (which in the
case of S3 is achieved with HTTP range requests) (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/cmd/userfaultfd-go-example-s3/main.go)</li>
</ul></li>
</ul>
</section>
<section id="file-based-synchronization" data-number="0.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span>
File-Based Synchronization</h3>
<ul>
<li>File-based synchronization
<ul>
<li>We can do this by using <code>mmap</code>, which allows us to map a
file into memory</li>
<li>By default, <code>mmap</code> doesn’t write changes from a file back
into memory, no matter if the file descriptor passed to it would allow
it to or not</li>
<li>We can however add the <code>MAP_SHARED</code> flag; this tells the
kernel to write back changes to the memory region to the corresponding
regions of the backing file</li>
<li>Linux caches reads to such a backing file, so only the first page
fault would be answered by fetching from disk, just like with
<code>userfaultfd</code></li>
<li>The same applies to writes; similar to how files need to be
<code>sync</code>ed in order for them to be written to disks,
<code>mmap</code>ed regions need to be <code>msync</code>ed in order to
flush changes to the backing file</li>
<li>In order to synchronize changes to the region between hosts by
syncing the underlying file, we need to have the changes actually be
represented in the file, which is why <code>msync</code> is
critical</li>
<li>For files, you can use <code>O_DIRECT</code> to skip this kernel
caching if your process already does caching on its own, but this flag
is ignored by the <code>mmap</code></li>
</ul></li>
<li><code>inotify</code> vs. polling
<ul>
<li>Usually, one would use <code>inotify</code> to watch changes to a
file</li>
<li><code>inotify</code> allows applications to register handlers on a
file’s events, e.g. <code>WRITE</code> or <code>SYNC</code>. This allows
for efficient file synchronization, and is used by many file
synchronization tools</li>
<li>It is also possible to filter only the events that we need to sync
the writes, making it the perfect choice for this use case</li>
<li>For technical reasons however (mostly because the file is
represented by a memory region), Linux doesn’t fire these events for
<code>mmap</code>ed files though, so we can’t use it</li>
<li>The next best option are two: Either polling for file attribute
changes (e.g. last write), or by continously hashing the file to check
if it has changed</li>
<li>Polling on its own has a lot of downsides, like it adding a
guaranteed minimum latency by virtue of having to wait for the next
polling cycle</li>
<li>This negatively impacts a maximum allowed downtime scenario, where
the overhead of polling can make or break a system</li>
<li>Hashing the entire file is also a naturally IO- and CPU-intensive
process because the entire file needs to be read at some point</li>
<li>Still, polling &amp; hashing is probably the only reliable way of
detecting changes to a file</li>
<li>Instead of hashing the entire file, then syncing the entire file, we
can want to really sync only the parts of the file that have changed
between two polling iterations</li>
</ul></li>
<li>Detecting file changes
<ul>
<li>We can do this by opening up the file multiple times, then hashing
individual offsets, and aggregating the chunks that have changed</li>
<li>When picking algorithms for this hashing process, the most important
metric to consider is the throughput with which it can compute hashes,
as well as the change of collisions</li>
<li>If the underlying hashing algorithm is CPU-bound, this also allows
for better concurrent processing</li>
<li>Increases the initial latency/overhead by having to open up multiple
file descriptors</li>
<li>But this can not only increase the speed of each individual polling
tick, it can also drastically decrease the amount of data that needs to
be transferred since only the delta needs to be synchronized</li>
<li>Hashing and/or syncing individual chunks that have changed is a
common practice</li>
</ul></li>
<li>Delta synchronization protocol
<ul>
<li>We have implemented a simple protocol for this delta
synchronization, just like rsync’s delta synchronization algorithm (code
snippet from
https://github.com/loopholelabs/darkmagyk/blob/master/cmd/darkmagyk-orchestrator/main.go#L1337-L1411
etc.)</li>
<li>For this protocol specifically, we send the changed file’s name as
the first message when starting the synchronization, but a simple
multiplexing system could easily be implemented by sending a file ID
with each message</li>
<li>Its intended to be simpler than <code>rsync</code>, and to support a
central forwarding up instead of requiring P2P connectivity between each
host</li>
<li>Defines three actors: Multiplexer, file advertiser, and file
receiver</li>
<li>TODO: Add sequence diagram for the protocol</li>
</ul></li>
<li>Multiplexer hub
<ul>
<li>Accepts TLS connections</li>
<li>Reads client certificate</li>
<li>Reads the common name from the certificate, contains the
<code>syncerID</code></li>
<li>Spawns a new goroutine for the syncerID to handle the communication
with the specific syncer</li>
<li>In goroutine it reads the type of the peer</li>
<li>For the <code>src-control</code> peer type (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4af/cmd/darkmagyk-cloudpoint/main.go#L824-L844)
<ul>
<li>Reads a file name from the syncer</li>
<li>Registers the connection as the one providing the file with this
name</li>
<li>Broadcasts the file as one now being available</li>
</ul></li>
<li>For the <code>dst-control</code> peer type (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4af/cmd/darkmagyk-cloudpoint/main.go#L845-L880)
<ul>
<li>It listens to the broadcasted files from <code>src-control</code>
peers</li>
<li>Relays any received file names to the <code>dst-control</code> peers
so that it can subscribe</li>
<li>Also sends all currently known file names to the peer</li>
</ul></li>
<li>For the <code>dst</code> peer type (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4af/cmd/darkmagyk-cloudpoint/main.go#L882-L942)
<ul>
<li>Decodes a file name from the connection</li>
<li>Looks for a corresponding <code>src-control</code> peer</li>
<li>If it has found a peer, it creates and sends a new ID for this
connection to the <code>src-control</code> peer</li>
<li>Waits until the <code>src-control</code> peer has connected to the
hub with this ID with a new <code>src-data</code> peer by listening for
<code>src-data</code> peer ID broadcasts</li>
<li>Spawns two new goroutines that copy to/from this newly created
synchronization connection and the connection of the <code>dst</code>
peer, relaying any information between the two</li>
</ul></li>
<li>For the <code>src-data</code> peer type (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4af/cmd/darkmagyk-cloudpoint/main.go#L943-L954)
<ul>
<li>Decodes the ID for the peer</li>
<li>Broadcasts the ID, which allows the <code>dst</code> peer to
continue</li>
</ul></li>
</ul></li>
<li>File advertisement (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4afe0828452c61c63233aa134fb3c2a45a1a/pkg/filetransfer/src.go#L112-L269)
<ul>
<li>Connects to the multiplexer hub</li>
<li>Registers itself as a <code>src-control</code> type of peer</li>
<li>Sends the file name to the multipler hub</li>
<li>Starts a loop to handle <code>dst</code> peer types</li>
<li>As mentioned earlier, <code>dst</code> peer types send a ID</li>
<li>Waits until ID is received, then spawns new goroutine</li>
<li>Connects to the multiplexer hub again, this time registering itself
as a <code>src-data</code> peer</li>
<li>Opens the file that it advertises</li>
<li>Sends the ID that it has received back to the multiplexer hub, which
connects it to the matching <code>dst</code> peer</li>
<li>Starts a new loop that starts the file transmission to the
<code>dst</code> peer through the multiplexer</li>
<li>Checks if context is cancelled to stop the transmission by breaking
the loop if requested</li>
<li>After a file transmission is done, waits for the specified polling
interval, then starts a file transmission cycle again</li>
</ul></li>
<li>File receiver loop (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4afe0828452c61c63233aa134fb3c2a45a1a/pkg/filetransfer/dst.go#L127-L243)
<ul>
<li>Connects to the multiplexer hub</li>
<li>Registers itself as the <code>dst-control</code> type of peer</li>
<li>Spawns a new goroutine after a file name has been received from the
multiplexer hub</li>
<li>Connects to the multiplexer hub again, this time registering itself
as a <code>dst</code> peer</li>
<li>Opens up the destination file</li>
<li>Registers itself as the <code>dst</code> peer</li>
<li>Sends the file name to the multiplexer, causing the multiplexer to
look for a peer that advertises the requested file</li>
<li>Starts the file receiption process in a loop, and exits if a
callback determines that the file has been synced completely</li>
</ul></li>
<li>File transmission (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4afe0828452c61c63233aa134fb3c2a45a1a/pkg/filetransfer/src.go#L14-L110)
<ul>
<li>Does the actual file transmission part of delta synchronization</li>
<li>Receives the remote hashes from the mulitplexer hub</li>
<li>Calculates the local hashes</li>
<li>Compares local and remote hashes</li>
<li>Sends hashes that don’t match to the remote via the multiplexer
hub</li>
<li>If the remote sent less hashes than there were locally, asks the
remote to truncate it’s file to the size of the local file that is being
synchronized</li>
<li>Loops over each of the chunks that need to be sent, and sends the
updated data for the file in order</li>
</ul></li>
<li>Hash calculation (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4afe0828452c61c63233aa134fb3c2a45a1a/pkg/filetransfer/hashes.go#L17-L73)
<ul>
<li><code>GetHashesForBlocks</code> handles the concurrent calculation
of the hashes for a file on both the file transmitter and receiver</li>
<li>It uses a semaphore to limit the amount of concurrent access to the
file that is being hashed</li>
<li>Acquires a lock on the semaphore for each hash calculation
worker</li>
<li>Each worker goroutine then opens up the file and calculates a CRC32
hash</li>
<li>To allow for easy transmission of the hashes over the network, it
encodes them as hash values</li>
<li>After all hashes have been calculated, it adds them to the array and
returns them</li>
</ul></li>
<li>File receiption (code snippet from
https://github.com/loopholelabs/darkmagyk/blob/159d4afe0828452c61c63233aa134fb3c2a45a1a/pkg/filetransfer/dst.go#L23-L125)
<ul>
<li>Is the receiving part of the delta synchronization algorithm</li>
<li>Starts by calculating hashes for the local copy of the file</li>
<li>Sends the local hashes to the emote, then waits until it has
received the remote’s hashes</li>
<li>If the remote detected that the file needs to be truncated (by
sending a negative cutoff value), it truncates the file</li>
<li>If the remote’s file has grown beyond what the local file is since
the last synchronization cycle, it extends the file</li>
<li>It then reads from the remote in order, and copies each chunk to the
matching offset in the local file</li>
</ul></li>
</ul>
</section>
<section id="fuse-implementation-in-go" data-number="0.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span>
FUSE Implementation in Go</h3>
<ul>
<li>Implementing a FUSE in Go means tight integration with
libraries</li>
<li>It makes sense to divide the process into two aspects</li>
<li>Creating a backend for a file system abstraction API like
<code>afero.Fs</code>
<ul>
<li>By using a file system abstraction API like <code>afero.Fs</code>,
we can separate the FUSE implementation from the actual file system
structure, making it unit testable and making it possible to add caching
in user space (code snippet from
https://github.com/pojntfx/stfs/blob/main/pkg/fs/file.go)</li>
<li>It is possible to use even very complex and at first view
non-compatible backends as a FUSE file system’s backend</li>
<li>For example, STFS used a tape drive as the backend, which is not
random access, but instead append-only and linear
(https://github.com/pojntfx/stfs/blob/main/pkg/operations/update.go)</li>
<li>By using an on-disk index and access optimizations, the resulting
file system was still performant enough to be used, and supported almost
all features required for the average user</li>
<li>Creating an adapter between the FS abstraction API and the FUSE
library’s backend interface</li>
<li>It is possible to map any <code>afero.Fs</code> to a FUSE backend,
so it would be possible to switch between different file system backends
without having to write FUSE-specific (code snippet from
https://github.com/JakWai01/sile-fystem/blob/main/pkg/filesystem/fs.go)</li>
</ul></li>
<li>While this approach is interesting, the required overhead of
implementing it (which is known from prior projects like STFS<span class="citation" data-cites="pojtinger2022stfs">[31]</span> and
sile-fystem<span class="citation" data-cites="waibel2022silefystem">[32]</span>) and other factors that
will be touched upon later led to it not being pursued further</li>
</ul>
</section>
<section id="nbd-with-go-nbd" data-number="0.4.4">
<h3 data-number="1.4.4"><span class="header-section-number">1.4.4</span>
NBD with <code>go-nbd</code></h3>
<ul>
<li>Block Device-based synchronization
<ul>
<li>As hinted at before, a better API would be able to catch
reads/writes to a single <code>mmap</code>ed file instead of having to
implement a complete file system</li>
<li>It does however not have to be an actual file, a block device also
works</li>
<li>In Linux, block devices are (typically storage) devices that support
reading/writing fixed chunks (blocks) of data</li>
<li>We can <code>mmap</code> a block device in the same way that we can
<code>mmap</code> a file</li>
<li>Similarly to how a file system can be implemented in a kernel
module, a block device is typically implemented as a kernel module/in
kernel space</li>
<li>However, the same security, portability and developer experience
issues as with the former also apply here</li>
<li>Instead of implementing a FUSE to solve this, we can create a NBD
(network block device) server that can be used by the kernel NBD module
similarly to how the process that connected to the FUSE kernel module
functioned</li>
<li>The difference between a FUSE and NBD is that a NBD server doesn’t
provide a file system</li>
<li>A NBD server provides the storage device that (typically) hosts a
file system, which means that the interface for it is much, much
simpler</li>
<li>The implementation overhead of a NBD server’s backend is much more
similar to how <code>userfaultfd-go</code> works, rather than a
FUSE</li>
</ul></li>
<li>Implementing <code>go-nbd</code>
<ul>
<li>Due to the lack of pre-existing libraries, a new pure Go NBD library
was implemented</li>
<li>This library does not rely on CGo/a pre-existing C library, meaning
that a lot of context switching can be skipped</li>
<li>The backend interface for <code>go-nbd</code> is very simple and
only requires four methods: <code>ReadAt</code>, <code>WriteAt</code>,
<code>Size</code> and <code>Sync</code></li>
<li>A good example backend that maps well to a block device is the file
backend (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/backend/file.go)</li>
<li>The key difference here to the way backends were designed in
<code>userfaultfd-go</code> is that they can also handle writes</li>
<li><code>go-nbd</code> exposes a <code>Handle</code> function to
support multiple users without depending on a specific transport layer
(code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/server/nbd.go)</li>
<li>This means that systems that are peer-to-peer (e.g. WebRTC), and
thus don’t provide a TCP-style <code>accept</code> syscall can still be
used easily</li>
<li>It also allows for easily hosting NBD and other services on the same
TCP socket</li>
<li>The server encodes/decodes messages with the <code>binary</code>
package (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/server/nbd.go#L73-L76)</li>
<li>To make it easier to parse, the headers and other structured
messages are modeled as Go structs (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/protocol/negotiation.go)</li>
<li>The handshake is implemented using a simple for loop, which either
returns on error or breaks</li>
<li>The actual transmission phase is done similarly, by reading in a
header, switching on the message type and reading/sending the relevant
data/reply</li>
<li>The server is completely in user space, there are no kernel
components involved here</li>
<li>The NBD client however is implemented by using the kernel NBD
client</li>
<li>In order to use it, one needs to find a free NBD device first</li>
<li>NBD devices are pre-created by the NBD kernel module and more can be
specified with the <code>nbds_max</code> parameter</li>
<li>In order to find a free one, we can either specify it directly, or
check whether we can find a NBD device with zero size in
<code>sysfs</code> (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/utils/unused.go)</li>
<li>Relevant <code>ioctl</code> numbers depend on the kernel and are
extracted using <code>CGo</code> (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/ioctl/negotiation_cgo.go)</li>
<li>The handshake for the NBD client is negotiated in user space by the
Go program</li>
<li>Simple for loop, basically the same as for the server (code snippet
from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L221-L288)</li>
<li>After the metadata for the export has been fetched in the handshake,
the kernel NBD client is configured using <code>ioctl</code>s (code
snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L290-L328)</li>
</ul></li>
<li>Optimizations for the NBD implementation
<ul>
<li>The <code>DO_IT</code> syscall never returns, meaning that an
external system must be used to detect whether the device is actually
ready</li>
<li>Two ways of detecting whether the device is ready: By polling
<code>sysfs</code> for the size parameter, or by using
<code>udev</code></li>
<li><code>udev</code> manages devices in Linux</li>
<li>When a device becomes available, the kernel sends a
<code>udev</code> event, which we can subscribe to and use as a reliable
and idiomatic way of waiting for the ready state (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L104C10-L138)</li>
<li>In reality however, polling <code>sysfs</code> directly can be
faster than subscribing to the <code>udev</code> event, so we give the
user the option to switch between both options (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L140-L178)</li>
<li>When <code>open</code>ing the block device that the client has
connected to, usually the kernel does provide a caching mechanism and
thus requires <code>sync</code> to flush changes</li>
<li>By using <code>O_DIRECT</code> however, it is possible to skip the
kernel caching layer and write all changes directly to the NBD
client/server</li>
<li>This is particularly useful if both the client and server are on the
local system, and if the amount of time spent on <code>sync</code>ing
should be as small as possible</li>
<li>It does however require reads and writes on the device node to be
aligned to the system’s page size, which is possible to implement with a
client-side chunking system but does require application-specific
code</li>
</ul></li>
<li>Combining the NBD client and server
<ul>
<li>The server and client are connected by creating a connected UNIX
socket pair (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_direct.go#L59-L62)</li>
<li>By building on this basic direct mount, we can add a file (code
snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/file_direct.go) and
slice (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/slice_direct.go)
mount API, which allows for easy usage and integration with
<code>sync</code>/<code>msync</code> respectively</li>
<li>Using the <code>mmap</code>/slice approach has a few benefits</li>
<li>First, it makes it possible to use the byte slice directly as though
it were a byte slice allocated by <code>make</code>, except its
transparently mapped to the (remote) backend</li>
<li><code>mmap</code>/the byte slices also swaps out the syscall-based
file interface with a random access one, which allows for faster
concurrent reads from the underlying backend</li>
<li>Alternatively, it would also be possible to format the server’s
backend or the block device using standard file system tools</li>
<li>When the device then becomes ready, it can be mounted to a directory
on the system</li>
<li>This way it is possible to <code>mmap</code> one or multiple files
on the mounted file system instead of <code>mmap</code>ing the block
device directly</li>
<li>This allows for handling multiple remote regions using a single
server, and thus saving on initialization time and overhead</li>
<li>Using a proper file system however does introduce both storage
overhead and complexity, which is why e.g. the FUSE approach was not
chosen</li>
</ul></li>
</ul>
</section>
<section id="chunking-pushpull-mechanisms-and-lifecycle-for-mounts" data-number="0.4.5">
<h3 data-number="1.4.5"><span class="header-section-number">1.4.5</span>
Chunking, Push/Pull Mechanisms and Lifecycle for Mounts</h3>
<ul>
<li>The <code>ReadWriterAt</code> pipeline
<ul>
<li>In order to implement the chunking system, we can use a abstraction
layer that allows us to create a pipeline of readers/writers - the
<code>ReadWriterAt</code>, combining an <code>io.ReaderAt</code> and a
<code>io.WriterAt</code></li>
<li>This way, we can forward the <code>Size</code> and <code>Sync</code>
syscalls directly to the underlying backend, but wrap a backend’s
<code>ReadAt</code> and <code>WriteAt</code> methods in a pipeline of
other <code>ReadWriterAt</code>s</li>
<li>One such <code>ReadWriterAt</code> is the
<code>ArbitraryReadWriterAt</code> (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/arbitrary_rwat.go)</li>
<li>It allows breaking down a larger data stream into smaller
chunks</li>
<li>In <code>ReadAt</code>, it calculates the index of the chunk that
the offset falls into and the position within the offsets</li>
<li>It then reads the entire chunk from the backend into a buffer,
copies the necessary portion of the buffer into the input slice, and
repeats the process until all requested data is read</li>
<li>Similarly for the writer, it calculates the chunk’s index and
offset</li>
<li>If an entire chunk is being written to, it bypasses the chunking
system, and writes it directly to not have to unnecessarily copy the
data twice</li>
<li>If only parts of a chunk need to be written, it first reads the
complete chunk into a buffer, modifies the buffer with the data that has
changed, and writes the entire chunk back, until all data has been
written</li>
<li>This simple implementation can be used to allow for writing data of
arbitrary length at arbitrary offsets, even if the backend only supports
a few chunks</li>
<li>In addition to this chunking system, there is also a
<code>ChunkedReadWriterAt</code>, which ensures that the limits
concerning the maximum size supported by the backend and the actual
chunks are being respected</li>
<li>This is particularly useful when the client is expected to do the
chunking, and the server simply checks that the chunking system’s chunk
size is respected</li>
<li>In order to check if a read or write is valid, it checks whether a
read is done to an offset multiple of the chunk size, and whether the
length of the slice of data to read/write is the chunk size (code
snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/chunked_rwat.go)</li>
</ul></li>
<li>Background pull
<ul>
<li>The <code>Puller</code> component asynchronously pulls chunks in the
background (code snipped from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/puller.go)</li>
<li>After sorting the chunks, the puller starts a fixed number of worker
threads in the background, each of which ask for a chunk to pull</li>
<li>Note that the puller itself does not copy to/from a destination;
this use case is handled by a separate component</li>
<li>It simply reads from the provided <code>ReaderAt</code>, which is
then expected to handle the actual copying on its own</li>
<li>The actual copy logic is provided by the
<code>SyncedReadWriterAt</code> instead</li>
<li>This component takes both a remote reader and a local
<code>ReadWriterAt</code></li>
<li>If a chunk is read, e.g. by the puller component calling
<code>ReadAt</code>, it is tracked and market as remote by adding it to
a local map</li>
<li>The chunk is then read from the remote reader and written to the
local <code>ReadWriterAt</code>, and is then marked as locally
available, so that on the second read it is fetched locally
directly</li>
<li>A callback is then called which can be used to monitor the pull
process</li>
<li>Note that if it is used in a pipeline with the <code>Puller</code>,
this also means that if a chunk which hasn’t been fetched asynchronously
yet will be scheduled to be pulled immediately</li>
<li>WriteAt also starts by tracking a chunk, but then immediately marks
the chunk as available locally no matter whether it has been pulled
before</li>
<li>The combination of the <code>SyncedReadWriterAt</code> and the
<code>Puller</code> component implements the pull post-copy system in a
modular and testable way</li>
<li>Unlike the usual way of only fetching chunks when they are available
however, this system also allows fetching them pre-emptively, gaining
some benefits of pre-copy migration, too</li>
<li>Using this <code>Puller</code> interface, it is possible to
implement a read-only managed mount</li>
<li>This is very similar the <code>rr+</code> prefetching mechanism from
“Remote Regions” (reference atc18-aguilera)</li>
</ul></li>
<li>Background push
<ul>
<li>Chunks that have changed/are pushable are marked with
<code>MarkOffsetPushable</code> (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L171C24-L185)</li>
<li>Once opened, the pusher starts a new goroutine in the background
which calls <code>Sync</code> in a set recurring interval (code snippet
from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/pusher.go)</li>
<li>Once sync is called by this background worker system or manually, it
launches workers in the background</li>
<li>These workers all wait for a chunk to handle</li>
<li>Once a worker receives a chunk, it reads it from the local
<code>ReadWriter</code>, and copies it to the remote</li>
<li>Safe access is ensures by individually locking each chunk</li>
<li>The pusher also serves as a step in the <code>ReadWriterAt</code>
pipeline</li>
<li>In order to do this, it exposes <code>ReadAt</code> and
<code>WriteAt</code></li>
<li><code>ReadAt</code> is a simple proxy, while <code>WriteAt</code>
also marks a chunk as pushable (since it mutates data) before writing to
the local <code>ReadWriterAt</code></li>
<li>For the direct mount, the NBD server was directly connected to the
remote, while in this setup a pipeline of pullers, pushers, a syncer and
an <code>ArbitraryReadWriter</code> is used (graphic of the four systems
and how they are connected to each other vs. how the direct mounts
work)</li>
<li>For a read-only scenario, the <code>Pusher</code> step is simply
skipped (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L142-L169)</li>
</ul></li>
<li>Parallelizing NBD device initialization
<ul>
<li>If no background pulls are enabled, the creation of the
<code>Puller</code> is simply skipped (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L187-L222)</li>
<li>This setup allows pulling from the remote <code>ReadWriterAt</code>
before the NBD device is open</li>
<li>This means that we can start pulling in the background as the NBD
client and server are still starting (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L251-L272)</li>
<li>These two components typically start fairly quickly, but can still
take multiple ms</li>
<li>Often, it takes as long as one RTT, so parallelizing this startup
process can significantly reduce the initial latency and pre-emptively
pull quite a bit of data</li>
</ul></li>
<li>Implementing device lifecycles
<ul>
<li>Using this simple interface also makes the entire system very
testable</li>
<li>In the tests, a memory reader or file can be used as the local or
remote <code>ReaderWriterAt</code>s and then a simple table-driven test
can be used (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/puller_test.go)</li>
<li>With all of these components in place, the managed mounts API serves
as a fast and efficient option to access almost any remote resource in
memory</li>
<li>Similarly to how the direct mounts API used the basic path mount to
build the file and <code>mmap</code> interfaces, the managed mount
builds on this interface in order to provide the same interfaces</li>
<li>It is however a bit more complicated for the lifecycle to work</li>
<li>For example, in order to allow for a <code>Sync()</code> API,
e.g. the <code>msync</code> on the <code>mmap</code>ed file must happen
before <code>Sync()</code> is called on the syncer</li>
<li>This is done through a hooks system (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/file_managed.go#L34-L37)</li>
<li>The same hooks system is also used to implement the correct
lifecycle when <code>Close</code>ing the mount</li>
</ul></li>
<li>Optimization for WAN
<ul>
<li>Another optimization that has been made to support this WAN
deployment scenario is the pull-only architecture</li>
<li>Usually, a pre-copy system pushes changes to the destination in the
migration API</li>
<li>This however makes such a system hard to use in a scenario where
NATs exist, or a scenario in which the network might have an outage
during the migration</li>
<li>With a pull-only system emulating the pre-copy setup, the client can
simply keep track of which chunks it still needs to pull itself, so if
there is a network outage, it can just resume pulling like before, which
would be much harder to implement with a push system as the server would
have to track this state for multiple clients and handle the lifecycle
there</li>
<li>The pull-only system also means that unlike the push system that was
implemented for the hash-based synchronization, a central forwarding hub
is not necessary</li>
<li>In the push-based system for the hash-based solution, the topology
had to be static/all destinations would have to have received the
changes made to the remote app’s memory since it was started</li>
<li>In order to cut down on unnecessary duplicate data transmissions, a
central forwarding hub was implemented</li>
<li>This central forwarding hub does however add additional latency,
which can be removed completely with the migration protocol’s P2P,
pull-only algorithm</li>
</ul></li>
</ul>
</section>
<section id="live-migration-for-mounts" data-number="0.4.6">
<h3 data-number="1.4.6"><span class="header-section-number">1.4.6</span>
Live Migration for Mounts</h3>
<ul>
<li>Optimization mounts for migration scenarios
<ul>
<li>The flexible architecture of the <code>ReadWriterAt</code>
components allow the reuse of lots of code for both use cases</li>
</ul></li>
<li>Implementing the seeder
<ul>
<li>To achieve this, the seeder defines a simple read-only API with the
familiar <code>ReadAt</code> methods, but also new APIs such as
returning dirty chunks from <code>Sync</code> and adding a
<code>Track</code> method (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/seeder.go#L15-L21)</li>
<li>Unlike the remote backend, a seeder also exposes a mount through a
path, file or byte slice, so that as the migration is happening, the
underlying data can still be accessed by the application</li>
<li>The tracking aspect is implemented in the same modular and
composable way as the syncer etc. - by using a
<code>TrackingReadWriterAt</code> that is connected to the seeder’s
<code>ReadWriterAt</code> pipeline (graphic of the pipeline here)</li>
<li>Once activated by <code>Track</code>, the tracker intercepts all
<code>WriteAt</code> calls and adds them to a local de-duplicated store
(code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/tracking_rwat.go#L28-L40)</li>
<li>When <code>Sync</code> is called, the changed chunks are returned
and the de-duplicated store is cleared</li>
<li>A benefit of the protocol being defined in such a way that only the
client ever calls an RPC, not the other way around, is that the
transport layer/RPC system is completely interchangeable</li>
<li>This works by returning a simple abstract <code>service</code>
utility struct struct from <code>Open</code> (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/seeder.go)</li>
<li>This abstract struct can then be used as the backend for any RPC
framework, e.g. for gRPC (code snippet
https://github.com/pojntfx/r3map/blob/main/pkg/services/seeder_grpc.go)</li>
</ul></li>
<li>Implementing the leecher
<ul>
<li>The leecher then takes this abstract API as an argument</li>
<li>As soon as the leecher is opened, it calls track on the seeder, and
in parallel starts the device (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/migration/path_leecher.go#L144-L203)</li>
<li>The leecher introduces a new component, the
<code>LockableReadWriterAt</code>, into its internal pipeline (add
graphic of the internal pipeline)</li>
<li>This component simply blocks all read and write operations to/from
the NBD device until <code>Finalize</code> has been called (code snippet
from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/lockable_rwat.go#L19-L37)</li>
<li>This is required because otherwise stale data (since
<code>Finalize</code> did not yet mark the changed chunks) could have
poisoned the cache on the e.g. <code>mmap</code>ed device</li>
<li>Once the leecher has started the device, it sets up a syncer (code
snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/migration/path_leecher.go#L214-L252)</li>
<li>A callback can be used to monitor the pull process (code snippet
from
https://github.com/pojntfx/r3map/blob/main/cmd/r3map-migration-benchmark/main.go#L544-L548)</li>
<li>As described before, after a satisfactory local availability level
has been reached, <code>Finalize</code> can be called</li>
<li><code>Finalize</code> then calls <code>Sync()</code> on the remote,
marks the changed chunks as remote, and schedules them also be pulled in
the background (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/migration/path_leecher.go#L257-L280)</li>
<li>As an additional measure aside from the lockable
<code>ReadWriterAt</code> to make accessing the path/file/slice too
early harder, only <code>Finalize</code> returns the managed object, so
that the happy path can less easily lead to deadlocks</li>
<li>After a leecher has successfully reached 100% local availability, it
calls <code>Close</code> on the seeder and disconnects the leecher from
the seeder, causing both to shut down (code snippet from
https://github.com/pojntfx/r3map/blob/main/cmd/r3map-migration-benchmark-server/main.go#L137)</li>
<li>Once the leecher has exited, a seeder can be started, to allow for
migrating from the destination to another destination again</li>
</ul></li>
</ul>
</section>
<section id="pluggable-encryption-and-authentication" data-number="0.4.7">
<h3 data-number="1.4.7"><span class="header-section-number">1.4.7</span>
Pluggable Encryption and Authentication</h3>
<ul>
<li>Compared to existing remote mount and migration solutions, r3map is
a bit special</li>
<li>As mentioned before, most systems are designed for scenarios where
such resources are accessible in a high-bandwidth, low-latency LAN</li>
<li>This means that some assumptions concerning security,
authentication, authorization and scalability were made that can not be
made here</li>
<li>For example encryption; while for a LAN deployment scenario it is
probably assumed that there are no bad actors in the subnet, the same
can not be said for WAN</li>
<li>While depending on e.g. TLS etc. for the migration could have been
an option, r3map should still be useful for LAN migration use cases,
too, which is why it was made to be completely transport-agnostic</li>
<li>This makes adding encryption very simple</li>
<li>E.g. for LAN, the same assumptions that are being made in existing
systems can be made, and fast latency-sensitive protocols like the SCSI
RDMA protocol (SRP) or a bespoke protocol can be used</li>
<li>For WAN, a standard internet protocol like TLS over TCP or QUIC can
be used instead, which will allow for migration over the public
internet, too</li>
<li>For RPC frameworks with exchangeable transport layers such as
dudirekta (will be explained later), this also allows for unique
migration or mount scenarios in NATed environments over WebRTC data
channels, which would be very hard to implement with more traditional
setups</li>
<li>Similarly so, authentication and authorization can be implemented in
many ways</li>
<li>While for migration in LAN, the typical approach of simply trusting
the local subnet can be used, for public deployments mTLS certificates
or even higher-level protocols such as OIDC can be used depending on the
transport layer chosen</li>
<li>For WAN specifically, new protocols such as QUIC allow tight
integration with TLS for authentication and encryption</li>
<li>While less relevant for the migration use case (since connections
can be established ahead of time), for the mount use case the initial
remote <code>ReadAt</code> requests’ latency is an important metric
since it strongly correlates with the total latency</li>
<li>QUIC has a way to establish 0-RTT TLS, which can save one or
multiple RTTs and thus signficantly reduce this overhead, and handle
authentication in the same step</li>
</ul>
</section>
<section id="optimizing-backends-for-high-rtt" data-number="0.4.8">
<h3 data-number="1.4.8"><span class="header-section-number">1.4.8</span>
Optimizing Backends For High RTT</h3>
<ul>
<li>In WAN, where latency is high, the ability to fetch chunks
concurrently is very important</li>
<li>Without concurrent background pulls, latency adds up very quickly as
every memory request would have at least the RTT as latency</li>
<li>The first prerequisite for supporting this is that the remote
backend has to be able to read from multiple regions without locking the
backend globally</li>
<li>For the file backend for example, this is not the case, as the lock
needs to be acquired for the entire file before an offset can be
accessed (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/backend/file.go#L17-L25)</li>
<li>For high-latency scenarios, this can quickly become a
bottleneck</li>
<li>While there are many ways to solve this, one is to use the directory
backend</li>
<li>Instead of using just one backing file, the directory backend is a
chunked backend that uses a directory with one file for each chunk
instead of a global file</li>
<li>This means that the directory backend can lock each file
individually, speeding up concurrent access</li>
<li>This also applies to writes, where even concurrent writes to
different chunks can be done at the same time as they are all backed by
a separate file</li>
<li>The directory backend keeps track of these chunks by using an
internal map of locks (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/backend/directory.go#L22-L24)</li>
<li>When a chunk is first accessed, a new file is created for the chunk
(code snipped from
https://github.com/pojntfx/r3map/blob/main/pkg/backend/directory.go#L77-L94)</li>
<li>If the chunk is being read, the file is also truncated to one chunk
length</li>
<li>Since this could easily exhaust the number of maximum allowed file
descriptors for a process, a check is added (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/backend/directory.go#L55-L75)</li>
<li>If the maximum allowed number of open files is exceeded, the first
file is closed and removed from the map, causing it to be reopened on a
subsequent read</li>
<li>These optimizations add an initial overhead to operations, but can
significantly improve the pull speed in scenarios where the backing disk
is slow or the latency is high</li>
</ul>
</section>
<section id="optimizing-the-transport-protocol-for-throughput" data-number="0.4.9">
<h3 data-number="1.4.9"><span class="header-section-number">1.4.9</span>
Optimizing The Transport Protocol For Throughput</h3>
<ul>
<li>Despite these benefits, gRPC is not perfect however</li>
<li>Protobuf specifically, while being faster than JSON, is not the
fastest serialization framework that could be used</li>
<li>This is especially true for large chunks of data, and becomes a real
bottleneck if the connection between source and destination would allow
for a high throughput</li>
<li>This is where fRPC, a RPC library that is easy to replace gRPC with,
becomes useful</li>
<li>Because throughput and latency determine the maximum acceptable
downtime of a migration/the initial latency for mounts, choosing the
right RPC protocol is an important decision</li>
<li>fRPC also uses the same proto3 DSL, which makes it an easy drop-in
replacement, and it also supports multiplexing and connection
polling</li>
<li>Because of these similarities, the usage of fRPC in r3map is
extremely similar to gRPC (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/seeder_frpc.go)</li>
<li>A good way to test how well the RPC framework scales for concurrent
requests is scaling the amount of pull workers, where dudirekta only
gains marginally from increasing their number, whereas both gRPC and
fRPC can increase throughput and decrease the initial latency by pulling
more chunks pre-emptively</li>
</ul>
</section>
<section id="using-remote-stores-as-backends" data-number="0.4.10">
<h3 data-number="1.4.10"><span class="header-section-number">1.4.10</span> Using Remote Stores as
Backends</h3>
<ul>
<li>Using key-value stores as ephemeral mounts
<ul>
<li>RPC backends provide a way to access a remote backend</li>
<li>This is useful, esp. if the remote resource should be protected in
some way or if it requires some kind of authorization</li>
<li>Depending on the use case however, esp. for the mount API, having
access to a remote backend without this level of indirection can be
useful</li>
<li>Fundamentally, a mount maps fairly well to a remote random-access
storage device</li>
<li>Many existing protocols and systems provide a way to access
essentially this concept over a network</li>
<li>One of these is Redis, an in-memory key-value store with network
access</li>
<li>Chunk offsets can be mapped to keys, and bytes are a valid key type,
so the chunk itself can be stored directly in the KV store (code snippet
from
https://github.com/pojntfx/r3map/blob/main/pkg/backend/redis.go#L36-L63)</li>
<li>Using Redis is particularly useful because it is able to handle the
locking server-side in a very efficient way, and is well-tested for
high-throughput etc. scenarious</li>
<li>Authentication can also be handled using the Redis protocol, so can
multi-tenancy by using multiple databases or a prefix</li>
<li>Redis also has very fast read/write speeds due to its bespoke
protocol and fast serialization</li>
</ul></li>
<li>Mapping large resources into memory with S3
<ul>
<li>While the Redis backend is very useful for read-/write usage, when
deployment to the public internet is required, it might not be the best
one</li>
<li>The S3 backend is an good choice for mapping public information,
e.g. media assets, binaries, large read-only filesystems etc. into
memory</li>
<li>S3 used to be an object storage service from AWS, but has since
become a more or less standard way for accessing blobs thanks to
open-source S3 implementations such as Minio</li>
<li>Similarly to how files were used as individual files, one S3 object
per chunk is used to store them</li>
<li>S3 is based on HTTP, and like the Redis backend requires chunking
due to it not supporting updates of part of a file</li>
<li>In order to prevent having to store empty data, the backend
interprets “not found” errors as empty chunks</li>
</ul></li>
<li>Document databases as persistent mount remotes
<ul>
<li>Another backend option is a NoSQL server such as Cassandra</li>
<li>Specifically, as noted earlier, ScyllaDB - which improves upon
Cassandra’s latency, which is important for the mount API</li>
<li>This is more of a proof of concept than a real usecase, but shows
the versitility and flexibility of how a database can be mapped to a
memory region, which can be interesting for accessing e.g. a remote
database’s content without having to use a specific client</li>
<li><code>ReadAt</code> and <code>WriteAt</code> are implemented using
Cassandra’s query language (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/backend/cassandra.go#L36-L63)</li>
<li>Similarly to Redis, locking individual keys can be handled by the DB
server</li>
<li>But in the case of Cassandra, the DB server stores chunks on disk,
so it can be used for persistent data</li>
<li>In order to use Cassandra, migrations have to be applied for
creating the table etc. (code snippet from
https://github.com/pojntfx/r3map/blob/main/cmd/r3map-direct-mount-benchmark/main.go#L369-L396)</li>
</ul></li>
</ul>
</section>
<section id="bi-directional-protocols-with-dudirekta" data-number="0.4.11">
<h3 data-number="1.4.11"><span class="header-section-number">1.4.11</span> Bi-Directional Protocols
With Dudirekta</h3>
<ul>
<li>Another aspect that plays an important role in performance for
real-life deployments is the choice of RPC framework and transport
protocol</li>
<li>As mentioned before, both the mount and the migration APIs are
transport-independent</li>
<li>A simple RPC framework to use is dudirekta</li>
<li>Dudirekta is reflection-based, which makes it very simple to use to
iterate on the protocol quickly</li>
<li>To use it, a simple wrapper struct with the needed RPC methods is
created (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/backend.go#L41-L61)</li>
<li>This wrapper struct simply calls the backend (or seeder etc.)
functions</li>
<li>The wrapper struct is then passed as the local function struct into
a registry, which creates the RPC server (code snippet from
https://github.com/pojntfx/r3map/blob/main/cmd/r3map-mount-benchmark-server/main.go#L146-L166)</li>
<li>When the transport protocol, in this case TCP, <code>accept</code>s
a client it is linked to the registry (code snippet from
https://github.com/pojntfx/r3map/blob/main/cmd/r3map-mount-benchmark-server/main.go#L198-L200)</li>
<li>The used protocol is very simple (code snippet from
https://github.com/pojntfx/dudirekta#protocol)</li>
<li>If an RPC, such as <code>ReadAt</code>, is called, it is looked up
via reflection and validated (code snippet from
https://github.com/pojntfx/dudirekta/blob/main/pkg/rpc/registry.go#L323-L357)</li>
<li>The arguments, which have been supplied as JSON, are then
unmarshalled into their native types, and the local wrapper struct’s
method is called in a new goroutine (code snippet from
https://github.com/pojntfx/dudirekta/blob/main/pkg/rpc/registry.go#L417-L521)</li>
<li>This allows for one important feature: Concurrent RPC calls</li>
<li>Many simple RPC frameworks only support one RPC call at a time,
because no demuxing is implemented</li>
<li>For example, when dRPC (https://github.com/storj/drpc) was used,
drastic performance issues were noted compared to gRPC etc., because no
support for concurrent RPCs was implemented</li>
<li>To use a RPC backend on the destination side, the wrapper struct’s
remote representation is used (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/backend.go#L14-L19)</li>
<li>For the destination site, the remote representation’s fields are
iterated over, and replaced by functions which marshal and unmarshal the
function calls into the dudirekta JSON protocol (code snippet from
https://github.com/pojntfx/dudirekta/blob/main/pkg/rpc/registry.go#L228-L269)</li>
<li>To do this, the arguments are marshalled into JSON, and a unique
call ID is generated (code snippet from
https://github.com/pojntfx/dudirekta/blob/main/pkg/rpc/registry.go#L109-L130)</li>
<li>Once the remote has responded with a message containing the unique
call ID, it unmarshalls the arguments, and returns (code snippet from
https://github.com/pojntfx/dudirekta/blob/main/pkg/rpc/registry.go#L145-L217)</li>
<li>This makes both calling RPCs and defining them completely
transparent to the user</li>
<li>Since dudirekta has a few limitations (such as the fact that slices
are passed as copies, not references, and that context needs to be
provided), the resulting remote struct can’t be used directly</li>
<li>To work around this, the standard <code>go-nbd</code> backend
interface is implemented for the remote representation, creating a
universally reusable, generic RPC backend wrapper (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/backend/rpc.go)</li>
<li>The same backend implementation is also done for the seeder protocol
(code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/seeder.go)</li>
<li>While the dudirekta RPC serves as a good reference implementation of
the basic RPC protocol, it does not scale particularly well</li>
<li>This mostly stems from two aspects of how it is designed</li>
<li>JSON(L) is used for the wire format, which while simple and easy to
analyze, is slow to marshal and unmarshal</li>
<li>Dudirekta supports defining functions on both the client and the
server</li>
<li>This is very useful for implementing e.g. a pre-copy protocol where
the source pushes chunks to the destination by simply calling a RPC on
the destination</li>
<li>Usually, RPCs don’t support exposing or calling RPCs on the client,
too, only on the server</li>
<li>This would mean that in order to implement a pre-copy protocol with
pushes, the destination would have to be <code>dial</code>able from the
source</li>
<li>In a LAN scenario, this is easy to implement, but in WAN it is
complicated and requires authentication of both the client and the
server</li>
<li>Dudirekta fixes this by making the protocol itself bi-directional
(example and code snippet from
https://github.com/pojntfx/dudirekta/tree/main#1-define-local-functions)</li>
<li>In addition to this, dudirekta works over any
<code>io.ReadWriter</code></li>
</ul>
</section>
<section id="connection-pooling-for-high-rtt-scenarios" data-number="0.4.12">
<h3 data-number="1.4.12"><span class="header-section-number">1.4.12</span> Connection Pooling For High
RTT Scenarios</h3>
<ul>
<li>This does however come at the cost of not being able to do
connection pooling, since each client <code>dial</code>ing the server
would mean that the server could not reference the multiple client
connections as one composite client without changes to the protocol</li>
<li>While implementing such a pooling mechanism in the future could be
interesting, it turned out to not be necessary thanks to the pull-based
pre-copy solution described earlier</li>
<li>Instead, only calling RPCs exposed on the server from the client is
the only requirement for an RPC framework, and other, more optimized RPC
frameworks can already offer this</li>
<li>Dudirekta uses reflection to make the RPCs essentially almost
transparent to use</li>
<li>By switching to a well-defined protocol with a DSL instead, we can
gain further benefits from not having to use reflection and generating
code instead</li>
<li>One popular such framework is gRPC</li>
<li>gRPC is a high-performance RPC framework based on Protocol
Buffers</li>
<li>Because it is based on Protobuf, we can define the protocol itself
in the <code>proto3</code> DSL</li>
<li>The DSL also allows to specify the order of each field in the
resulting wire protocol, making it possible to evolve the protocol over
time without having to break backwards compatibility</li>
<li>This is very important given that r3map could be ported to a
language with less overhead in the future, e.g. Rust, and being able to
re-use the existing wire protocol would make this much easier</li>
<li>While dudirekta is a simple protocol that is easy to adapt for other
languages, currently it only supports Go and JS, while gRPC supports
many more</li>
<li>A fairly unique feature of gRPC are streaming RPCs, where a stream
of requests can be sent to/from the server/client, which, while not used
for the r3map protocol, could be very useful to implementing a pre-copy
migration API with pushes similarly to how dudirekta does it by exposing
RPCs from the client</li>
<li>As mentioned before, for WAN migration or mount scenarios, things
like authentication, authorization and encryption are important, which
gRPC is well-suited for</li>
<li>Protobuf being a proper byte-, not plaintext-based wire format is
also very helpful, since it means that e.g. sending bytes back from
<code>ReadAt</code> RPCs doesn’t require any encoding (wereas JSON, used
by dudirekta, <code>base64</code> encodes these chunks)</li>
<li>gRPC is also based on HTTP/2, which means that it can benefit from
existing load balancing tooling etc. that is popular in WAN for web uses
even today</li>
<li>This backend is implemented by first defining the protocol in the
DSL (code snippet from
https://github.com/pojntfx/r3map/blob/main/api/proto/migration/v1/seeder.proto)</li>
<li>After generating the bindings, the generated backend interface is
implemented by using the dudirekta wrapper struct as the abstraction
layer (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/seeder_grpc.go)</li>
<li>Unlike dudirekta, gRPC also implements concurrent RPCs and
connection pooling</li>
<li>Similarly to how having a backend that allows concurrent
reads/writes can be useful to speed up the concurrent push/pull steps,
having a protocol that allows for concurrent RPCs can do the same</li>
<li>Connection pooling is another aspect that can help with this</li>
<li>Instead of either using one single connection with a multiplexer
(which is possible because it uses HTTP/2) to allow for multiple
requests, or creating a new connection for every request, gRPC is able
to intelligently re-use existing connections for RPCs or create new
ones, speeding up parallel requests</li>
</ul>
</section>
</section>
<section id="results" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span>
Results</h2>
<section id="latency" data-number="0.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span>
Latency</h3>
<ul>
<li><p>First chunk latency of baseline vs. userfault vs. direct
vs. managed</p></li>
<li><p>Development of first chunk latency as RTT increases</p></li>
<li><p>Development of first chunk latency as number of workers
increases</p></li>
<li><p>MB of pre-emptive pulls in managed API as RTT increases</p></li>
<li><p>Development of MB of pre-emptive pulls as RTT increases</p></li>
<li><p>Development of MB of pre-emptive pulls as number of workers
increases</p></li>
<li><p>Development of MB of pre-emptive pulls as chunk size
increases</p></li>
</ul>
</section>
<section id="throughput" data-number="0.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span>
Throughput</h3>
<ul>
<li><p>Throughput of different Go hashing algorithms</p></li>
<li><p>Throughput of dudirekta, gRPC and fRPC as RTT increases</p></li>
<li><p>Throughput of dudirekta, gRPC and fRPC as number of workers
increases</p></li>
<li><p>Throughput of dudirekta, gRPC and fRPC as chunk size
increases</p></li>
<li><p>Throughput of baseline vs. userfault vs. direct vs. migration (we
can use the <code>onChunkLocal</code> value)</p></li>
<li><p>Development of throughput as RTT increases</p></li>
<li><p>Development of throughput as number of workers increases
(migration only)</p></li>
<li><p>Development of throughput as chunk size increases</p></li>
</ul>
</section>
<section id="pull-heuristics" data-number="0.5.3">
<h3 data-number="1.5.3"><span class="header-section-number">1.5.3</span>
Pull Heuristics</h3>
<ul>
<li>Effect of correct (return n) vs wrong (return -n) pull heuristic on
first chunk latency for managed API</li>
<li>Effect of correct (return n) vs wrong (return -n) pull heuristic on
throughput for managed API</li>
</ul>
</section>
<section id="downtime" data-number="0.5.4">
<h3 data-number="1.5.4"><span class="header-section-number">1.5.4</span>
Downtime</h3>
<ul>
<li><code>Open</code> time for userfault vs. r3map (doesn’t matter if
direct/mount/migration)</li>
<li>Minimum downtime of direct/mount (one step - <code>Open</code> =
downtime) vs. migration (two steps - <code>Finalize</code> =
downtime)</li>
</ul>
</section>
</section>
<section id="discussion" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span>
Discussion</h2>
<section id="userfaults" data-number="0.6.1">
<h3 data-number="1.6.1"><span class="header-section-number">1.6.1</span>
Userfaults</h3>
<ul>
<li>As we can see, using <code>userfaultfd</code> we are able to map
almost any object into memory</li>
<li>This approach is very clean and has comparatively little overhead,
but also has significant architecture-related problems that limit its
uses</li>
<li>The first big problem is only being able to catch page faults - that
means we can only ever respond the first time a chunk of memory gets
accessed, all future requests will return the memory directly from RAM
on the destination host</li>
<li>This prevents us from using this approach for remote resources that
update over</li>
<li>Also prevents us from using it for things that might have concurrent
writers/shared resources, since there would be no way of updating the
conflicting section</li>
<li>Essentially makes this system only usable for a read-only “mount” of
a remote resource, not really synchronization</li>
<li>Also prevents pulling chunks before they are being accessed without
layers of indirection</li>
<li>The <code>userfaultfd</code> API socket is also synchronous, so each
chunk needs to be sent one after the other, meaning that it is very
vulnerable to long RTT values</li>
<li>Also means that the initial latency will be at minimum the RTT to
the remote source, and (without caching) so will be each future
request</li>
<li>The biggest problem however: All of these drawbacks mean that in
real-life usecases, the maximum throughput, even if a local process
handles page faults on a modern computer, is ~50MB/s</li>
<li>In summary, while this approach is interesting and very idiomatic to
Go, for most data, esp. larger datasets and in high-latency scenarios/in
WAN, we need a better solution</li>
</ul>
</section>
<section id="file-based-synchronization-1" data-number="0.6.2">
<h3 data-number="1.6.2"><span class="header-section-number">1.6.2</span>
File-Based Synchronization</h3>
<ul>
<li>Similarly to <code>userfaultfd</code>, this system also has
limitations</li>
<li>While <code>userfaultfd</code> was only able to catch reads, this
system is only able to catch writes to the file</li>
<li>Essentially this system is write-only, and it is very inefficient to
add hosts to the network later on</li>
<li>As a result, if there are many possible destinations to migrate
state too, a star-based architecture with a central forwarding hub can
be used</li>
<li>The static topology of this approach can be used to only ever
require hashing on one of the destinations and the source instead of all
of them</li>
<li>This way, we only need to push the changes to one component (the
hub), instead of having to push them to each destination on their
own</li>
<li>The hub simply forwards the messages to all the other
destinations</li>
</ul>
</section>
<section id="fuse" data-number="0.6.3">
<h3 data-number="1.6.3"><span class="header-section-number">1.6.3</span>
FUSE</h3>
<ul>
<li>FUSE does however also have downsides</li>
<li>It operates in user space, which means that it needs to do context
switching</li>
<li>Some advanced features aren’t available for a FUSE</li>
<li>The overhead of FUSE (and implementing a completely custom file
system) for synchronizing memory is still significant</li>
<li>If possible, the optimal solution would be to not expose a full file
system to track changes, but rather a single file</li>
<li>As a result of this, the significant implementation overhead of such
a file system led to it not being chosen</li>
</ul>
</section>
<section id="nbd" data-number="0.6.4">
<h3 data-number="1.6.4"><span class="header-section-number">1.6.4</span>
NBD</h3>
<ul>
<li>Limitations of NBD and <code>ublk</code> as an alternative
<ul>
<li>NBD is a battle-tested solution for this with fairly good
performance, but in the future a more lean implemenation called
<code>ublk</code> could also be used</li>
<li><code>ublk</code> uses <code>io_uring</code>, which means that it
could potentially allow for much faster concurrent access</li>
<li>It is similar to NBD; it also uses a user space server to provide
the block device backend, and a kernel <code>ublk</code> driver that
creates <code>/dev/ublkb*</code> devices</li>
<li>Unlike as it is the case for the NBD kernel module, which uses a
rather slow UNIX or TCP socket to communicate, <code>ublk</code> is able
to use <code>io_uring</code> pass-through commands</li>
<li>The <code>io_uring</code> architecture promises lower latency and
better throughput</li>
<li>Because it is however still experimental and docs are lacking, NBD
was chosen</li>
</ul></li>
<li><code>BUSE</code> and <code>CUSE</code> as alternatives to NBD
<ul>
<li>Another option of implementing a block device is BUSE (block devices
in user space)</li>
<li>BUSE is similar to FUSE in nature, and similarly to it has a kernel
and user space server component</li>
<li>Similarly to <code>ublk</code> however, BUSE is experimental</li>
<li>Client libraries in Go are also experimental, preventing it from
being used as easily as NBD</li>
<li>Similarly so, a CUSE could be implemented (char device in user
space)</li>
<li>CUSE is a very flexible way of defining a char (and thus block)
device, but also lacks documentation</li>
<li>The interface being exposed by CUSE is more complicated than that of
e.g. NBD, but allows for interesting features such as custom
<code>ioctl</code>s (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/device.go#L3-L15)</li>
<li>The only way of implementing it without too much overhead however is
CGo, which comes with its own overhead</li>
<li>It also requires calling Go closures from C code, which is
complicated (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/bindings.go#L79-L132)</li>
<li>Implementing closures is possible by using the <code>userdata</code>
parameter in the CUSE C API (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/cuse.c#L20-L22)</li>
<li>To fully use it, it needs to first resolve a Go callback in C, and
then call it with a pointer to the method’s struct in user data,
effectively allowing for the use of closures (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/bindings.go#L134-L162)</li>
<li>Even with this however, it is hard to implement even a simple
backend, and the CGo overhead is a significant drawback (code snippet
from
https://github.com/pojntfx/webpipe/blob/main/pkg/devices/trace_device.go)</li>
<li>The last alternative to NBD devices would be to extend the kernel
with a new construct that allows for essentially a virtual file to be
<code>mmap</code>ed, not a block device</li>
<li>This could use a custom protocol that is optimized for this use case
instead of a full block device</li>
<li>Because of the extensive setup required to implement such a system
however, and the possibility of <code>ublk</code> providing a performant
alternative in the future, going forward with NBD was chosen for
now</li>
</ul></li>
</ul>
</section>
<section id="mounts-and-live-migration" data-number="0.6.5">
<h3 data-number="1.6.5"><span class="header-section-number">1.6.5</span>
Mounts and Live Migration</h3>
<ul>
<li>Overhead of using managed mounts
<ul>
<li>Another interesting aspect of optimization to look at is the
overhead of managed mounts</li>
<li>It is possible for managed mounts (and migrations) to deliver
signficantly lower performance compared to direct mounts</li>
<li>This is because using managed mounts come with the cost of potential
duplicate I/O operations</li>
<li>For example, if memory is being accessed linearly from the first to
the last offset immediately after it being mounted, then using the
background pulls will have no effect other than causing a write
operation (to the local caching backend) compared to just directly
reading from the remote backend</li>
<li>This however is only the case in scenarios with a very low latency
between the local and remote backends</li>
<li>If latency becomes higher, then the ability to pull the chunks in
the background and in parallel with the puller will offset the cost of
duplicate I/O</li>
<li>The same applies to slow local backends, e.g. if slow disks or
memory are being used, which can mean that offsetting the duplicate I/O
will need a significantly higher latency to be worth it</li>
</ul></li>
<li>Comparing mount vs. migration API performance
<ul>
<li>It is interesting to look at how the migration API performs compared
to the single-phase mount API</li>
<li>The mounts API should have a shorter total latency, but a higher
“downtime” since it needs to initialize the device first</li>
</ul></li>
<li>Comparing it to RegionFS, An existing remote memory system
<ul>
<li>A similar approach was made in RegionFS<span class="citation" data-cites="aguilera2018remoteregions">[33]</span></li>
<li>RegionFS is implemented as a kernel module, but it is functionally
similar to how this API exposes a NBD device for memory interaction</li>
<li>In RegionFS, the regions file system is mounted to a path, which
then exposes regions as virtual files</li>
<li>Instead of using a custom configuration (such as configuring the
amount of pushers to make a mount read-only), such an approach makes it
possible to use <code>chmod</code> on the virtual file for a memory
region to set permissions</li>
<li>By using standard utilities like <code>open</code> and
<code>chmod</code>, this API usable from different programming languages
with ease</li>
<li>Unlike the managed mounts API however, the system proposed in Remote
Regions is mostly intended for private usecases with a limited amount of
hosts and in LAN, with low-RTT connections</li>
<li>It is also not designed to be used for a potential migration
scenarios, which the modular approach of r3map allows for</li>
<li>While Remote Regions’ file system approach does allow for
authorization based on permissions, it doesn’t specify how
authentication could work</li>
<li>In terms of the wire protocol, Remote Regions also seems to target
mostly LAN with protocols like RDMA comm modules, while r3map targets
mostly WAN with a pluggable transport protocol interface</li>
</ul></li>
<li>Issues with the implementation
<ul>
<li>While the managed mounts API mostly works, there are some issues
with it being implemented in Go</li>
<li>This is mostly due to deadlocking issues; if the GC tries to release
memory, it has to stop the world</li>
<li>If the <code>mmap</code> API is used, it is possible that the GC
tries to manage the underlying slice, or tries to release memory as data
is being copied from the mount</li>
<li>Because the NBD server that provides the byte slice is also running
in the same process, this causes a deadlock as the server that provides
the backend for the mount is also frozen
(https://github.com/pojntfx/r3map/blob/main/pkg/mount/slice_managed.go#L70-L93)</li>
<li>A workaround for this is to lock the <code>mmap</code>ed region into
memory, but this will also cause all chunks to be fetched, which leads
to a high <code>Open()</code> latency</li>
<li>This is fixable by simply starting the server in separate thread,
and then <code>mmap</code>ing</li>
<li>Issues like this however are hard to fix, and point to Go
potentially not being the correct language to use for this part of the
system</li>
<li>In the future, using a language without a GC (such as Rust) could
provide a good alternative</li>
<li>While the current API is Go-specific, it could also be exposed
through a different interface to make it usable in Go</li>
</ul></li>
</ul>
</section>
<section id="use-cases" data-number="0.6.6">
<h3 data-number="1.6.6"><span class="header-section-number">1.6.6</span>
Use Cases</h3>
<section id="remote-swap-with-ram-dl" data-number="0.6.6.1">
<h4 data-number="1.6.6.1"><span class="header-section-number">1.6.6.1</span> Remote Swap With
<code>ram-dl</code></h4>
<ul>
<li>ram-dl is a fun experiment</li>
<li>Tech demo for r3map</li>
<li>Uses the fRPC backend to expand local system memory</li>
<li>Can allow mounting a remote system’s RAM locally</li>
<li>Can be used to inspect a remote system’s memory contents</li>
<li>Is based on the direct mount API</li>
<li>Uses mkswap, swapon and swapoff (code snippet from
https://github.com/pojntfx/ram-dl/blob/main/cmd/ram-dl/main.go#L170-L190)</li>
<li>Enables paging out to the block device provided by the direct mount
API</li>
<li><code>ram-ul</code> “uploads” RAM by exposing a memory, file or
directory-backed file over fRPC</li>
<li><code>ram-dl</code> then does all of the above</li>
<li>Not really intended for real-world usecases, but does show that this
can be used for interesting usecases such as real, remote RAM/Swap</li>
<li>Shows how easy it is to use the resulting library, as the entire
project is just ~300 SLOC including backend configuration, flags and
other boilerplate</li>
</ul>
</section>
<section id="mapping-tape-into-memory-with-tapisk" data-number="0.6.6.2">
<h4 data-number="1.6.6.2"><span class="header-section-number">1.6.6.2</span> Mapping Tape Into Memory
With <code>tapisk</code></h4>
<ul>
<li>tapisk is an interesting usecase because of how close it is to STFS,
which provided the inspiration for the FUSE-based approach</li>
<li>Very high read/write backend latency (multiple seconds, up to 90s,
due to seeking)</li>
<li>Linear access, no random reads</li>
<li>Can benefit a lot from asynchronous writes provided by managed
mounts</li>
<li>Fast storage acts as a caching layer</li>
<li>Backend is linear, so only one read/write possible at a time</li>
<li>With local backend, writes are de-duplicated automatically and both
can be asynchronous/concurrent</li>
<li>Writes go to fast (“local”) backend first, syncer then handles in
both directions</li>
<li>Chunking works on tape drive records and blocks</li>
<li>Only one concurrent reader/writer makes sense</li>
<li>Syncing intervals to/from can maybe be minutes or more to make it
more efficient (since long, connected write intervals prevent having to
seek to a different position on disk)</li>
<li>Tape backends can also be used to access large amounts of linear
data (terrabytes) from a tape as though it were in memory</li>
<li>Makes it possible to access even large datasets or backups in a way
that is comfortable</li>
<li>If a file system (e.g. EXT4) is written to the tape, the tape can be
mounted as a random-access device</li>
<li>Tape-specific backend is implemented as the
<code>backend.Backend</code> interface from go-nbd</li>
<li>Backend can be re-used outside of tapisk, too (e.g. as a
<code>ram-dl</code> backend)</li>
<li><code>bbolt</code> DB is used as an index</li>
<li>Index maps simulated offsets to real tape records (code snippet from
https://github.com/pojntfx/tapisk/blob/main/pkg/index/bbolt.go#L43-L68)</li>
<li>When reading, first the tape looks up the real tape record for the
requested offset (code snippet from
https://github.com/pojntfx/tapisk/blob/main/pkg/backend/tape.go#L71-L78)</li>
<li>Seeking can then use the accelerated <code>MTSEEK</code> ioctl to
fast-forward to a record on the tape (code snippet from
https://github.com/pojntfx/tapisk/blob/main/pkg/mtio/tape.go#L25-L40)</li>
<li>After seeking to the block, the chunk is read from the tape into
memory</li>
<li>When writing the drive seeks to the end of the tape (unless the last
operation was a write already, in which case we’re already at the end)
(code snippet from
https://github.com/pojntfx/tapisk/blob/main/pkg/mtio/tape.go#L57-L72)</li>
<li>After that, the current real tape block position is requested,
stored in the index, and the offset is written to the tape (code snippet
from
https://github.com/pojntfx/tapisk/blob/main/pkg/backend/tape.go#L87-L119)</li>
<li>This essentially makes the tape a chunked, reusable
<code>ReadWriterAt</code>, in the same way as the directory backend</li>
<li>By using a RPC backend, a remote tape can be accessed in the same
way, making it possible to map e.g. a remote library’s tape robot to
your local memory</li>
<li><code>tapisk</code> is a unique usecase that shows the versitility
of the approach chosen and how flexible it is</li>
<li>E.g. the chunking system didn’t have to be reimplemented (like with
STFS) - we could just use the managed mount API directly without
changes</li>
<li>Can actually be a real usecase to replace LTFS</li>
<li>LTFS is a kernel module filesystem for tape drives that makes them a
file system the same way as STFS</li>
<li>But is its own filesystem, while tapisk allows using any existing
and tested filesystem on top of the generic block device</li>
<li>Doesn’t support the caching, making it hard to use for memory
mapping, too</li>
<li>Tapisk also shows how minimal it is: While LTFS is 10s of thousands
of SLOC, tapisk achieves the same and more in just under 350 SLOC</li>
</ul>
</section>
<section id="improving-file-storage-solutions" data-number="0.6.6.3">
<h4 data-number="1.6.6.3"><span class="header-section-number">1.6.6.3</span> Improving File Storage
Solutions</h4>
<ul>
<li>Another potential usecase is using r3map to create a mountable
remote filesystem with unique properties</li>
<li>Currently there are two choices on how these can be implemented</li>
<li>Google Drive, Nextcloud etc. listen to file changes on a folder and
synchronize files when they change</li>
<li>The big drawback is that everything needs to be stored locally</li>
<li>If a lot of data is stored (e.g. terabytes), the locally available
files would need to be manually selected</li>
<li>There is no way to dynamically download files this way as they are
required</li>
<li>It is however very efficient, since the filesystem is completely
transparent to the user (writes are being synced back
asynchronously)</li>
<li>It also supports an offline usecase easily</li>
<li>The other option is to use a FUSE, e.g. <code>s3-fuse</code></li>
<li>This means that files can be fetched on demand</li>
<li>But comes with a heavy performance penalty</li>
<li>Writes are usually done (esp. for <code>s3-fuse</code>) when they
are communicated to the FUSE by the kernel, which makes writes very
slow</li>
<li>Offline usage is also hard/usually impossible</li>
<li>Also usually not a fully featured system (e.g. not
<code>inotify</code> support, missing permissions etc.)</li>
<li>This leaves the choice between two imperfect systems, all with their
own downsides</li>
<li>Using r3map can combine both approaches by both not having to
download all files in advance and being able to write back changes
asynchronously</li>
<li>It is also a fully-featured filesystem</li>
<li>Files can also be downloaded preemptively for offline access, just
like with e.g. the Google Drive approach</li>
<li>This can be achieved by using the managed mount API</li>
<li>The block device is formatted using any valid filesystem (e.g. EXT4)
and then mounted</li>
<li>If files should be downloaded in the background, the amount of pull
workers can be set to anything <code>&gt;0</code></li>
<li>Reads to blocks/files that haven’t been read yet can be resolved
from the remote backend, giving a FUSE-like experience</li>
<li>Since read block are then written to the local one, making
subsequent reads almost as fast as native disk reads (unlike in
FUSE/like in the Nextcloud approach)</li>
<li>Writes are done to the local backend and can then be synced up in
periodic intervals like with the Nextcloud approach, making them much
faster than a FUSE, too</li>
<li>Similarly to how the mount API is used, the migration API could then
be used to move the file system between two hosts in a highly efficient
way and without a long downtime</li>
<li>This essentially creates a system that bridges the gap between FUSE
and file synchronization, once again showing how the memory
synchronization tooling can be used to solve essentially the
synchronization of any state, including disks</li>
</ul>
</section>
<section id="universal-database-media-and-asset-streaming" data-number="0.6.6.4">
<h4 data-number="1.6.6.4"><span class="header-section-number">1.6.6.4</span> Universal Database, Media
and Asset Streaming</h4>
<ul>
<li>Another usecase is accessing a remote database locally</li>
<li>While using a database backend is one option of storing chunks, an
actual database can also be stored in a mount as well</li>
<li>Particularly interesting for in-memory or on-disk databases like
SQLite</li>
<li>Instead of having to download the entire SQLite database before
using it, it can simply be mounted, and accessed as it is being
used</li>
<li>This allows very efficient network access, as only very rarely the
entire database is needed</li>
<li>Since reads are cached with the managed mount API, only the first
read should potentially have a performance impact</li>
<li>Similarly so writes to the database will be more or less the same
throughput as to the local disk, since changes are written back
asynchronously</li>
<li>If the full database should eventually be accessible locally, the
background pullers can be used</li>
<li>If the location of e.g. indexes is known, then the pull heuristic
can be specified to fetch these first to speed up initial queries</li>
<li>SQLite itself doesn’t have to support a special interface or changes
for this to work, since it can simply use the mount’s block device as
it’s backend, which shows how universally the concept is usable</li>
<li>This concept doesn’t just apply to SQLite however</li>
<li>Other uses could for example be to make formats that are usually not
streamable due to technical limitations</li>
<li>One such format is famously MP4</li>
<li>Usually, if someone downloads a MP4, they can’t start playing it
before they have finished downloading</li>
<li>This is because MP4s store metadata at the end of the file (graphic
with metadata)</li>
<li>The reason for storing this at the end is that generating the
metadata requires encoding the video first</li>
<li>Usually, if the file is downloaded from start to finish, then this
metadata wouldn’t be accessible, and simply inverting the download order
wouldn’t work since the stream would still be stored in different
offsets of the file</li>
<li>While there are ways of changing this (for example by storing the
metadata in the beginning after the entire MP4 has been transcoded),
this is not possible for already existing files</li>
<li>With r3map however, the pull heuristic function can be used to
immediately pre-fetch the metadata, and the individual chunks of the MP4
file are then being fetched in the background or as they are being
accessed</li>
<li>This allows making a format such as MP4 streamable before it has
been fully downloaded yet, or making almost any format streamable no
matter the codec without any changes required to the video/audio player
or the format</li>
<li>Another use case of this in-place streaming approach could be in
gaming</li>
<li>Usually, all assets of a game are currently being downloaded before
the game is playable</li>
<li>For many new AAA games, this can be hundreds of gigabytes</li>
<li>Many of these assets aren’t required to start the game however</li>
<li>Only some of them (e.g. the launcher, UI libraries, first level
etc.) are</li>
<li>In theory, one could design a game engine in such a way that these
things are only fetched from a remote as they are required</li>
<li>This would however require changing the way new games work, and
would not work for existing games</li>
<li>Playing games of e.g. a network drive or FUSE is also unrealistic
due to performance limitations</li>
<li>Using the managed mount API, the performance limitations of such an
approach can be reduced, without changes to the game</li>
<li>By using the background pull system, reads from chunks that have
already been pulled are almost as fast as native disk reads</li>
<li>By analyzing the data access pattern of an existing game, a pull
heuristic function can be created which will then preemptively pull the
game assets that are required first, keeping latency spikes as low as
possible</li>
<li>Using the chunk availability hooks, the game could also be paused
for a loading screen until a certain local availability percentage is
reached to prevent latency spikes from missing assets, while also still
allowing for faster startup times</li>
<li>The same concept could also be applied to launching any
application</li>
<li>For many languages, completely loading a binary or script into
memory isn’t required for it to start executing</li>
<li>Without significant changes to most interpreters and language VMs
however, loading a file from a file system is the only available
interface for loading programs</li>
<li>With the managed mount API, this interface can be re-used to add
streaming execution support to any intepreter or VM, simply by using the
managed mount either as a filesystem to stream multiple
binaries/scripts/libraries from, or by using the block device as a file
or <code>mmap</code>ing it directly to fetch it</li>
</ul>
</section>
<section id="universal-app-state-synchronization-and-migration" data-number="0.6.6.5">
<h4 data-number="1.6.6.5"><span class="header-section-number">1.6.6.5</span> Universal App State
Synchronization and Migration</h4>
<ul>
<li>Synchronization of app state is hard</li>
<li>Even for hand-off scenarios a custom protocol is built most of the
times</li>
<li>It is possible to use a database sometimes (e.g. Firebase) to
synchronize things</li>
<li>But that can’t sync all data structures and requires using specific
APIs to sync things</li>
<li>What if you could mount and/or migrate any ressource?</li>
<li>Usually these structures are marshalled, sent over a network,
received on a second system, unmarshalled, and then are done being
synced</li>
<li>Requires a complex sync protocol, and when to sync, when to pull
etc. is inefficient and usually happens over a third party (e.g. a
database)</li>
<li>Data structures can almost always be represented by am
<code>[]byte</code></li>
<li>If the data structures are allocated from on a <code>[]byte</code>
from the block device, we can use the managed mount/direct
mount/migration APIs to send/receive them or mount them</li>
<li>This makes a few interesting usecases possible</li>
<li>For example, one could imagine a TODO app</li>
<li>By default, the app mounts the TODO list as a byte slice from a
remote server</li>
<li>Since authentication is pluggable and e.g. a database backend with a
prefix for the user can be used, this could scaly fairly well</li>
<li>Using the pre-emptive background pulls, when the user connects, they
could stream the byte slice in from the remote server as the app is
accessing it, and also pre-emptively pull the rest</li>
<li>If the TODO list is modified by changing it in memory, these changes
are automatically being written to the local backend</li>
<li>The asynchronous push algorithm can then take these changes and sync
them up to the remote backend when required</li>
<li>If the local backend is persistent, such a setup can even survive
network outages</li>
<li>Locking is not handled by this, only one user is supported</li>
<li>The in-memory representation would also need to be universal</li>
<li>This is not the case with Go and for different CPU
architectures</li>
<li>But if e.g. Go is compiled to Wasm, and the Wasm VM’s linear memory
is pointed to the NBD device, it is</li>
<li>This would also allow storing in the entire app state in the Wasm
remote, with no changes to the app itself</li>
<li>More interesting possibly than this smart mount, where the remote
struct is completely transparent to the user, could be the migration
usecase</li>
<li>If a user has the TODO app running on a phone, but wants to continue
writing on their desktop, they can migrate the app’s state with
r3map</li>
<li>In this usecase, the migration API could be used</li>
<li>The pre-copy phase could be automatically detected if the phone
comes physically close to the computer</li>
<li>Since this would be a LAN scenario, the migration would be very fast
and allow for interesting hand-off usecases</li>
<li>The migration API also provides hooks and the protocol that would
ensure that the app would be closed on the phone before it would be
resumed on the desktop, making state corruption during the migration
much harder than with a third-party that changes are synced (where the
remote state may not be up-to-date yet as it is resumed)</li>
<li>Migrating an app’s TODO list or any internal data structure is one
option</li>
<li>Using a toolkit like Apache Arrow can provide a universal format for
this memory, but many other things are possible if a layer of
indirection is added</li>
<li>As mentioned before, a Wasm VM’s memory could also be migrated this
way, allowing for any Wasm app to be transfered between hosts</li>
<li>Similarly so, the Wasm app’s binary, a mounted WASI filesystem etc.
could all be synchronized this way, too</li>
<li>Due to the flexible nature of remap, it is possible to bring the
semantics of VM live migration - moving running VMs between hosts - to
anything that has state</li>
<li>r3map can also be used for actual VM migration, with any hypervisor
that supports mapping a VM’s memory to a block device</li>
<li>This could also be other VMs (like e.g. the JVM), a browser’s JS VM,
or - if the processor architectures etc. match - an entire process’s
memory space</li>
<li>Using the mount API, it would also be possible to bring the concept
of VM snapshots to almost any app</li>
<li>Thanks to the preemptive pull mechanism, restoring e.g. a Wasm VM
outside a migration would also be a very fast operation</li>
<li>Essentially, its VM memory migration and snapshots, but for any kind
of state</li>
<li>Its particularly interesting because it can do so without any
specific requirements for the data structures of the state other than
them being ultimately a <code>[]byte</code>, which by definition every
state is (def. a process level or VM level etc.)</li>
</ul>
</section>
</section>
</section>
<section id="summary" data-number="0.7">
<h2 data-number="1.7"><span class="header-section-number">1.7</span>
Summary</h2>
<ul>
<li>Looking back at all synchronization options and comparing ease of
implementation, CPU load and network traffic between them</li>
<li>Summary of the different approaches, and how the new solutions might
make it possible to use memory as the universal access format</li>
</ul>
</section>
<section id="conclusion" data-number="0.8">
<h2 data-number="1.8"><span class="header-section-number">1.8</span>
Conclusion</h2>
<ul>
<li>Answer to the research question (Could memory be the universal way
to access and migrate state?)</li>
<li>What would be possible if memory became the universal way to access
state?</li>
<li>Further research recommendations (e.g. <code>ublk</code>)</li>
</ul>
</section>
<section id="references" data-number="0.9">
<h2 data-number="1.9"><span class="header-section-number">1.9</span>
References</h2>
<section id="structure" data-number="0.9.1">
<h3 data-number="1.9.1"><span class="header-section-number">1.9.1</span>
Structure</h3>
<ul>
<li>Abstract</li>
<li>Tables
<ul>
<li>Content</li>
<li>Graphics</li>
<li>Abbrevations</li>
</ul></li>
<li>Introduction: Research question/goal</li>
<li>Technology: Current state of research and introductions to related
topics (like a reference book)</li>
<li>Planning: Ideas on how to solve the research question/goal</li>
<li>Implementation: Description of how the ideas were implemented</li>
<li>Results: Which results each of the methods yielded
(i.e. benchmarks)</li>
<li>Discussion: How the individual results can be interpreted and what
use cases there are for the methods</li>
<li>Summary: How the discussion can be interpreted as a whole</li>
<li>Conclusion: Answer to the research question and future outlook,
future research questions</li>
</ul>
</section>
<h3 class="unnumbered" id="citations">Citations</h3>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-linux2023docs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">T.
kernel development community, <span>“Quick start.”</span> <a href="https://www.kernel.org/doc/html/next/rust/quick-start.html" class="uri">https://www.kernel.org/doc/html/next/rust/quick-start.html</a>,
2023.</div>
</div>
<div id="ref-love2010linux" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">R.
Love, <em>Linux kernel development</em>, 3rd ed. Pearson Education,
Inc., 2010.</div>
</div>
<div id="ref-maurer2008professional" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">W.
Mauerer, <em>Professional linux kernel architecture</em>. Indianapolis,
IN: Wiley Publishing, Inc., 2008.</div>
</div>
<div id="ref-stevens2000advanced" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">W.
R. Stevens, <em>Advanced programming in the UNIX environment</em>.
Delhi: Addison Wesley Logman (Singapore) Pte Ltd., Indian Branch,
2000.</div>
</div>
<div id="ref-robbins2003unix" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">K.
A. Robbins and S. Robbins, <em>Unix™ systems programming: Communication,
concurrency, and threads</em>. Prentice Hall PTR, 2003.</div>
</div>
<div id="ref-smith1982cache" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">A.
J. Smith, <span>“Cache memories,”</span> <em>ACM Comput. Surv.</em>,
vol. 14, no. 3, pp. 473–530, Sep. 1982, doi: <a href="https://doi.org/10.1145/356887.356892">10.1145/356887.356892</a>.</div>
</div>
<div id="ref-maruf2023memory" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">H.
A. Maruf and M. Chowdhury, <span>“Memory disaggregation: Advances and
open challenges.”</span> 2023.Available: <a href="https://arxiv.org/abs/2305.03943">https://arxiv.org/abs/2305.03943</a></div>
</div>
<div id="ref-bonwick1994slaballoc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">J.
Bonwick, <span>“The slab allocator: An <span>Object-Caching</span>
kernel,”</span> Jun. 1994.Available: <a href="https://www.usenix.org/conference/usenix-summer-1994-technical-conference/slab-allocator-object-caching-kernel">https://www.usenix.org/conference/usenix-summer-1994-technical-conference/slab-allocator-object-caching-kernel</a></div>
</div>
<div id="ref-gorman2004linuxmem" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">M.
Gorman, <em>Understanding the linux virtual memory manager</em>. Upper
Saddle River, New Jersey 07458: Pearson Education, Inc. Publishing as
Prentice Hall Professional Technical Reference, 2004.</div>
</div>
<div id="ref-silberschatz2018operating" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">A.
Silberschatz, P. B. Galvin, and G. Gagne, <em>Operating system
concepts</em>, 10th ed. Hoboken, NJ: Wiley, 2018.Available: <a href="https://lccn.loc.gov/2017043464">https://lccn.loc.gov/2017043464</a></div>
</div>
<div id="ref-choi2017mmap" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">J.
Choi, J. Kim, and H. Han, <span>“Efficient memory mapped file
<span>I/O</span> for <span>In-Memory</span> file systems,”</span> Jul.
2017.Available: <a href="https://www.usenix.org/conference/hotstorage17/program/presentation/choi">https://www.usenix.org/conference/hotstorage17/program/presentation/choi</a></div>
</div>
<div id="ref-prokop2010inotify" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">M.
Prokop, <span>“Inotify: Efficient, real-time linux file system event
monitoring,”</span> Apr. 2010. <a href="https://www.infoq.com/articles/inotify-linux-file-system-event-monitoring/">https://www.infoq.com/articles/inotify-linux-file-system-event-monitoring/</a></div>
</div>
<div id="ref-postel1981tcp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline"><span>“<span>Transmission Control
Protocol</span>.”</span> RFC 793; J. Postel, Sep. 1981. doi: <a href="https://doi.org/10.17487/RFC0793">10.17487/RFC0793</a>.</div>
</div>
<div id="ref-postel1980udp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline"><span>“<span>User Datagram
Protocol</span>.”</span> RFC 768; J. Postel, Aug. 1980. doi: <a href="https://doi.org/10.17487/RFC0768">10.17487/RFC0768</a>.</div>
</div>
<div id="ref-langley2017quic" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">A.
Langley <em>et al.</em>, <span>“The QUIC transport protocol: Design and
internet-scale deployment,”</span> in <em>Proceedings of the conference
of the ACM special interest group on data communication</em>, 2017, pp.
183–196. doi: <a href="https://doi.org/10.1145/3098822.3098842">10.1145/3098822.3098842</a>.</div>
</div>
<div id="ref-xiao2018rsync" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">H.
Xiao <em>et al.</em>, <span>“Towards web-based delta synchronization for
cloud storage services,”</span> in <em>16th USENIX conference on file
and storage technologies (FAST 18)</em>, Feb. 2018, pp.
155–168.Available: <a href="https://www.usenix.org/conference/fast18/presentation/xiao">https://www.usenix.org/conference/fast18/presentation/xiao</a></div>
</div>
<div id="ref-vangoor2017fuse" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">B.
K. R. Vangoor, V. Tarasov, and E. Zadok, <span>“To <span>FUSE</span> or
not to <span>FUSE</span>: Performance of <span>User-Space</span> file
systems,”</span> in <em>15th USENIX conference on file and storage
technologies (FAST 17)</em>, Feb. 2017, pp. 59–72.Available: <a href="https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor">https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor</a></div>
</div>
<div id="ref-blake2023nbd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">E.
Blake, W. Verhelst, and other NBD maintainers, <span>“The NBD
protocol.”</span> <a href="https://github.com/NetworkBlockDevice/nbd/blob/master/doc/proto.md" class="uri">https://github.com/NetworkBlockDevice/nbd/blob/master/doc/proto.md</a>,
Apr. 2023.</div>
</div>
<div id="ref-he2016migration" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">S.
He, C. Hu, B. Shi, T. Wo, and B. Li, <span>“Optimizing virtual machine
live migration without shared storage in hybrid clouds,”</span> in
<em>2016 IEEE 18th international conference on high performance
computing and communications; IEEE 14th international conference on
smart city; IEEE 2nd international conference on data science and
systems (HPCC/SmartCity/DSS)</em>, 2016, pp. 921–928. doi: <a href="https://doi.org/10.1109/HPCC-SmartCity-DSS.2016.0132">10.1109/HPCC-SmartCity-DSS.2016.0132</a>.</div>
</div>
<div id="ref-baruchi2015workload" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">A.
Baruchi, E. Toshimi Midorikawa, and L. Matsumoto Sato, <span>“Reducing
virtual machine live migration overhead via workload analysis,”</span>
<em>IEEE Latin America Transactions</em>, vol. 13, no. 4, pp. 1178–1186,
2015, doi: <a href="https://doi.org/10.1109/TLA.2015.7106373">10.1109/TLA.2015.7106373</a>.</div>
</div>
<div id="ref-akidau2018streaming" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">T.
Akidau, S. Chernyak, and R. Lax, <em>Streaming systems</em>. Sebastopol,
CA: O’Reilly Media, Inc., 2018.</div>
</div>
<div id="ref-peek1994unix" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">J.
D. Peek, <em>UNIX power tools</em>. Sebastopol, CA; New York: O’Reilly
Associates; Bantam Books, 1994.</div>
</div>
<div id="ref-google2023grpc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">gRPC Authors, <span>“Introduction to
gRPC.”</span> 2023.Available: <a href="https://grpc.io/docs/what-is-grpc/introduction/">https://grpc.io/docs/what-is-grpc/introduction/</a></div>
</div>
<div id="ref-redis2023introduction" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">Redis Ltd, <span>“Introduction to
redis.”</span> <a href="https://redis.io/docs/about/" class="uri">https://redis.io/docs/about/</a>, 2023.</div>
</div>
<div id="ref-redis2023pubsub" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">Redis Ltd, <span>“Redis pub/sub.”</span> <a href="https://redis.io/docs/interact/pubsub/" class="uri">https://redis.io/docs/interact/pubsub/</a>, 2023.</div>
</div>
<div id="ref-aws2023s3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">Amazon Web Services, Inc, <span>“What is amazon
S3?”</span> <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html" class="uri">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html</a>,
2023.</div>
</div>
<div id="ref-minio2023coreadmin" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">MinIO, Inc, <span>“Core administration
concepts.”</span> <a href="https://min.io/docs/minio/kubernetes/upstream/administration/concepts.html" class="uri">https://min.io/docs/minio/kubernetes/upstream/administration/concepts.html</a>,
2023.</div>
</div>
<div id="ref-lakshman2010cassandra" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">A.
Lakshman and P. Malik, <span>“Cassandra: A decentralized structured
storage system,”</span> <em>SIGOPS Oper. Syst. Rev.</em>, vol. 44, no.
2, pp. 35–40, Apr. 2010, doi: <a href="https://doi.org/10.1145/1773912.1773922">10.1145/1773912.1773922</a>.</div>
</div>
<div id="ref-scylladb2023ring" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">Inc. ScyllaDB, <span>“ScyllaDB ring
architecture - overview.”</span> <a href="https://opensource.docs.scylladb.com/stable/architecture/ringarchitecture/index.html" class="uri">https://opensource.docs.scylladb.com/stable/architecture/ringarchitecture/index.html</a>,
2023.</div>
</div>
<div id="ref-grabowski2021scylladb" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">P.
Grabowski, J. Stasiewicz, and K. Baryla, <span>“Apache cassandra 4.0
performance benchmark: Comparing cassandra 4.0, cassandra 3.11 and
scylla open source 4.4,”</span> ScyllaDB Inc, 2021.Available: <a href="https://www.scylladb.com/wp-content/uploads/wp-apache-cassandra-4-performance-benchmark-3.pdf">https://www.scylladb.com/wp-content/uploads/wp-apache-cassandra-4-performance-benchmark-3.pdf</a></div>
</div>
<div id="ref-pojtinger2022stfs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">F.
Pojtinger, <span>“<span class="nocase">STFS: Simple Tape File System, a
file system for tapes and tar files</span>.”</span> <a href="https://github.com/pojntfx/stfs" class="uri">https://github.com/pojntfx/stfs</a>, 2022.</div>
</div>
<div id="ref-waibel2022silefystem" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">J.
Waibel and F. Pojtinger, <span>“<span class="nocase">sile-fystem: A
generic FUSE implementation</span>.”</span> <a href="https://github.com/jakWai01/sile-fystem" class="uri">https://github.com/jakWai01/sile-fystem</a>, 2022.</div>
</div>
<div id="ref-aguilera2018remoteregions" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">M.
K. Aguilera <em>et al.</em>, <span>“Remote regions: A simple abstraction
for remote memory,”</span> in <em>Proceedings of the 2018 USENIX
conference on usenix annual technical conference</em>, 2018, pp.
775–787.</div>
</div>
</div>
</section>
</body>
</html>
