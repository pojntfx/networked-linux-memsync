<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Felicitas Pojtinger (Stuttgart Media University)" />
  <meta name="dcterms.date" content="2023-08-04" />
  <meta name="keywords" content="linux memory
management, userfaultfd, mmap, inotify, hash-based change
detection, delta synchronization, msync, custom filesystem, nbd
protocol, performance evaluation" />
  <title>Efficient Synchronization of Linux Memory Regions over a Network: A Comparative Study and Implementation (Notes)</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Efficient Synchronization of Linux Memory Regions over
a Network: A Comparative Study and Implementation (Notes)</h1>
<p class="subtitle">A user-friendly approach to application-agnostic
state synchronization</p>
<p class="author">Felicitas Pojtinger (Stuttgart Media University)</p>
<p class="date">2023-08-04</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#unsorted-research-questions"><span class="toc-section-number">1.1</span> Unsorted Research
Questions</a></li>
<li><a href="#structure"><span class="toc-section-number">1.2</span>
Structure</a></li>
</ul>
</nav>
<section id="unsorted-research-questions" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Unsorted Research Questions</h2>
</section>
<section id="structure" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Structure</h2>
<ul>
<li><strong>Introduction</strong>
<ul>
<li>How does memory in Linux work? Paging, swap etc.</li>
<li>Examining Linux’s memory management and relevant APIs</li>
<li>An introduction to mmap and how we can use it to map a file into
memory/a byte slice, the role of <code>msync</code></li>
<li>Concept: <code>mmap</code> a memory region with MMAP_SHARED to track
changes in a file</li>
<li>Use cases for memory region synchronization (rough overview,
esp. what is currently popular in the industry)</li>
</ul></li>
<li><strong>Synchronization Strategies</strong>
<ul>
<li><strong>Implementing push-based memory sync by tracking changes to a
<code>mmap</code>ed slice with polled hashing of individual
chunks</strong>, why we can’t use <code>inotify</code>, and the
CPU-bound limitations of this approach
<ul>
<li>Detecting file changes: inotify; but <code>mmap</code> does not
generate WRITE events</li>
<li>Comparing hashes of local and remote <code>mmap</code>ed
regions</li>
<li>Evaluation of hashing algorithms</li>
<li>Introduction to delta synchronization (e.g., rsync)</li>
<li>Custom protocol for delta synchronization</li>
<li>CPU-bound bottlenecks</li>
<li>Multiplexing synchronization streams (central forwarding hub
etc.)</li>
<li>Comparing hashes of local and remote <code>mmap</code>ed
regions</li>
<li>O_DIRECT vs <code>mmap</code>, RAM vs Swap etc.</li>
<li>Function of <code>msync</code>, lack of <code>O_DIRECT</code> (and
why it lacks this/doesn’t support it)</li>
<li>All clients need to be connected from the beginning; strong risk of
inconsistencies</li>
<li>Performance of different hashing algorithms for detecting changes to
a <code>mmap</code>ed region</li>
</ul></li>
<li><strong>Implementing pull-based memory sync with
<code>userfaultfd</code>; and implementation and throughput
limitations</strong>
<ul>
<li>Introduction to userfaultfd</li>
<li>Implementing userfaultfd handlers and registration in Go</li>
<li>The function of <code>msync</code></li>
<li>Transferring sockets between processes</li>
<li>Performance assessment of this approach</li>
<li>Examples of handler and registration interfaces (byte slice, file,
S3 object)</li>
<li>Performance assessment of this approach</li>
<li>Explanation of userfaultfd and its implementation</li>
<li>Description of userfaultfd handlers and registration in Go</li>
<li>I/O-bound bottlenecks</li>
<li>No option to pre-emptively pull data</li>
<li>userfaultfd is read-only</li>
<li>userfaultfd can only be used to fetch the first (missing) chunk, not
subsequent ones</li>
<li>userfaultfd is limited to ~50MB/s of throughput</li>
<li>Biggest benefit of userfaultfd: It has minimal registration overhead
&amp; latency</li>
<li>userfaultfd’s interface is just an io.ReaderAt, making it extremely
simple to use</li>
</ul></li>
<li><strong>Implementing push-pull based memory sync with a
FUSE</strong>; limitations and complexity (citing STFS)
<ul>
<li>Intercepting writes to the <code>mmap</code>ed region using a custom
filesystem</li>
<li>Mounting the filesystem</li>
<li>Exploring methods for creating a new, custom Linux filesystem: FUSE,
in the Kernel</li>
<li>Taking a look at STFS for how to implement this; tape-specific
optimizations were possible, so we could also do this for files I
presume</li>
<li>Why this approach is suboptimal (we need just one file, not a full
filesystem)</li>
</ul></li>
<li><strong>Implementing push-pull based memory sync with NBD;
implementation of go-nbd</strong>
<ul>
<li>Detailed analysis of the NBD protocol (client &amp; server)</li>
<li>Implementing the client and server in Go based on the protocol</li>
<li>Server backend interface and example implementations</li>
<li>Backends can use custom indexes to map linear media (e.g. tape
drives) into memory by mapping the block device offset to a real,
append-only record number and swapping it out for a new one when things
get overwritten in the block device vs. FUSE, where its much more
complicated/needs to be a full filesystem</li>
<li>Caching mechanism (mounting the fd with O_DIRECT vs. not)</li>
<li>Performance assessment of this approach</li>
<li>Project scope (why only the minimal protocol was implemented)</li>
<li>NBD protocol overview and limitations</li>
<li>NBD protocol phases</li>
<li>Minimal viable NBD protocol needs</li>
<li>Go NBD server implementation: Multiple clients, error handling</li>
<li>Go NBD client and server implementation: The kernel’s NBD client,
CGo for <code>ioctl</code> numbers</li>
<li>Finding unused NBD devices, detecting client availability (polling
sysfs vs. udev; add benchmarks)</li>
<li>go-nbd pluggable backend API design/interface</li>
<li>go-nbd project scope &amp; keeping it maintainable, esp. vs other
NBD implementations</li>
<li>Using <code>ublk</code> instead of NBD in the future; should be much
faster than NBD for concurrency/random-access as it supports</li>
<li>Using a BUSE as a NBD server (library/CGo limitations)</li>
<li>Extending the kernel with a new resource or a filesystem like FUSE,
but actually in the kernel with a more efficient user-space protocol
optimized for random access</li>
</ul></li>
<li><strong>Using NBD directly as a mount-based sync system with the
direct mount API</strong>; limitations with latency etc., and
improvements with background pulls and pushes, different backends etc.,
the mount wire protocol, pull heuristics
<ul>
<li>Use cases</li>
<li>Path vs. file vs. slice mounts/migrations</li>
<li>Limitations and benefits of <code>mmap</code> for accessing a mount
vs. a file (concurrent reads/writes etc.)</li>
<li>Instead of <code>mmap</code>ing the block device, formatting the
block device with e.g. EXT4 and then <code>mmap</code>ing a file on
through that (would allow synchronization of multiple separate memory
regions, e.g. multiple app states, over the network)</li>
<li>Discussion on mount protocol actors, phases and state machine</li>
<li>Managed mount protocol actors, phases, sequence and state
machine</li>
<li>The asynchronous background push method (for mounts); how chunks are
marked as dirty when they are being written to before the download has
finished completely</li>
<li>Chunking system/non-aligned reads and writes, checking for correct
chunking behavior</li>
<li>Making it unit testable; rwat pipeline</li>
<li>Local vs. remote chunking</li>
<li>Pull priority function/heuristic: Benchmarks when accessing from end
of file to start vs. other way around, latency vs. throughput changes
with/without heuristic, using LLMs etc. to analyze access patterns with
<code>pullWorkers: 0</code> and then generating an automatic pull
heuristic</li>
<li>Usage of the r3map’s API vs. e.g. “Remote regions” paper</li>
<li>Using Rust for a future implementation; esp. to prevent
<code>mmap</code> blocks from being scanned by the garbage
collector</li>
</ul></li>
<li><strong>Taking inspiration from VM live migration and adding a
migration system</strong> for memory sync, two-phase commit, the sync
wire protocol, minimum acceptable downtime as the metric to optimize for
<ul>
<li>Use cases</li>
<li>Discussion on migration protocol actors, phases and state
machine</li>
<li>Migration protocol actors, phases and state machine</li>
<li>Examination of P2P vs. relay systems/hub and spoke systems</li>
<li>Preemptive pulls and parallelized startups (n MB saved)</li>
<li>Background pulling system and interface (rwat), % of
availability</li>
<li>Migration API lifecycle &amp; the role of lockable rwats</li>
<li>Minimum acceptable downtime</li>
<li>Concurrent access/consistency guarantees for mounts vs. migrations
etc. - <code>Track()</code>, why we can’t modify a mount’s source</li>
<li>When to best <code>Finalize()</code> a migration; analyzing app
usage patterns?</li>
</ul></li>
<li><strong>Optimizing the Mount and Migration Implementations</strong>
<ul>
<li>Encryption of regions and the wire protocol, authn, DoS
vulnerabilities without max size</li>
<li>Criticality/critical phases in protocols (e.g. recovering from a
network outage in mount vs. migration, finalization step can’t be
aborted etc.)</li>
<li>Performance tuning parameters (chunk size, push/pull workers)</li>
<li>Backend implementations: File, memory, directory, dudirekta, gRPC,
fRPC, Redis, S3, Cassandra (a section for each)</li>
<li>Transport layers: Dudirekta, gRPC, fRPC (esp. benefits and problems
with concurrent RPCs, connection pooling like with dRPC, benchmarks with
latency and throughput etc.)</li>
<li>Usage of QUIC, UDP and other protocols for skipping on RTT to
improve minimal latency</li>
<li>Examination of different transport layers and their implications on
performance and concurrency</li>
<li>Discuss potential effects of high latency, slow local disks or RAM
on different pull methods</li>
<li>Effects of slow local disks or RAM on pull methods</li>
<li>Comparing options in terms of ease of implementation, CPU load, and
network traffic</li>
<li>Effects of high latency on different pull methods (esp. direct
vs. managed)</li>
<li>Performance tuning parameters (chunk size, push/pull workers)</li>
<li>P2P vs. relay systems/hub and spoke systems</li>
</ul></li>
</ul></li>
<li><strong>Case Studies</strong>
<ul>
<li>Mount backend API vs. seeder API</li>
<li>Use cases: Direct Mount vs. Managed Mount vs. Migration</li>
<li>Use cases, case studies and comparison of approaches, finding the
one that is right for each one, and showing an implementation for each,
benchmarks, performance tuning</li>
<li>Identifying the optimal solution for specific use cases: data change
frequency, kernel/OS compatibility, etc.</li>
<li>Tapisk as an example of using the mount APIs for a filesystem,
esp. one with very low read/write speeds and high latency; esp. with
support for writebacks in the future, and comparing this to STFS which
was a FUSE instead of a block device</li>
<li><code>ram-dl</code> as an example of using a remote backend to
provide more RAM/Swap</li>
<li>Migrating app state (e.g. a TODO list) between two hosts in a
universal (byte-slice/by using the underlying memory) manner,
integration with existing app migration systems</li>
<li>Mounting remote filesystems and combining the benefits of
traditional FUSE-based mounts (e.g. s3-fuse) with Google
Drive/Nextcloud-style synchronization</li>
<li>Using mounts for SQLite etc. database access without having to use
range requests</li>
<li>Streaming video using formats that usually don’t support streaming,
e.g. MP4, where an index is required</li>
<li>Improving game download speeds by mounting the remote assets with
managed mounts, using a pull heuristic that defines typical access
patterns, e.g. by levels (making the game immediately playable)</li>
<li>Executing remote binaries or scripts that don’t need to be scanned
first without fully downloading them</li>
</ul></li>
<li><strong>Conclusion</strong>
<ul>
<li>Conclusion with a summary of the different approaches, further
research recommendations</li>
</ul></li>
</ul>
</section>
</body>
</html>
