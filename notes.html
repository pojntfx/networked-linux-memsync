<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Felicitas Pojtinger (Stuttgart Media University)" />
  <meta name="dcterms.date" content="2023-08-04" />
  <meta name="keywords" content="linux memory
management, userfaultfd, mmap, inotify, hash-based change
detection, delta synchronization, msync, custom filesystem, nbd
protocol, performance evaluation" />
  <title>Efficient Synchronization of Linux Memory Regions over a Network: A Comparative Study and Implementation (Notes)</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Efficient Synchronization of Linux Memory Regions over
a Network: A Comparative Study and Implementation (Notes)</h1>
<p class="subtitle">A user-friendly approach to application-agnostic
state synchronization</p>
<p class="author">Felicitas Pojtinger (Stuttgart Media University)</p>
<p class="date">2023-08-04</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#unsorted-research-questions"><span class="toc-section-number">1.1</span> Unsorted Research
Questions</a></li>
<li><a href="#structure"><span class="toc-section-number">1.2</span>
Structure</a></li>
<li><a href="#content"><span class="toc-section-number">1.3</span>
Content</a></li>
</ul>
</nav>
<section id="unsorted-research-questions" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Unsorted Research Questions</h2>
</section>
<section id="structure" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Structure</h2>
<ul>
<li>Introduction
<ul>
<li>Memory management in Linux</li>
<li>Memory as the universal storage API</li>
<li>What would be possible if memory would be the universal way to
access resources?</li>
<li>Why efficient memory synchronization is the missing key
component</li>
<li>High-level use cases for memory synchronization in the industry
today</li>
</ul></li>
<li>Pull-Based Memory Synchronization with <code>userfaultfd</code>
<ul>
<li>Plain language description of <code>userfaultfd</code> (what are
page faults)</li>
<li>Exploring an alternative method by handling page faults using
signals</li>
<li>Handlers and registration</li>
<li>History of <code>userfaultfd</code></li>
<li>Allocating the shared region</li>
<li>Maximum shared region size is limited by available physical
memory</li>
<li>Transferring handler sockets between processes</li>
<li>Implementing <code>userfaultfd</code> bindings in Go</li>
<li>Example usages of <code>userfaultfd</code> in Go (byte slice, file,
S3 object)</li>
<li>Implications of not being able to catch writes to the region (its
read-only)</li>
<li>Design of a <code>userfaultfd</code> backend (io.ReaderAt)</li>
<li>Limiations: ~50MB/s of throughput</li>
<li>Limitations of only being able to catch the first page fault (no way
of updating the region)</li>
<li>Implications of not being able to pull chunks before they are being
accessed</li>
<li>Limitations of only being able to pull chunks synchronously</li>
<li>Benefits of minimal registration and latency overhead</li>
<li>Benchmark: Sensitivity of <code>userfaultfd</code> to network
latency and throughput</li>
</ul></li>
<li>Push-Based Memory Synchronization with <code>mmap</code> and Hashing
<ul>
<li>Plain language description of this approach (mapping a file into
memory, then syncing the file)</li>
<li>Paging and swap in Linux</li>
<li>Introduction to <code>mmap</code> to map a file into a memory
region</li>
<li><code>MAP_SHARED</code> for writing changes back from the memory
region to a file</li>
<li>Caching with <code>mmap</code>, why <code>O_DIRECT</code> doesn’t
work and what the role of <code>msync</code> is</li>
<li>Detecting writes to a file with <code>inotify</code> and why this
does not work for <code>mmap</code></li>
<li>Hashing (chunks of) the backing file in order to detect writes on a
periodic basis</li>
<li>Benchmark: Performance of different Go hashing algorithms for
detecting writes</li>
<li>Effects on maximum acceptable downtime due to CPU-bound limitations
in having to calculate hashes and polling</li>
<li>Introduction to delta synchronization protocols like rsync</li>
<li>Implementation of a custom delta synchronization protocol with chunk
support</li>
<li>Multiplexing different synchronization streams in the protocol</li>
<li>Benchmark: Throughput of this custom synchronization protocol
vs. rsync (which hashes entire files)</li>
<li>Using a central forwarding hub for star-based architectures with
multiple destinations</li>
<li>Limitations of only being able to catch writes, not reads to the
region (its write-only, can’t add hosts later on)</li>
</ul></li>
<li>Push-Pull Memory Synchronization with a FUSE
<ul>
<li>Plain language description of this approach (mapping a file into
memory, catching reads/writes to/from the file with a custom
filesystem)</li>
<li>Methods for creating a new, custom file system (FUSE vs. in the
kernel)</li>
<li>STFS shows that it is possible to create file systems backed by
complex data stores, e.g. a tape/non-random access stores</li>
<li>Is not the best option for implementing this due to significant
overhead and us only needing a single file to map</li>
</ul></li>
<li>Pull-Based Memory Synchronization with NBD
<ul>
<li>Plain language description of this approach (mapping a block device
into memory, catching reads/writes with a NBD server)</li>
<li>Why NBD is the better choice compared to FUSE (much less complex
interface)</li>
<li>Overview of the NBD protocol</li>
<li>Phases, actors and messages in the NBD protocol (negotiation,
transmission)</li>
<li>Minimal viable NBD protocol needs, and why only this minimal set is
implemented</li>
<li>Listing exports</li>
<li>Limitations of NBD, esp. the kernel client and message sizes</li>
<li>Reduced flexibility of NBD compared to FUSE (can still be used for
e.g. linear media, but will offer fewer interfaces for
optimization)</li>
<li>Server backend interface design</li>
<li>File backend example</li>
<li>How the servers handling multiple users/connections</li>
<li>Server handling in the NBD protocol implementation</li>
<li>Using the kernel NBD client without CGO/with ioctls</li>
<li>Finding an unused NBD device using <code>sysfs</code></li>
<li>Benchmark: <code>nbd</code> kernel module quirks and how to detect
whether a NBD device is open (polling <code>sysfs</code>
vs. <code>udev</code>)</li>
<li>Caching mechanisms and limitations (aligned reads) when opening the
block device (<code>O_DIRECT</code>)</li>
<li>Future outlook: Using <code>ublk</code> instead of NBD, allowing for
potentially much faster concurrent access thanks to
<code>io_uring</code></li>
<li>Why using BUSE to implement a NBD server would be possible but
unrealistic (library &amp; docs situation, CGo)</li>
<li><code>go-buse</code> as a preview of how such a CGo implementation
could still work</li>
<li>Alternatively implementing a new file system entirely in the kernel,
only exposing a single file/block device to <code>mmap</code> and
optimizing the user space protocol for this</li>
</ul></li>
<li>Push-Pull Memory Synchronization with Mounts
<ul>
<li>Plain language description of this approach (like NBD, but starting
the client and server locally, then connecting the <em>server</em>’s
backend to a backend)</li>
<li>Benefits: Can use a secure wire protocol and more complex/abstract
backends</li>
<li>Mounting the block device as a path vs. file vs. slice: Benefits of
<code>mmap</code> (concurrent reads/writes)</li>
<li>Alternative approach: Formatting the block device as e.g. EXT4, then
mounting the filesystem, and <code>mmap</code>ing a file on the file
system (allows syncing multiple regions with a single file system, but
has FS overhead)</li>
<li>Mount protocol actors, phases and state machine</li>
<li>Chunking system for non-aligned reads/writes (arbitrary rwat and
chunked rwat)</li>
<li>Benchmark: Local vs. remote chunking</li>
<li>Optimizing the mount process with the Managed Mount interface</li>
<li>Pre- and post-copy systems and why we should combine them (see
Optimizing Virtual Machine Live Migration without Shared Storage in
Hybrid Clouds)</li>
<li>Asynchronous background push system interface and how edge cases
(like accessing dirty chunks as they are being pulled or being written
to) are handled</li>
<li>Preemptive background pulls interface</li>
<li>Syncer interface</li>
<li>Benchmark: Parallelizing startups and pulling n MBs as the device
starts</li>
<li>Using a pull heuristic function to optimize which chunks should be
scheduled to be pulled first</li>
<li>Internal rwat pipeline (create a graphic) for direct mounts
vs. managed mounts</li>
<li>Unit testing the rwats</li>
<li>Comparing this mount API to other existing remote memory access
APIs, e.g. “Remote Regions” (“Remote regions: a simple abstraction for
remote memory”)</li>
<li>Complexities when <code>mmap</code>ing a region in Go as the GC
halts the entire world to collect garbage, but that also stops the NBD
server in the same process that tries to satisfy the region being
scanned</li>
<li>Potentially using Rust for this component to cut down on memory
complexities and GC latencies</li>
</ul></li>
<li>Pull-Based Memory Synchronization with Migrations
<ul>
<li>Plain language description of this approach (like NBD, but two
phases to start the device and pull, then only flush the latest changes
to minimize downtime)</li>
<li>Inspired by live VM migration, where changes are continuously being
pulled to the destination node until a % has been reached, after which
the VM is migrated</li>
<li>Migration protocol actors (seeders, leechers etc.), phases and state
machine</li>
<li>How the migration API is completely independent of a transport
layer</li>
<li>Switching from the seeder to the leecher state</li>
<li>Using preemptive pulls and pull heuristics to optimize just like for
the mounts</li>
<li>Lifecycle of the migration API and why lockable rwats are
required</li>
<li>How a successful migration causes the <code>Seeder</code> to
exit</li>
<li>The role of maximum acceptable downtime</li>
<li>The role of <code>Track()</code>, concurrent access and consistency
guarantees vs. mounts (where the source must not change)</li>
<li>When to best <code>Finalize()</code> a migration and how analyzing
app usage patterns could help (A Framework for Task-Guided Virtual
Machine Live Migration, Reducing Virtual Machine Live Migration Overhead
via Workload Analysis)</li>
<li>Benchmark: Maximum acceptable downtime for a migration scenario with
the Managed Mount API vs the Migration API</li>
</ul></li>
<li>Optimizing Mounts and Migrations
<ul>
<li>Encryption of memory regions and the wire protocol</li>
<li>Authentication of the protocol</li>
<li>DoS vulnerabilities in the NBD protocol (large message sizes; not
meant for the public internet) and why the indirection of client &amp;
server on each node is needed</li>
<li>Mitigating DoS vulnerabilities in the
<code>ReadAt</code>/<code>WriteAt</code> RPCs with
<code>maxChunkSize</code> and/or client-side chunking</li>
<li>Critical <code>Finalizing</code> state in the migration API and how
it could be remedied</li>
<li>How network outages are handled in the mount and migration API</li>
<li>Analyzing the file and memory backend implementations</li>
<li>Analyzing the directory backend</li>
<li>Analyzing the dudirekta, gRPC and fRPC backends</li>
<li>Benchmark: Latency till first n chunks <em>and</em> throughput for
dudirekta, gRPC and fRPC backends (how they are affected by having/not
having connection polling and/or concurrent RPCs)</li>
<li>Benchmark: Effect of tuning the amount of push/pull workers in
high-latency scenarios</li>
<li>Analyzing the Redis backend</li>
<li>Analyzing the S3 backend</li>
<li>Analyzing the Cassandra backend</li>
<li>Benchmark: Latency and throughput of all benchmarks on localhost and
in a realistic latency and throughput scenario</li>
<li>Effects of slow disks/memory on local backends, and why direct
mounts can outperform managed mounts in tests on localhosts</li>
<li>Using P2P vs. star architectures for mounts and migrations</li>
<li>Looking back at all options and comparing ease of implementation,
CPU load and network traffic between them</li>
</ul></li>
<li>Case Studies
<ul>
<li><code>ram-dl</code> as an example of using the direct mount API for
extending system memory</li>
<li><code>tapisk</code> as an example of using the managed mount API for
a file system with very high latencies, linear access and asynchronous
to/from fast local memory vs. STFS</li>
<li>Migration app state (e.g. TODO list) between two hosts in a
universal (underlying memory) manner</li>
<li>Mounting remote file systems as managed mounts and combining the
benefits of traditional FUSE mounts (e.g. s3-fuse) with Google
Drive-style synchronization</li>
<li>Using manged mounts for remote SQLite databases without having to
download it first</li>
<li>Streaming video formats like e.g. MP4 that don’t support streaming
due to indexes/compression</li>
<li>Improving game download speeds by mounting the remote assets with
managed mounts, using a pull heuristic that defines typical access
patterns (like which levels are accessed first), making any game
immediately playable without changes</li>
<li>Executing remote binaries or scrips that don’t need to be scanned
first without having to fully download them</li>
</ul></li>
<li>Conclusion
<ul>
<li>Summary of the different approaches, and how the new solutions might
make it possible to use memory as the universal access format</li>
<li>Further research recommendations (e.g. <code>ublk</code>)</li>
</ul></li>
</ul>
</section>
<section id="content" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span>
Content</h2>
<ul>
<li><p>Pull-Based Memory Synchronization with
<code>userfaultfd</code></p>
<ul>
<li>Page faults occur when a process tries to access a memory region
that has not yet been mapped into a process’ address space</li>
<li>By listening to these page faults, we know when a process wants to
access a specific piece of memory</li>
<li>We can use this to then pull the chunk of memory from a remote, map
it to the address on which the page fault occured, thus only fetching
data when it is required</li>
<li>Usually, handling page faults is something that the kernel does</li>
<li>In our case, we want to handle page faults in userspace</li>
<li>In the past, this used to be possible by handling the
<code>SIGSEGV</code> signal in the process</li>
<li>In our case however, we can use a recent system called
<code>userfaultfd</code> to do this in a more elegant way (available
since kernel 4.11)</li>
<li><code>userfaultfd</code> allows handling these page faults in
userspace</li>
<li>Implementing this in Go was quite tricky, and it involves using
<code>unsafe</code></li>
<li>We can use the <code>syscall</code> and <code>unix</code> packages
to interact with <code>ioctl</code> etc.</li>
<li>We can use the <code>ioctl</code> syscall to get a file descriptor
to the <code>userfaultfd</code> API, and then register the API to handle
any faults on the region (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/pkg/mapper/register.go#L15)</li>
<li>The region that should be handled can be allocated with
e.g. <code>mmap</code></li>
<li>Once we have the file descriptor for the <code>userfaultfd</code>
API, we need to transfer this file descriptor to a process that should
respond with the chunks of memory to be put into the faulting
address</li>
<li>Passing file descriptors between processes is possible by using a
UNIX socket (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/pkg/transfer/unix.go)</li>
<li>Once we have received the socket we need to register the handler for
the API to use</li>
<li>If the handler receives an address that has faulted, it responds
with the <code>UFFDIO_COPY</code> <code>ioctl</code> and a pointer to
the chunk of memory that should be used on the file descriptor (code
snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/pkg/mapper/handler.go)</li>
<li>A big benefit of using <code>userfaultfd</code> and the pull method
is that we are able to simplify the backend of the entire system down to
a <code>io.ReaderAt</code> (code snippet from
https://pkg.go.dev/io#ReaderAt)</li>
<li>That means we can use almost any <code>io.ReaderAt</code> as a
backend for a <code>userfaultfd-go</code> registered object</li>
<li>We know that access will always be aligned to 4 KB chunks/the system
page size, so we can assume a chunk size on the server based on
that</li>
<li>For the first example, we can return a random pattern in the backend
(code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/cmd/userfaultfd-go-example-abc/main.go)
- this shows a great way of exposing truly arbitrary information into a
byte slice without having to pre-compute everything or changing the
application</li>
<li>Since a file is a valid <code>io.ReaderAt</code>, we can also use a
file as the backend directly, creating a system that essentially allows
for mounting a (remote) file into memory (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/cmd/userfaultfd-go-example-file/main.go)</li>
<li>Similarly so, we can use it map a remote object from S3 into memory,
and access only the chunks of it that we actually require (which in the
case of S3 is achieved with HTTP range requests) (code snippet from
https://github.com/loopholelabs/userfaultfd-go/blob/master/cmd/userfaultfd-go-example-s3/main.go)</li>
<li>As we can see, using <code>userfaultfd</code> we are able to map
almost any object into memory</li>
<li>This approach is very clean and has comparatively little overhead,
but also has significant architecture-related problems that limit its
uses</li>
<li>The first big problem is only being able to catch page faults - that
means we can only ever respond the first time a chunk of memory gets
accessed, all future requests will return the memory directly from RAM
on the destination host</li>
<li>This prevents us from using this approach for remote resources that
update over</li>
<li>Also prevents us from using it for things that might have concurrent
writers/shared resources, since there would be no way of updating the
conflicting section</li>
<li>Essentially makes this system only usable for a read-only “mount” of
a remote resource, not really synchronization</li>
<li>Also prevents pulling chunks before they are being accessed without
layers of indirection</li>
<li>The <code>userfaultfd</code> API socket is also synchronous, so each
chunk needs to be sent one after the other, meaning that it is very
vulnerable to long RTT values</li>
<li>Also means that the initial latency will be at minimum the RTT to
the remote source, and (without caching) so will be each future
request</li>
<li>The biggest problem however: All of these drawbacks mean that in
real-life usecases, the maximum throughput, even if a local process
handles page faults on a modern computer, is ~50MB/s</li>
<li>Benchmark: Sensitivity of <code>userfaultfd</code> to network
latency and throughput</li>
<li>In summary, while this approach is interesting and very idiomatic to
Go, for most data, esp. larger datasets and in high-latency scenarios/in
WAN, we need a better solution</li>
</ul></li>
<li><p>Push-Based Memory Synchronization with <code>mmap</code> and
Hashing</p>
<ul>
<li>This approach tries to improve on <code>userfaultfd</code> by
switching to push-based synchronization method</li>
<li>Instead of reacting to page faults, this one a file to track changes
to a memory region</li>
<li>By synchronizing the file representing the memory region between two
systems, we can effectively synchronize the memory region itself</li>
<li>In Linux, swap space allows Linux to move pages of memory to disk or
other swap partition if the fast speed of RAM is not needed (“paging
out”)</li>
<li>Similarly to this, Linux can also load missing pages from a
disk</li>
<li>This works similarly to how <code>userfaultfd</code> handled page
faults, except this time it doesn’t need to go through user space, which
can make it much faster</li>
<li>We can do this by using <code>mmap</code>, which allows us to map a
file into memory</li>
<li>By default, <code>mmap</code> doesn’t write changes from a file back
into memory, no matter if the file descriptor passed to it would allow
it to or not</li>
<li>We can however add the <code>MAP_SHARED</code> flag; this tells the
kernel to write back changes to the memory region to the corresponding
regions of the backing file</li>
<li>Linux caches reads to such a backing file, so only the first page
fault would be answered by fetching from disk, just like with
<code>userfaultfd</code></li>
<li>The same applies to writes; similar to how files need to be
<code>sync</code>ed in order for them to be written to disks,
<code>mmap</code>ed regions need to be <code>msync</code>ed in order to
flush changes to the backing file</li>
<li>In order to synchronize changes to the region between hosts by
syncing the underlying file, we need to have the changes actually be
represented in the file, which is why <code>msync</code> is
critical</li>
<li>For files, you can use <code>O_DIRECT</code> to skip this kernel
caching if your process already does caching on its own, but this flag
is ignored by the <code>mmap</code></li>
<li>Usually, one would use <code>inotify</code> to watch changes to a
file</li>
<li><code>inotify</code> allows applications to register handlers on a
file’s events, e.g. <code>WRITE</code> or <code>SYNC</code>. This allows
for efficient file synchronization, and is used by many file
synchronization tools</li>
<li>It is also possible to filter only the events that we need to sync
the writes, making it the perfect choice for this use case</li>
<li>For technical reasons however (mostly because the file is
represented by a memory region), Linux doesn’t fire these events for
<code>mmap</code>ed files though, so we can’t use it</li>
<li>The next best option are two: Either polling for file attribute
changes (e.g. last write), or by continously hashing the file to check
if it has changed</li>
<li>Polling on its own has a lot of downsides, like it adding a
guaranteed minimum latency by virtue of having to wait for the next
polling cycle</li>
<li>This negatively impacts a maximum allowed downtime scenario, where
the overhead of polling can make or break a system</li>
<li>Hashing the entire file is also a naturally IO- and CPU-intensive
process because the entire file needs to be read at some point</li>
<li>Still, polling &amp; hashing is probably the only reliable way of
detecting changes to a file</li>
<li>When picking algorithms for this hashing process, the most important
metric to consider is the throughput with which it can compute hashes,
as well as the change of collisions</li>
<li>Benchmark: Performance of different Go hashing algorithms for
detecting writes</li>
<li>Instead of hashing the entire file, then syncing the entire file, we
can want to really sync only the parts of the file that have changed
between two polling iterations</li>
<li>We can do this by opening up the file multiple times, then hashing
individual offsets, and aggregating the chunks that have changed</li>
<li>If the underlying hashing algorithm is CPU-bound, this also allows
for better concurrent processing</li>
<li>Increases the initial latency/overhead by having to open up multiple
file descriptors</li>
<li>Benchmark: Hashing the chunks individually vs. hashing the entire
file</li>
<li>But this can not only increase the speed of each individual polling
tick, it can also drastically decrease the amount of data that needs to
be transferred since only the delta needs to be synchronized</li>
<li>Hashing and/or syncing individual chunks that have changed is a
common practice</li>
<li>The probably most popular tool for file synchronization like this is
rsync</li>
<li>When the delta-transfer algorithm for rsync is active, it computes
the difference between the local and the remote file, and then
synchronizes the changes</li>
<li>The delta sync algorithm first does file block division</li>
<li>The file on the destination is divided into fixed-size blocks</li>
<li>For each block in the destination, a weak and fast checksum is
calculated</li>
<li>The checksums are sent over to the source</li>
<li>On the source, the same checksum calculation process is run, and
compared against the checksums that were sent over (matching block
identification)</li>
<li>Once the changed blocks are known, the source sends over the offset
of each block and the changed block’s data to the destination</li>
<li>When a block is received, the destination writes the chunk to the
specified offset, reconstructing the file</li>
<li>Once one polling interval is done, the process begins again</li>
<li>We have implemented a simple TCP-based protocol for this delta
synchronization, just like rsync’s delta synchronization algorithm (code
snippet from
https://github.com/loopholelabs/darkmagyk/blob/master/cmd/darkmagyk-orchestrator/main.go#L1337-L1411
etc.)</li>
<li>For this protocol specifically, we send the changed file’s name as
the first message when starting the synchronization, but a simple
multiplexing system could easily be implemented by sending a file ID
with each message</li>
<li>Similarly to <code>userfaultfd</code>, this system also has
limitations</li>
<li>While <code>userfaultfd</code> was only able to catch reads, this
system is only able to catch writes to the file</li>
<li>Essentially this system is write-only, and it is very inefficient to
add hosts to the network later on</li>
<li>As a result, if there are many possible destinations to migrate
state too, a star-based architecture with a central forwarding hub can
be used</li>
<li>The static topology of this approach can be used to only ever
require hashing on one of the destinations and the source instead of all
of them</li>
<li>This way, we only need to push the changes to one component (the
hub), instead of having to push them to each destination on their
own</li>
<li>The hub simply forwards the messages to all the other
destinations</li>
<li>Benchmark: Throughput of this custom synchronization protocol
vs. rsync (which hashes entire files)</li>
</ul></li>
<li><p>Push-Pull Memory Synchronization with a FUSE</p>
<ul>
<li>Since the push method requires polling and is very CPU and I/O
intensive, and <code>userfaultfd-go</code> has too low of a throughput,
a better solution is needed</li>
<li>What if we could still get the events for the writes and reads
without having to use <code>userfaultfd-go</code> or hashing?</li>
<li>We can create a custom file system in Linux and load it as a kernel
module</li>
<li>This file system could then intercept reads/writes to/from the
<code>mmap</code>ed region, making it possible to respond to them with a
custom backend</li>
<li>But such a system would need to run in the kernel directly, which
leads to a lot of potential drawbacks</li>
<li>While it is possible to write kernel modules with Rust instead of C
these days, a lot of problems remain</li>
<li>Kernel modules aren’t portable; they are built for a specific
kernel, which makes them hard to distribute to users</li>
<li>A kernel file system is able to skip having to go through user space
and thus save on context switching, but that will also mean that it will
run in the kernel address space, making for a poor level of
isolation</li>
<li>Iterating on a kernel module is much harder than iterating on a
program running in user space</li>
<li>If we want the user to be able to provide their own backends from/to
which to pull/push, that will still require communication between user
space and the kernel</li>
<li>So while adding this implementation in the kernel would be possible,
it would also be very complex</li>
<li>In order to implement file systems in user space, we can use the
FUSE API</li>
<li>Here, a user space program registers itself with the FUSE kernel
module</li>
<li>This program provides callbacks for the file system operations,
e.g. for <code>open</code>, <code>read</code>, <code>write</code>
etc.</li>
<li>When the user performs a file system operation on a mounted FUSE
file system, the kernel module will send a request for the operation to
the user space program, which can then reply with a response, which the
FUSE kernel module will then return to the user</li>
<li>This makes it much easier to create a file system compared to
writing it in the kernel, as it can run in user space</li>
<li>It is also much safer as no custom kernel module is required and an
error in the FUSE or the backend can’t crash the entire kernel</li>
<li>Unlike a file system implemented as a kernel module, this layer of
indirection makes the file system portable, since it only needs to
communicate with the FUSE module</li>
<li>It is possible to use even very complex and at first view
non-compatible backends as a FUSE file system’s backend</li>
<li>By using a file system abstraction API like <code>afero.Fs</code>,
we can separate the FUSE implementation from the actual file system
structure, making it unit testable and making it possible to add caching
in user space (code snippet from
https://github.com/pojntfx/stfs/blob/main/pkg/fs/file.go)</li>
<li>It is possible to map any <code>afero.Fs</code> to a FUSE backend,
so it would be possible to switch between different file system backends
without having to write FUSE-specific (code snippet from
https://github.com/JakWai01/sile-fystem/blob/main/pkg/filesystem/fs.go)</li>
<li>For example, STFS used a tape drive as the backend, which is not
random access, but instead append-only and linear
(https://github.com/pojntfx/stfs/blob/main/pkg/operations/update.go)</li>
<li>By using an on-disk index and access optimizations, the resulting
file system was still performant enough to be used, and supported almost
all features required for the average user</li>
<li>FUSE does however also have downsides</li>
<li>It operates in user space, which means that it needs to do context
switching</li>
<li>Some advanced features aren’t available for a FUSE</li>
<li>The overhead of FUSE (and implementing a completely custom file
system) for synchronizing memory is still significant</li>
<li>If possible, the optimal solution would be to not expose a full file
system to track changes, but rather a single file</li>
<li>As a result of this, the significant implementation overhead of such
a file system led to it not being chosen</li>
</ul></li>
<li><p>Pull-Based Memory Synchronization with NBD</p>
<ul>
<li>As hinted at before, a better API would be able to catch
reads/writes to a single <code>mmap</code>ed file instead of having to
implement a complete file system</li>
<li>It does however not have to be an actual file, a block device also
works</li>
<li>In Linux, block devices are (typically storage) devices that support
reading/writing fixed chunks (blocks) of data</li>
<li>We can <code>mmap</code> a block device in the same way that we can
<code>mmap</code> a file</li>
<li>Similarly to how a file system can be implemented in a kernel
module, a block device is typically implemented as a kernel module/in
kernel space</li>
<li>However, the same security, portability and developer experience
issues as with the former also apply here</li>
<li>Instead of implementing a FUSE to solve this, we can create a NBD
(network block device) server that can be used by the kernel NBD module
similarly to how the process that connected to the FUSE kernel module
functioned</li>
<li>The difference between a FUSE and NBD is that a NBD server doesn’t
provide a file system</li>
<li>A NBD server provides the storage device that (typically) hosts a
file system, which means that the interface for it is much, much
simpler</li>
<li>The implementation overhead of a NBD server’s backend is much more
similar to how <code>userfaultfd-go</code> works, rather than a
FUSE</li>
<li>NBD uses a protocol to communicate between a server (provided by
user space) and a client (provided by the NBD kernel module)</li>
<li>The protocol can run over WAN, but is really mostly meant for LAN or
localhost usage</li>
<li>It has two phases: Handshake and transmission</li>
<li>There are two actors in the protocol: One or multiple clients, the
server and the virtual concept of an export</li>
<li>When the client connects to the server, the server sends a greeting
message with the server’s flags</li>
<li>The client responds with its own flags and an export name (a single
NBD server can expose multiple devices) to use</li>
<li>The server sends the export’s size and other metadata, after which
the client acknowledges the received data and the handshake is
complete</li>
<li>After the handshake, the client and server start exchanging commands
and replies</li>
<li>A command can be any of the basic operations needed to access a
block device, e.g. read, write or flush</li>
<li>Depending on the command, it can also contain data (such as the
chunk to be written), offsets, lengths and more</li>
<li>Replies can contain an error, success value or data depending on the
reply’s type</li>
<li>NBD is however limited in some respects; the maximum message size is
32 MB, but the maximum block/chunk size supported by the kernel is just
4096 KB, making it a suboptimal protocol to run over WAN, esp. in high
latency scenarios</li>
<li>The protocol also allows for listing exports, making it possible to
e.g. list multiple memory regions on a single server</li>
<li>NBD is an older protocol with multiple different handshake versions
and legacy features</li>
<li>Since the purpose of NBD in this use case is minimal and both the
server and the client are typically controlled, it makes sense to only
implement the latest recommended versions and the baseline feature
set</li>
<li>The baseline feature set requires no TLS, the latest “fixed
newstyle” handshake, the ability to list and choose an export, as well
as the read, write and disc(onnect) commands and replies</li>
<li>As such, the protocol is very simple to implement</li>
<li>With this simplicity however also come some drawbacks: NBD is less
suited for use cases where the backing device behaves very differently
from a random-access store device, like for example a tape drive, since
it is not possible to work with high-level abstractions such as files or
directories</li>
<li>This is, for the narrow memory synchronization use case, however
more of a feature than a bug</li>
<li>Due to the lack of pre-existing libraries, a new pure Go NBD library
was implemented</li>
<li>This library does not rely on CGo/a pre-existing C library, meaning
that a lot of context switching can be skipped</li>
<li>The backend interface for <code>go-nbd</code> is very simple and
only requires four methods: <code>ReadAt</code>, <code>WriteAt</code>,
<code>Size</code> and <code>Sync</code></li>
<li>A good example backend that maps well to a block device is the file
backend (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/backend/file.go)</li>
<li>The key difference here to the way backends were designed in
<code>userfaultfd-go</code> is that they can also handle writes</li>
<li><code>go-nbd</code> exposes a <code>Handle</code> function to
support multiple users without depending on a specific transport layer
(code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/server/nbd.go)</li>
<li>This means that systems that are peer-to-peer (e.g. WebRTC), and
thus don’t provide a TCP-style <code>accept</code> syscall can still be
used easily</li>
<li>It also allows for easily hosting NBD and other services on the same
TCP socket</li>
<li>The server encodes/decodes messages with the <code>binary</code>
package (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/server/nbd.go#L73-L76)</li>
<li>To make it easier to parse, the headers and other structured
messages are modeled as Go structs (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/protocol/negotiation.go)</li>
<li>The handshake is implemented using a simple for loop, which either
returns on error or breaks</li>
<li>The actual transmission phase is done similarly, by reading in a
header, switching on the message type and reading/sending the relevant
data/reply</li>
<li>The server is completely in user space, there are no kernel
components involved here</li>
<li>The NBD client however is implemented by using the kernel NBD
client</li>
<li>In order to use it, one needs to find a free NBD device first</li>
<li>NBD devices are pre-created by the NBD kernel module and more can be
specified with the <code>nbds_max</code> parameter</li>
<li>In order to find a free one, we can either specify it directly, or
check whether we can find a NBD device with zero size in
<code>sysfs</code> (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/utils/unused.go)</li>
<li>Relevant <code>ioctl</code> numbers depend on the kernel and are
extracted using <code>CGo</code> (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/ioctl/negotiation_cgo.go)</li>
<li>The handshake for the NBD client is negotiated in user space by the
Go program</li>
<li>Simple for loop, basically the same as for the server (code snippet
from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L221-L288)</li>
<li>After the metadata for the export has been fetched in the handshake,
the kernel NBD client is configured using <code>ioctl</code>s (code
snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L290-L328)</li>
<li>The <code>DO_IT</code> syscall never returns, meaning that an
external system must be used to detect whether the device is actually
ready</li>
<li>Two ways of detecting whether the device is ready: By polling
<code>sysfs</code> for the size parameter, or by using
<code>udev</code></li>
<li><code>udev</code> manages devices in Linux</li>
<li>When a device becomes available, the kernel sends a
<code>udev</code> event, which we can subscribe to and use as a reliable
and idiomatic way of waiting for the ready state (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L104C10-L138)</li>
<li>In reality however, polling <code>sysfs</code> directly can be
faster than subscribing to the <code>udev</code> event, so we give the
user the option to switch between both options (code snippet from
https://github.com/pojntfx/go-nbd/blob/main/pkg/client/nbd.go#L140-L178)</li>
<li>When <code>open</code>ing the block device that the client has
connected to, usually the kernel does provide a caching mechanism and
thus requires <code>sync</code> to flush changes</li>
<li>By using <code>O_DIRECT</code> however, it is possible to skip the
kernel caching layer and write all changes directly to the NBD
client/server</li>
<li>This is particularly useful if both the client and server are on the
local system, and if the amount of time spent on <code>sync</code>ing
should be as small as possible</li>
<li>It does however require reads and writes on the device node to be
aligned to the system’s page size, which is possible to implement with a
client-side chunking system but does require application-specific
code</li>
<li>NBD is a battle-tested solution for this with fairly good
performance, but in the future a more lean implemenation called
<code>ublk</code> could also be used</li>
<li><code>ublk</code> uses <code>io_uring</code>, which means that it
could potentially allow for much faster concurrent access</li>
<li>It is similar to NBD; it also uses a user space server to provide
the block device backend, and a kernel <code>ublk</code> driver that
creates <code>/dev/ublkb*</code> devices</li>
<li>Unlike as it is the case for the NBD kernel module, which uses a
rather slow UNIX or TCP socket to communicate, <code>ublk</code> is able
to use <code>io_uring</code> pass-through commands</li>
<li>The <code>io_uring</code> architecture promises lower latency and
better throughput</li>
<li>Because it is however still experimental and docs are lacking, NBD
was chosen</li>
<li>Another option of implementing a block device is BUSE (block devices
in user space)</li>
<li>BUSE is similar to FUSE in nature, and similarly to it has a kernel
and user space server component</li>
<li>Similarly to <code>ublk</code> however, BUSE is experimental</li>
<li>Client libraries in Go are also experimental, preventing it from
being used as easily as NBD</li>
<li>Similarly so, a CUSE could be implemented (char device in user
space)</li>
<li>CUSE is a very flexible way of defining a char (and thus block)
device, but also lacks documentation</li>
<li>The interface being exposed by CUSE is more complicated than that of
e.g. NBD, but allows for interesting features such as custom
<code>ioctl</code>s (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/device.go#L3-L15)</li>
<li>The only way of implementing it without too much overhead however is
CGo, which comes with its own overhead</li>
<li>It also requires calling Go closures from C code, which is
complicated (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/bindings.go#L79-L132)</li>
<li>Implementing closures is possible by using the <code>userdata</code>
parameter in the CUSE C API (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/cuse.c#L20-L22)</li>
<li>To fully use it, it needs to first resolve a Go callback in C, and
then call it with a pointer to the method’s struct in user data,
effectively allowing for the use of closures (code snippet from
https://github.com/pojntfx/webpipe/blob/main/pkg/cuse/bindings.go#L134-L162)</li>
<li>Even with this however, it is hard to implement even a simple
backend, and the CGo overhead is a significant drawback (code snippet
from
https://github.com/pojntfx/webpipe/blob/main/pkg/devices/trace_device.go)</li>
<li>The last alternative to NBD devices would be to extend the kernel
with a new construct that allows for essentially a virtual file to be
<code>mmap</code>ed, not a block device</li>
<li>This could use a custom protocol that is optimized for this use case
instead of a full block device</li>
<li>Because of the extensive setup required to implement such a system
however, and the possibility of <code>ublk</code> providing a performant
alternative in the future, going forward with NBD was chosen for
now</li>
</ul></li>
<li><p>Push-Pull Memory Synchronization with Mounts</p>
<ul>
<li>Usually, the NBD server and client don’t run on the same system</li>
<li>NBD was originally designed to used as a LAN protocol to access a
remote hard disk</li>
<li>As mentioned before, NBD can run over WAN, but is not designed for
this</li>
<li>The biggest problem with running NBD over a public network, even if
TLS is enabled is latency</li>
<li>Individual chunks would only be fetched to the local system as they
are being accessed, adding a guaranteed minimum latency of at least the
RTT</li>
<li>Instead of directly connecting a client to a remote server, we add a
layer of indirection, called a <code>Mount</code> that consists of both
a client and a server, both running on the local system</li>
<li>We then connect the server to the backend with an API that is better
suited for WAN usage</li>
<li>This also makes it possible to implement smart pull/push strategies
instead of simply directly writing to/from the network (“managed
mounts”)</li>
<li>The server and client are connected by creating a connected UNIX
socket pair (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_direct.go#L59-L62)</li>
<li>By building on this basic direct mount, we can add a file (code
snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/file_direct.go) and
slice (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/slice_direct.go)
mount API, which allows for easy usage and integration with
<code>sync</code>/<code>msync</code> respectively</li>
<li>Using the <code>mmap</code>/slice approach has a few benefits</li>
<li>First, it makes it possible to use the byte slice directly as though
it were a byte slice allocated by <code>make</code>, except its
transparently mapped to the (remote) backend</li>
<li><code>mmap</code>/the byte slices also swaps out the syscall-based
file interface with a random access one, which allows for faster
concurrent reads from the underlying backend</li>
<li>Alternatively, it would also be possible to format the server’s
backend or the block device using standard file system tools</li>
<li>When the device then becomes ready, it can be mounted to a directory
on the system</li>
<li>This way it is possible to <code>mmap</code> one or multiple files
on the mounted file system instead of <code>mmap</code>ing the block
device directly</li>
<li>This allows for handling multiple remote regions using a single
server, and thus saving on initialization time and overhead</li>
<li>Using a proper file system however does introduce both storage
overhead and complexity, which is why e.g. the FUSE approach was not
chosen</li>
<li>The simplest form of the mount API is the direct mount API</li>
<li>This API simply swaps out NBD for a transport-independent RPC
framework, but does not do additional optimizations</li>
<li>It has two simple actors: A client and a server, with only the
server providing methods to be called (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/services/backend.go#L14-L19)</li>
<li>The protocol as such is stateless, as there is only a simple remote
read/write interface (add state machine and sequence diagram here)</li>
<li>One additional layer that needs to be implemented however is proper
chunking support</li>
<li>While we can specify a chunk size for the NBD client in the form of
a block size, we can only go up to 4 KB chunks</li>
<li>For scenarios where the RTT between the backend and server is large,
it might make sense to use a much larger chunk size for the actual
networked transfers</li>
<li>Many backends also have constraints that prevent them from
functioning without a specific chunk size or aligned offsets, such as
using e.g. tape drives, which require setting a block size and work best
when these chunks are multiple MBs instead of KBs</li>
<li>Even if there are no constraints on chunking on the backend side
(e.g. when a file is used as the backend), it might make sense to limit
the maximum supported message size between the client and server to
prevent DoS attacks by forcing the backend to allocate large chunks of
memory to satisfy requests, which requires a chunking system to
work</li>
<li>In order to implement the chunking system, we can use a abstraction
layer that allows us to create a pipeline of readers/writers - the
<code>ReadWriterAt</code>, combining an <code>io.ReaderAt</code> and a
<code>io.WriterAt</code></li>
<li>This way, we can forward the <code>Size</code> and <code>Sync</code>
syscalls directly to the underlying backend, but wrap a backend’s
<code>ReadAt</code> and <code>WriteAt</code> methods in a pipeline of
other <code>ReadWriterAt</code>s</li>
<li>One such <code>ReadWriterAt</code> is the
<code>ArbitraryReadWriterAt</code> (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/arbitrary_rwat.go)</li>
<li>It allows breaking down a larger data stream into smaller
chunks</li>
<li>In <code>ReadAt</code>, it calculates the index of the chunk that
the offset falls into and the position within the offsets</li>
<li>It then reads the entire chunk from the backend into a buffer,
copies the necessary portion of the buffer into the input slice, and
repeats the process until all requested data is read</li>
<li>Similarly for the writer, it calculates the chunk’s index and
offset</li>
<li>If an entire chunk is being written to, it bypasses the chunking
system, and writes it directly to not have to unnecessarily copy the
data twice</li>
<li>If only parts of a chunk need to be written, it first reads the
complete chunk into a buffer, modifies the buffer with the data that has
changed, and writes the entire chunk back, until all data has been
written</li>
<li>This simple implementation can be used to allow for writing data of
arbitrary length at arbitrary offsets, even if the backend only supports
a few chunks</li>
<li>In addition to this chunking system, there is also a
<code>ChunkedReadWriterAt</code>, which ensures that the limits
concerning the maximum size supported by the backend and the actual
chunks are being respected</li>
<li>This is particularly useful when the client is expected to do the
chunking, and the server simply checks that the chunking system’s chunk
size is respected</li>
<li>In order to check if a read or write is valid, it checks whether a
read is done to an offset multiple of the chunk size, and whether the
length of the slice of data to read/write is the chunk size (code
snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/chunked_rwat.go)</li>
<li>It is possible to do the chunking in two places; on the mount API’s
side, and on the (potentially remote) backend’s side</li>
<li>Doing the chunking on the backend’s side is usually much faster than
on the mount API’s side, as writes with lengths smaller than the chunk
size will mean that the remote chunk needs to be fetched first,
significantly increasing the latency esp. in scenarios with high
RTT</li>
<li>Benchmark: Local vs. remote chunking</li>
<li>While these systems already allow for some optimizations over simply
using the NBD protocol over WAN, they still mean that chunks will only
be fetched as they are being needed, which means that there still is a
guaranteed minimum downtime</li>
<li>In order to improve on this, a more advanced API (the managed mount
API) was created</li>
<li>A field that tries to optimize for this use case is live migration
of VMs</li>
<li>Live migration refers to moving a virtual machine, its state and
connected devices from one host to another with as little downtime as
possible</li>
<li>There are two types of such migration algorithms; pre-copy and
post-copy migration</li>
<li>Pre-copy migration works by copying data from the source to the
destination as the VM continues to run (or in the case of a generic
migration, app/other state continues being written to)</li>
<li>First, the initial state of the VM’s memory is copied to the
destination</li>
<li>If, during the push, chunks are being modified, they are being
marked as dirty</li>
<li>These dirty chunks are being copied over to the destination until
the number of remaining chunks is small enough to satisfy a maximum
downtime criteria</li>
<li>Once this is the case, the VM is suspended on the source, and the
remaining chunks are synced over to destination</li>
<li>Once the transfer is complete, the VM is resumed on the
destination</li>
<li>This process is helpful since the VM is always available in full on
either the source or the destination, and it is resilient to a network
outage occurring during the synchronization</li>
<li>If the VM (or app etc.) is however changing too many chunks on the
source during the migration, the maximum acceptable downtime criteria
might never get reached, and the maximum acceptable downtime is also
somewhat limited by the available RTT</li>
<li>An alternative to pre-copy migration</li>
<li>In this approach, the VM is immediately suspended on the source,
moved to the destination with only a minimal set of chunks</li>
<li>After the VM has been moved to the destination, it is resumed</li>
<li>If the VM tries to access a chunk on the destination, a page fault
is raised, and the missing page is fetched from the source, and the VM
continues to execute</li>
<li>The benefit of post-copy migration is that it does not require
re-transmitting dirty chunks to the destination before the maximum
tolerable downtime is reached</li>
<li>The big drawback of post-copy migration is that it can result in
longer migration times, because the chunks need to be fetched from the
network on-demand, which is very latency/RTT-sensitive</li>
<li>For the managed mount API, both paradigms were implemented</li>
<li>The managed mount API is primarily intended as an API for reading
from a remote resource and then syncing changes back to it however, not
migrating a resource between two hosts</li>
<li>The migration API (will be discussed later) is a more well-optimized
version for this use case</li>
<li>The pre-copy API is implemented in the form of preemptive pulls
based on another <code>ReaderAt</code></li>
<li>The <code>Puller</code> component asynchronously pulls chunks in the
background (code snipped from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/puller.go)</li>
<li>It allows passing in a pull heuristic function, which it uses to
determine which chunks should be pulled in which order</li>
<li>Many applications commonly access certain bits of data first</li>
<li>If a resource should be available locally as quickly as possible,
then using the correct pull heuristic can help a lot</li>
<li>For example, if the data pulled consists of a header, then using a
pull heuristic that pulls these header chunks first can be of help</li>
<li>If a file system is being synchronized, and the superblocks of the
file system are being stored in a known pattern, the pull heuristic can
be used to fetch these superblocks first</li>
<li>If a format like MP4, which has an index, is used then said index
can be fetched first, be accessed first and during the time needed to
parse the index, the remaining data can be pulled in the background</li>
<li>After sorting the chunks, the puller starts a fixed number of worker
threads in the background, each of which ask for a chunk to pull</li>
<li>Note that the puller itself does not copy to/from a destination;
this use case is handled by a separate component</li>
<li>It simply reads from the provided <code>ReaderAt</code>, which is
then expected to handle the actual copying on its own</li>
<li>The actual copy logic is provided by the
<code>SyncedReadWriterAt</code> instead</li>
<li>This component takes both a remote reader and a local
<code>ReadWriterAt</code></li>
<li>If a chunk is read, e.g. by the puller component calling
<code>ReadAt</code>, it is tracked and market as remote by adding it to
a local map</li>
<li>The chunk is then read from the remote reader and written to the
local <code>ReadWriterAt</code>, and is then marked as locally
available, so that on the second read it is fetched locally
directly</li>
<li>A callback is then called which can be used to monitor the pull
process</li>
<li>Note that if it is used in a pipeline with the <code>Puller</code>,
this also means that if a chunk which hasn’t been fetched asynchronously
yet will be scheduled to be pulled immediately</li>
<li>WriteAt also starts by tracking a chunk, but then immediately marks
the chunk as available locally no matter whether it has been pulled
before</li>
<li>The combination of the <code>SyncedReadWriterAt</code> and the
<code>Puller</code> component implements the pull post-copy system in a
modular and testable way</li>
<li>Unlike the usual way of only fetching chunks when they are available
however, this system also allows fetching them pre-emptively, gaining
some benefits of pre-copy migration, too</li>
<li>Using this <code>Puller</code> interface, it is possible to
implement a read-only managed mount</li>
<li>This is very similar the <code>rr+</code> prefetching mechanism from
“Remote Regions” (reference atc18-aguilera)</li>
<li>In order to also be able to write back however, it needs to have a
push system as well</li>
<li>This push system is being started in parallel with the pull
system</li>
<li>It also takes a local and a remote <code>ReadWriterAt</code></li>
<li>Chunks that have changed/are pushable are marked with
<code>MarkOffsetPushable</code> (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L171C24-L185)</li>
<li>This integrates with the callbacks supplied by the syncer, which
ensures that we don’t sync back changes that have been pulled but not
modified, only the ones that have been changed locally</li>
<li>Once opened, the pusher starts a new goroutine in the background
which calls <code>Sync</code> in a set recurring interval (code snippet
from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/pusher.go)</li>
<li>Once sync is called by this background worker system or manually, it
launches workers in the background</li>
<li>These workers all wait for a chunk to handle</li>
<li>Once a worker receives a chunk, it reads it from the local
<code>ReadWriter</code>, and copies it to the remote</li>
<li>Safe access is ensures by individually locking each chunk</li>
<li>The pusher also serves as a step in the <code>ReadWriterAt</code>
pipeline</li>
<li>In order to do this, it exposes <code>ReadAt</code> and
<code>WriteAt</code></li>
<li><code>ReadAt</code> is a simple proxy, while <code>WriteAt</code>
also marks a chunk as pushable (since it mutates data) before writing to
the local <code>ReadWriterAt</code></li>
<li>For the direct mount, the NBD server was directly connected to the
remote, while in this setup a pipeline of pullers, pushers, a syncer and
an <code>ArbitraryReadWriter</code> is used (graphic of the four systems
and how they are connected to each other vs. how the direct mounts
work)</li>
<li>For a read-only scenario, the <code>Pusher</code> step is simply
skipped (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L142-L169)</li>
<li>If no background pulls are enabled, the creation of the
<code>Puller</code> is simply skipped (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L187-L222)</li>
<li>This setup allows pulling from the remote <code>ReadWriterAt</code>
before the NBD device is open</li>
<li>This means that we can start pulling in the background as the NBD
client and server are still starting (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/path_managed.go#L251-L272)</li>
<li>These two components typically start fairly quickly, but can still
take multiple ms</li>
<li>Often, it takes as long as one RTT, so parallelizing this startup
process can significantly reduce the initial latency and pre-emptively
pull quite a bit of data</li>
<li>Benchmark: Parallelizing startups and pulling n MBs as the device
starts</li>
<li>Using this benchmark and simple interface also makes the entire
system very testable</li>
<li>In the tests, a memory reader or file can be used as the local or
remote <code>ReaderWriterAt</code>s and then a simple table-driven test
can be used (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/chunks/puller_test.go)</li>
<li>With all of these components in place, the managed mounts API serves
as a fast and efficient option to access almost any remote resource in
memory</li>
<li>Similarly to how the direct mounts API used the basic path mount to
build the file and <code>mmap</code> interfaces, the managed mount
builds on this interface in order to provide the same interfaces</li>
<li>It is however a bit more complicated for the lifecycle to work</li>
<li>For example, in order to allow for a <code>Sync()</code> API,
e.g. the <code>msync</code> on the <code>mmap</code>ed file must happen
before <code>Sync()</code> is called on the syncer</li>
<li>This is done through a hooks system (code snippet from
https://github.com/pojntfx/r3map/blob/main/pkg/mount/file_managed.go#L34-L37)</li>
<li>The same hooks system is also used to implement the correct
lifecycle when <code>Close</code>ing the mount</li>
<li>While the managed mounts API mostly works, there are some issues
with it being implemented in Go</li>
<li>This is mostly due to deadlocking issues; if the GC tries to release
memory, it has to stop the world</li>
<li>If the <code>mmap</code> API is used, it is possible that the GC
tries to manage the underlying slice, or tries to release memory as data
is being copied from the mount</li>
<li>Because the NBD server that provides the byte slice is also running
in the same process, this causes a deadlock as the server that provides
the backend for the mount is also frozen
(https://github.com/pojntfx/r3map/blob/main/pkg/mount/slice_managed.go#L70-L93)</li>
<li>A workaround for this is to lock the <code>mmap</code>ed region into
memory, but this will also cause all chunks to be fetched, which leads
to a high <code>Open()</code> latency</li>
<li>This is fixable by simply starting the server in separate thread,
and then <code>mmap</code>ing</li>
<li>Issues like this however are hard to fix, and point to Go
potentially not being the correct language to use for this part of the
system</li>
<li>In the future, using a language without a GC (such as Rust) could
provide a good alternative</li>
<li>While the current API is Go-specific, it could also be exposed
through a different interface to make it usable in Go</li>
<li>A similar approach was made in RegionFS (reference
atc18-aguilera)</li>
<li>RegionFS is implemented as a kernel module, but it is functionally
similar to how this API exposes a NBD device for memory interaction</li>
<li>In RegionFS, the regions file system is mounted to a path, which
then exposes regions as virtual files</li>
<li>Instead of using a custom configuration (such as configuring the
amount of pushers to make a mount read-only), such an approach makes it
possible to use <code>chmod</code> on the virtual file for a memory
region to set permissions</li>
<li>By using standard utilities like <code>open</code> and
<code>chmod</code>, this API usable from different programming languages
with ease</li>
<li>Unlike the managed mounts API however, the system proposed in Remote
Regions is mostly intended for private usecases with a limited amount of
hosts and in LAN, with low-RTT connections</li>
<li>It is also not designed to be used for a potential migration
scenarios, which the modular approach of r3map allows for</li>
<li>While Remote Regions’ file system approach does allow for
authorization based on permissions, it doesn’t specify how
authentication could work</li>
<li>In terms of the wire protocol, Remote Regions also seems to target
mostly LAN with protocols like RDMA comm modules, while r3map targets
mostly WAN with a pluggable transport protocol interface</li>
</ul></li>
</ul>
</section>
</body>
</html>
